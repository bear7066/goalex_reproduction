{
  "goal": "Represent the scientific articles by their primary research subfield for clustering, for example, machine learning, Robotics, Cryptography, Signal Processing, Information Retrieval, computer vision, human-computer interaction. Please cluster them into 9 groups. Each group should contain at least 3 articles.",
  "texts": [
    "Class imbalance is a common problem in supervised learning and impedes the\npredictive performance of classification models. Popular countermeasures\ninclude oversampling the minority class. Standard methods like SMOTE rely on\nfinding nearest neighbours and linear interpolations which are problematic in\ncase of high-dimensional, complex data distributions. Generative Adversarial\nNetworks (GANs) have been proposed as an alternative method for generating\nartificial minority examples as they can model complex distributions. However,\nprior research on GAN-based oversampling does not incorporate recent\nadvancements from the literature on generating realistic tabular data with\nGANs. Previous studies also focus on numerical variables whereas categorical\nfeatures are common in many business applications of classification methods\nsuch as credit scoring. The paper propoes an oversampling method based on a\nconditional Wasserstein GAN that can effectively model tabular datasets with\nnumerical and categorical variables and pays special attention to the\ndown-stream classification task through an auxiliary classifier loss. We\nbenchmark our method against standard oversampling methods and the imbalanced\nbaseline on seven real-world datasets. Empirical results evidence the\ncompetitiveness of GAN-based oversampling.",
    "We revisit the standard formulation of tabular actor-critic algorithm as a\ntwo time-scale stochastic approximation with value function computed on a\nfaster time-scale and policy computed on a slower time-scale. This emulates\npolicy iteration. We observe that reversal of the time scales will in fact\nemulate value iteration and is a legitimate algorithm. We provide a proof of\nconvergence and compare the two empirically with and without function\napproximation (with both linear and nonlinear function approximators) and\nobserve that our proposed critic-actor algorithm performs on par with\nactor-critic in terms of both accuracy and computational effort.",
    "We show that using nearest neighbours in the latent space of autoencoders\n(AE) significantly improves performance of semi-supervised novelty detection in\nboth single and multi-class contexts. Autoencoding methods detect novelty by\nlearning to differentiate between the non-novel training class(es) and all\nother unseen classes. Our method harnesses a combination of the reconstructions\nof the nearest neighbours and the latent-neighbour distances of a given input's\nlatent representation. We demonstrate that our nearest-latent-neighbours (NLN)\nalgorithm is memory and time efficient, does not require significant data\naugmentation, nor is reliant on pre-trained networks. Furthermore, we show that\nthe NLN-algorithm is easily applicable to multiple datasets without\nmodification. Additionally, the proposed algorithm is agnostic to autoencoder\narchitecture and reconstruction error method. We validate our method across\nseveral standard datasets for a variety of different autoencoding architectures\nsuch as vanilla, adversarial and variational autoencoders using either\nreconstruction, residual or feature consistent losses. The results show that\nthe NLN algorithm grants up to a 17% increase in Area Under the Receiver\nOperating Characteristics (AUROC) curve performance for the multi-class case\nand 8% for single-class novelty detection.",
    "We introduce Universum learning for multiclass problems and propose a novel\nformulation for multiclass universum SVM (MU-SVM). We also propose a span bound\nfor MU-SVM that can be used for model selection thereby avoiding resampling.\nEmpirical results demonstrate the effectiveness of MU-SVM and the proposed\nbound.",
    "Reward-free reinforcement learning (RL) is a framework which is suitable for\nboth the batch RL setting and the setting where there are many reward functions\nof interest. During the exploration phase, an agent collects samples without\nusing a pre-specified reward function. After the exploration phase, a reward\nfunction is given, and the agent uses samples collected during the exploration\nphase to compute a near-optimal policy. Jin et al. [2020] showed that in the\ntabular setting, the agent only needs to collect polynomial number of samples\n(in terms of the number states, the number of actions, and the planning\nhorizon) for reward-free RL. However, in practice, the number of states and\nactions can be large, and thus function approximation schemes are required for\ngeneralization. In this work, we give both positive and negative results for\nreward-free RL with linear function approximation. We give an algorithm for\nreward-free RL in the linear Markov decision process setting where both the\ntransition and the reward admit linear representations. The sample complexity\nof our algorithm is polynomial in the feature dimension and the planning\nhorizon, and is completely independent of the number of states and actions. We\nfurther give an exponential lower bound for reward-free RL in the setting where\nonly the optimal $Q$-function admits a linear representation. Our results imply\nseveral interesting exponential separations on the sample complexity of\nreward-free RL.",
    "Canonical Correlation Analysis (CCA) is widely used for multimodal data\nanalysis and, more recently, for discriminative tasks such as multi-view\nlearning; however, it makes no use of class labels. Recent CCA methods have\nstarted to address this weakness but are limited in that they do not\nsimultaneously optimize the CCA projection for discrimination and the CCA\nprojection itself, or they are linear only. We address these deficiencies by\nsimultaneously optimizing a CCA-based and a task objective in an end-to-end\nmanner. Together, these two objectives learn a non-linear CCA projection to a\nshared latent space that is highly correlated and discriminative. Our method\nshows a significant improvement over previous state-of-the-art (including deep\nsupervised approaches) for cross-view classification, regularization with a\nsecond view, and semi-supervised learning on real data.",
    "We propose a new method of program learning in a Domain Specific Language\n(DSL) which is based on gradient descent with no direct search. The first\ncomponent of our method is a probabilistic representation of the DSL variables.\nAt each timestep in the program sequence, different DSL functions are applied\non the DSL variables with a certain probability, leading to different possible\noutcomes. Rather than handling all these outputs separately, whose number grows\nexponentially with each timestep, we collect them into a superposition of\nvariables which captures the information in a single, but fuzzy, state. This\nstate is to be contrasted at the final timestep with the ground-truth output,\nthrough a loss function. The second component of our method is an\nattention-based recurrent neural network, which provides an appropriate\ninitialization point for the gradient descent that optimizes the probabilistic\nrepresentation. The method we have developed surpasses the state-of-the-art for\nsynthesising long programs and is able to learn programs under noise.",
    "Several papers argue that wide minima generalize better than narrow minima.\nIn this paper, through detailed experiments that not only corroborate the\ngeneralization properties of wide minima, we also provide empirical evidence\nfor a new hypothesis that the density of wide minima is likely lower than the\ndensity of narrow minima. Further, motivated by this hypothesis, we design a\nnovel explore-exploit learning rate schedule. On a variety of image and natural\nlanguage datasets, compared to their original hand-tuned learning rate\nbaselines, we show that our explore-exploit schedule can result in either up to\n0.84% higher absolute accuracy using the original training budget or up to 57%\nreduced training time while achieving the original reported accuracy. For\nexample, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN)\ndataset by just modifying the learning rate schedule of a high performing\nmodel.",
    "Biclustering is a powerful approach to search for patterns in data, as it can\nbe driven by a function that measures the quality of diverse types of patterns\nof interest. However, due to its computational complexity, the exploration of\nthe search space is usually guided by an algorithmic strategy, sometimes\nintroducing random factors that simplify the computational cost (e.g. greedy\nsearch or evolutionary computation).\n  Shifting patterns are specially interesting as they account constant\nfluctuations in data, i.e. they capture situations in which all the values in\nthe pattern move up or down for one dimension maintaining the range amplitude\nfor all the dimensions. This behaviour is very common in nature, e.g. in the\nanalysis of gene expression data, where a subset of genes might go up or down\nfor a subset of patients or experimental conditions, identifying functionally\ncoherent categories.\n  Boolean reasoning was recently revealed as an appropriate methodology to\naddress the search for constant biclusters. In this work, this direction is\nextended to search for more general biclusters that include shifting patterns.\nThe mathematical foundations are described in order to associate Boolean\nconcepts with shifting patterns, and the methodology is presented to show that\nthe induction of shifting patterns by means of Boolean reasoning is due to the\nability of finding all inclusion--maximal {\\delta}-shifting patterns.\n  Experiments with a real dataset show the potential of our approach at finding\nbiclusters with {\\delta}-shifting patterns, which have been evaluated with the\nmean squared residue (MSR), providing an excellent performance at finding\nresults very close to zero.",
    "Explaining machine learning models is an important and increasingly popular\narea of research interest. The Shapley value from game theory has been proposed\nas a prime approach to compute feature importance towards model predictions on\nimages, text, tabular data, and recently graph neural networks (GNNs) on\ngraphs. In this work, we revisit the appropriateness of the Shapley value for\nGNN explanation, where the task is to identify the most important subgraph and\nconstituent nodes for GNN predictions. We claim that the Shapley value is a\nnon-ideal choice for graph data because it is by definition not\nstructure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method\nto leverage the critical graph structure information to improve the\nexplanation. Specifically, we define a scoring function based on a new\nstructure-aware value from the cooperative game theory proposed by Hamiache and\nNavarro (HN). When used to score node importance, the HN value utilizes graph\nstructures to attribute cooperation surplus between neighbor nodes, resembling\nmessage passing in GNNs, so that node importance scores reflect not only the\nnode feature importance, but also the node structural roles. We demonstrate\nthat GStarX produces qualitatively more intuitive explanations, and\nquantitatively improves explanation fidelity over strong baselines on chemical\ngraph property prediction and text graph sentiment classification.",
    "Low-cost cross-modal representation learning is crucial for deriving semantic\nrepresentations across diverse modalities such as text, audio, images, and\nvideo. Traditional approaches typically depend on large specialized models\ntrained from scratch, requiring extensive datasets and resulting in high\nresource and time costs. To overcome these challenges, we introduce a novel\napproach named Lightweight Cross-Modal Representation Learning (LightCRL). This\nmethod uses a single neural network titled Deep Fusion Encoder (DFE), which\nprojects data from multiple modalities into a shared latent representation\nspace. This reduces the overall parameter count while still delivering robust\nperformance comparable to more complex systems.",
    "To date, various neural methods have been proposed for causal effect\nestimation based on observational data, where a default assumption is the same\ndistribution and availability of variables at both training and inference\n(i.e., runtime) stages. However, distribution shift (i.e., domain shift) could\nhappen during runtime, and bigger challenges arise from the impaired\naccessibility of variables. This is commonly caused by increasing privacy and\nethical concerns, which can make arbitrary variables unavailable in the entire\nruntime data and imputation impractical. We term the co-occurrence of domain\nshift and inaccessible variables runtime domain corruption, which seriously\nimpairs the generalizability of a trained counterfactual predictor. To counter\nruntime domain corruption, we subsume counterfactual prediction under the\nnotion of domain adaptation. Specifically, we upper-bound the error w.r.t. the\ntarget domain (i.e., runtime covariates) by the sum of source domain error and\ninter-domain distribution distance. In addition, we build an adversarially\nunified variational causal effect model, named VEGAN, with a novel two-stage\nadversarial domain adaptation scheme to reduce the latent distribution\ndisparity between treated and control groups first, and between training and\nruntime variables afterwards. We demonstrate that VEGAN outperforms other\nstate-of-the-art baselines on individual-level treatment effect estimation in\nthe presence of runtime domain corruption on benchmark datasets.",
    "Given a resource-rich source graph and a resource-scarce target graph, how\ncan we effectively transfer knowledge across graphs and ensure a good\ngeneralization performance? In many high-impact domains (e.g., brain networks\nand molecular graphs), collecting and annotating data is prohibitively\nexpensive and time-consuming, which makes domain adaptation an attractive\noption to alleviate the label scarcity issue. In light of this, the\nstate-of-the-art methods focus on deriving domain-invariant graph\nrepresentation that minimizes the domain discrepancy. However, it has recently\nbeen shown that a small domain discrepancy loss may not always guarantee a good\ngeneralization performance, especially in the presence of disparate graph\nstructures and label distribution shifts. In this paper, we present TRANSNET, a\ngeneric learning framework for augmenting knowledge transfer across graphs. In\nparticular, we introduce a novel notion named trinity signal that can naturally\nformulate various graph signals at different granularity (e.g., node\nattributes, edges, and subgraphs). With that, we further propose a domain\nunification module together with a trinity-signal mixup scheme to jointly\nminimize the domain discrepancy and augment the knowledge transfer across\ngraphs. Finally, comprehensive empirical results show that TRANSNET outperforms\nall existing approaches on seven benchmark datasets by a significant margin.",
    "Reducing traffic accidents is a crucial global public safety concern.\nAccident prediction is key to improving traffic safety, enabling proactive\nmeasures to be taken before a crash occurs, and informing safety policies,\nregulations, and targeted interventions. Despite numerous studies on accident\nprediction over the past decades, many have limitations in terms of\ngeneralizability, reproducibility, or feasibility for practical use due to\ninput data or problem formulation. To address existing shortcomings, we propose\nCrashFormer, a multi-modal architecture that utilizes comprehensive (but\nrelatively easy to obtain) inputs such as the history of accidents, weather\ninformation, map images, and demographic information. The model predicts the\nfuture risk of accidents on a reasonably acceptable cadence (i.e., every six\nhours) for a geographical location of 5.161 square kilometers. CrashFormer is\ncomposed of five components: a sequential encoder to utilize historical\naccidents and weather data, an image encoder to use map imagery data, a raw\ndata encoder to utilize demographic information, a feature fusion module for\naggregating the encoded features, and a classifier that accepts the aggregated\ndata and makes predictions accordingly. Results from extensive real-world\nexperiments in 10 major US cities show that CrashFormer outperforms\nstate-of-the-art sequential and non-sequential models by 1.8% in F1-score on\naverage when using ``sparse'' input data.",
    "There is often variation in the shape and size of input data used for deep\nlearning. In many cases, such data can be represented using tensors with\nnon-uniform shapes, or ragged tensors. Due to limited and non-portable support\nfor efficient execution on ragged tensors, current deep learning frameworks\ngenerally use techniques such as padding and masking to make the data shapes\nuniform and then offload the computations to optimized kernels for dense tensor\nalgebra. Such techniques can, however, lead to a lot of wasted computation and\ntherefore, a loss in performance. This paper presents CoRa, a tensor compiler\nthat allows users to easily generate efficient code for ragged tensor operators\ntargeting a wide range of CPUs and GPUs. Evaluating CoRa on a variety of\noperators on ragged tensors as well as on an encoder layer of the transformer\nmodel, we find that CoRa (i)performs competitively with hand-optimized\nimplementations of the operators and the transformer encoder and (ii) achieves,\nover PyTorch, a 1.6X geomean speedup for the encoder on an Nvidia GPU and a\n1.86X geomean speedup for the multi-head attention module used in transformers\non an ARM CPU.",
    "In this paper, we study the robustness of graph convolutional networks\n(GCNs). Despite the good performance of GCNs on graph semi-supervised learning\ntasks, previous works have shown that the original GCNs are very unstable to\nadversarial perturbations. In particular, we can observe a severe performance\ndegradation by slightly changing the graph adjacency matrix or the features of\na few nodes, making it unsuitable for security-critical applications. Inspired\nby the previous works on adversarial defense for deep neural networks, and\nespecially adversarial training algorithm, we propose a method called\nGraphDefense to defend against the adversarial perturbations. In addition, for\nour defense method, we could still maintain semi-supervised learning settings,\nwithout a large label rate. We also show that adversarial training in features\nis equivalent to adversarial training for edges with a small perturbation. Our\nexperiments show that the proposed defense methods successfully increase the\nrobustness of Graph Convolutional Networks. Furthermore, we show that with\ncareful design, our proposed algorithm can scale to large graphs, such as\nReddit dataset.",
    "Explanations of time series models are useful for high stakes applications\nlike healthcare but have received little attention in machine learning\nliterature. We propose FIT, a framework that evaluates the importance of\nobservations for a multivariate time-series black-box model by quantifying the\nshift in the predictive distribution over time. FIT defines the importance of\nan observation based on its contribution to the distributional shift under a\nKL-divergence that contrasts the predictive distribution against a\ncounterfactual where the rest of the features are unobserved. We also\ndemonstrate the need to control for time-dependent distribution shifts. We\ncompare with state-of-the-art baselines on simulated and real-world clinical\ndata and demonstrate that our approach is superior in identifying important\ntime points and observations throughout the time series.",
    "Machine unlearning (MU) aims to eliminate information that has been learned\nfrom specific training data, namely forgetting data, from a pre-trained model.\nCurrently, the mainstream of existing MU methods involves modifying the\nforgetting data with incorrect labels and subsequently fine-tuning the model.\nWhile learning such incorrect information can indeed remove knowledge, the\nprocess is quite unnatural as the unlearning process undesirably reinforces the\nincorrect information and leads to over-forgetting. Towards more\n\\textit{natural} machine unlearning, we inject correct information from the\nremaining data to the forgetting samples when changing their labels. Through\npairing these adjusted samples with their labels, the model will tend to use\nthe injected correct information and naturally suppress the information meant\nto be forgotten. Albeit straightforward, such a first step towards natural\nmachine unlearning can significantly outperform current state-of-the-art\napproaches. In particular, our method substantially reduces the over-forgetting\nand leads to strong robustness to hyperparameters, making it a promising\ncandidate for practical machine unlearning.",
    "Density ratio estimation (DRE) is a fundamental machine learning technique\nfor identifying relationships between two probability distributions.\n$f$-divergence loss functions, derived from variational representations of\n$f$-divergence, are commonly employed in DRE to achieve state-of-the-art\nresults. This study presents a novel perspective on DRE using $f$-divergence\nloss functions by deriving the upper and lower bounds on $L_p$ errors. These\nbounds apply to any estimator within a class of Lipschitz continuous\nestimators, irrespective of the specific $f$-divergence loss functions\nutilized. The bounds are formulated as a product of terms that include the data\ndimension and the expected value of the density ratio raised to the power of\n$p$. Notably, the lower bound incorporates an exponential term dependent on the\nKullback--Leibler divergence, indicating that the $L_p$ error significantly\nincreases with the Kullback--Leibler divergence for $p > 1$, and this increase\nbecomes more pronounced as $p$ increases. Furthermore, these theoretical\nfindings are substantiated through numerical experiments.",
    "We propose a self-supervised framework that learns to group visual entities\nbased on their rate of co-occurrence in space and time. To model statistical\ndependencies between the entities, we set up a simple binary classification\nproblem in which the goal is to predict if two visual primitives occur in the\nsame spatial or temporal context. We apply this framework to three domains:\nlearning patch affinities from spatial adjacency in images, learning frame\naffinities from temporal adjacency in videos, and learning photo affinities\nfrom geospatial proximity in image collections. We demonstrate that in each\ncase the learned affinities uncover meaningful semantic groupings. From patch\naffinities we generate object proposals that are competitive with\nstate-of-the-art supervised methods. From frame affinities we generate movie\nscene segmentations that correlate well with DVD chapter structure. Finally,\nfrom geospatial affinities we learn groups that relate well to semantic place\ncategories.",
    "In this paper, we introduce a novel approach for diagnosis of Parkinson's\nDisease (PD) based on deep Echo State Networks (ESNs). The identification of PD\nis performed by analyzing the whole time-series collected from a tablet device\nduring the sketching of spiral tests, without the need for feature extraction\nand data preprocessing. We evaluated the proposed approach on a public dataset\nof spiral tests. The results of experimental analysis show that DeepESNs\nperform significantly better than shallow ESN model. Overall, the proposed\napproach obtains state-of-the-art results in the identification of PD on this\nkind of temporal data.",
    "Feature attribution methods, which explain an individual prediction made by a\nmodel as a sum of attributions for each input feature, are an essential tool\nfor understanding the behavior of complex deep learning models. However,\nensuring that models produce meaningful explanations, rather than ones that\nrely on noise, is not straightforward. Exacerbating this problem is the fact\nthat attribution methods do not provide insight as to why features are assigned\ntheir attribution values, leading to explanations that are difficult to\ninterpret. In real-world problems we often have sets of additional information\nfor each feature that are predictive of that feature's importance to the task\nat hand. Here, we propose the deep attribution prior (DAPr) framework to\nexploit such information to overcome the limitations of attribution methods.\nOur framework jointly learns a relationship between prior information and\nfeature importance, as well as biases models to have explanations that rely on\nfeatures predicted to be important. We find that our framework both results in\nnetworks that generalize better to out of sample data and admits new methods\nfor interpreting model behavior.",
    "In many naturally occurring optimization problems one needs to ensure that\nthe definition of the optimization problem lends itself to solutions that are\ntractable to compute. In cases where exact solutions cannot be computed\ntractably, it is beneficial to have strong guarantees on the tractable\napproximate solutions. In order operate under these criterion most optimization\nproblems are cast under the umbrella of convexity or submodularity. In this\nreport we will study design and optimization over a common class of functions\ncalled submodular functions. Set functions, and specifically submodular set\nfunctions, characterize a wide variety of naturally occurring optimization\nproblems, and the property of submodularity of set functions has deep\ntheoretical consequences with wide ranging applications. Informally, the\nproperty of submodularity of set functions concerns the intuitive \"principle of\ndiminishing returns. This property states that adding an element to a smaller\nset has more value than adding it to a larger set. Common examples of\nsubmodular monotone functions are entropies, concave functions of cardinality,\nand matroid rank functions; non-monotone examples include graph cuts, network\nflows, and mutual information.\n  In this paper we will review the formal definition of submodularity; the\noptimization of submodular functions, both maximization and minimization; and\nfinally discuss some applications in relation to learning and reasoning using\nsubmodular functions.",
    "Neural networks with binary weights are computation-efficient and\nhardware-friendly, but their training is challenging because it involves a\ndiscrete optimization problem. Surprisingly, ignoring the discrete nature of\nthe problem and using gradient-based methods, such as the Straight-Through\nEstimator, still works well in practice. This raises the question: are there\nprincipled approaches which justify such methods? In this paper, we propose\nsuch an approach using the Bayesian learning rule. The rule, when applied to\nestimate a Bernoulli distribution over the binary weights, results in an\nalgorithm which justifies some of the algorithmic choices made by the previous\napproaches. The algorithm not only obtains state-of-the-art performance, but\nalso enables uncertainty estimation for continual learning to avoid\ncatastrophic forgetting. Our work provides a principled approach for training\nbinary neural networks which justifies and extends existing approaches.",
    "We consider the bandit problem of selecting $K$ out of $N$ arms at each time\nstep. The reward can be a non-linear function of the rewards of the selected\nindividual arms. The direct use of a multi-armed bandit algorithm requires\nchoosing among $\\binom{N}{K}$ options, making the action space large. To\nsimplify the problem, existing works on combinatorial bandits {typically}\nassume feedback as a linear function of individual rewards. In this paper, we\nprove the lower bound for top-$K$ subset selection with bandit feedback with\npossibly correlated rewards. We present a novel algorithm for the combinatorial\nsetting without using individual arm feedback or requiring linearity of the\nreward function. Additionally, our algorithm works on correlated rewards of\nindividual arms. Our algorithm, aDaptive Accept RejecT (DART), sequentially\nfinds good arms and eliminates bad arms based on confidence bounds. DART is\ncomputationally efficient and uses storage linear in $N$. Further, DART\nachieves a regret bound of $\\tilde{\\mathcal{O}}(K\\sqrt{KNT})$ for a time\nhorizon $T$, which matches the lower bound in bandit feedback up to a factor of\n$\\sqrt{\\log{2NT}}$. When applied to the problem of cross-selling optimization\nand maximizing the mean of individual rewards, the performance of the proposed\nalgorithm surpasses that of state-of-the-art algorithms. We also show that DART\nsignificantly outperforms existing methods for both linear and non-linear joint\nreward environments.",
    "Gaussian processes (GPs) are non-parametric, flexible, models that work well\nin many tasks. Combining GPs with deep learning methods via deep kernel\nlearning (DKL) is especially compelling due to the strong representational\npower induced by the network. However, inference in GPs, whether with or\nwithout DKL, can be computationally challenging on large datasets. Here, we\npropose GP-Tree, a novel method for multi-class classification with Gaussian\nprocesses and DKL. We develop a tree-based hierarchical model in which each\ninternal node of the tree fits a GP to the data using the P\\'olya Gamma\naugmentation scheme. As a result, our method scales well with both the number\nof classes and data size. We demonstrate the effectiveness of our method\nagainst other Gaussian process training baselines, and we show how our general\nGP approach achieves improved accuracy on standard incremental few-shot\nlearning benchmarks.",
    "In a business-to-business (B2B) customer relationship management (CRM) use\ncase, each client is a potential business organization/company with a solid\nbusiness strategy and focused and rational decisions. This paper introduces a\ngraph-based analytics approach to improve CRM within a B2B environment. In our\napproach, in the first instance, we have designed a graph database using the\nNeo4j platform. Secondly, the graph database has been investigated by using\ndata mining and exploratory analysis coupled with cypher graph query language.\nSpecifically, we have applied the graph convolution network (GCN) to enable CRM\nanalytics to forecast sales. This is the first step towards a GCN-based binary\nclassification based on graph databases in the domain of B2B CRM. We evaluate\nthe performance of the proposed GCN model on graph databases and compare it\nwith Random Forest (RF), Convolutional Neural Network (CNN), and Artificial\nNeural Network (ANN). The proposed GCN approach is further augmented with the\nshortest path and eigenvector centrality attribute to significantly improve the\naccuracy of sales prediction. Experimental results reveal that the proposed\ngraph-based deep learning approach outperforms the Random Forests (RsF) and two\ndeep learning models, i.e., CNN and ANN under different combinations of graph\nfeatures.",
    "Multi-armed bandits (MAB) are extensively studied in various settings where\nthe objective is to \\textit{maximize} the actions' outcomes (i.e., rewards)\nover time. Since safety is crucial in many real-world problems, safe versions\nof MAB algorithms have also garnered considerable interest. In this work, we\ntackle a different critical task through the lens of \\textit{linear stochastic\nbandits}, where the aim is to keep the actions' outcomes close to a target\nlevel while respecting a \\textit{two-sided} safety constraint, which we call\n\\textit{leveling}. Such a task is prevalent in numerous domains. Many\nhealthcare problems, for instance, require keeping a physiological variable in\na range and preferably close to a target level. The radical change in our\nobjective necessitates a new acquisition strategy, which is at the heart of a\nMAB algorithm. We propose SALE-LTS: Safe Leveling via Linear Thompson Sampling\nalgorithm, with a novel acquisition strategy to accommodate our task and show\nthat it achieves sublinear regret with the same time and dimension dependence\nas previous works on the classical reward maximization problem absent any\nsafety constraint. We demonstrate and discuss our algorithm's empirical\nperformance in detail via thorough experiments.",
    "Deep neural networks (DNNs) have been emerged as the state-of-the-art\nalgorithms in broad range of applications. To reduce the memory foot-print of\nDNNs, in particular for embedded applications, sparsification techniques have\nbeen proposed. Unfortunately, these techniques come with a large hardware\noverhead. In this paper, we present a hardware-aware pruning method where the\nlocations of non-zero weights are derived in real-time from a Linear Feedback\nShift Registers (LFSRs). Using the proposed method, we demonstrate a total\nsaving of energy and area up to 63.96% and 64.23% for VGG-16 network on\ndown-sampled ImageNet, respectively for iso-compression-rate and iso-accuracy.",
    "This paper studies a new variant of the stochastic multi-armed bandits\nproblem where auxiliary information about the arm rewards is available in the\nform of control variates. In many applications like queuing and wireless\nnetworks, the arm rewards are functions of some exogenous variables. The mean\nvalues of these variables are known a priori from historical data and can be\nused as control variates. Leveraging the theory of control variates, we obtain\nmean estimates with smaller variance and tighter confidence bounds. We develop\nan upper confidence bound based algorithm named UCB-CV and characterize the\nregret bounds in terms of the correlation between rewards and control variates\nwhen they follow a multivariate normal distribution. We also extend UCB-CV to\nother distributions using resampling methods like Jackknifing and Splitting.\nExperiments on synthetic problem instances validate performance guarantees of\nthe proposed algorithms.",
    "Deep neural networks often suffer from overconfidence which can be partly\nremedied by improved out-of-distribution detection. For this purpose, we\npropose a novel approach that allows for the generation of out-of-distribution\ndatasets based on a given in-distribution dataset. This new dataset can then be\nused to improve out-of-distribution detection for the given dataset and machine\nlearning task at hand. The samples in this dataset are with respect to the\nfeature space close to the in-distribution dataset and therefore realistic and\nplausible. Hence, this dataset can also be used to safeguard neural networks,\ni.e., to validate the generalization performance. Our approach first generates\nsuitable representations of an in-distribution dataset using an autoencoder and\nthen transforms them using our novel proposed Soft Brownian Offset method.\nAfter transformation, the decoder part of the autoencoder allows for the\ngeneration of these implicit out-of-distribution samples. This newly generated\ndataset then allows for mixing with other datasets and thus improved training\nof an out-of-distribution classifier, increasing its performance.\nExperimentally, we show that our approach is promising for time series using\nsynthetic data. Using our new method, we also show in a quantitative case study\nthat we can improve the out-of-distribution detection for the MNIST dataset.\nFinally, we provide another case study on the synthetic generation of\nout-of-distribution trajectories, which can be used to validate trajectory\nprediction algorithms for automated driving.",
    "Deep neural networks trained on a wide range of datasets demonstrate\nimpressive transferability. Deep features appear general in that they are\napplicable to many datasets and tasks. Such property is in prevalent use in\nreal-world applications. A neural network pretrained on large datasets, such as\nImageNet, can significantly boost generalization and accelerate training if\nfine-tuned to a smaller target dataset. Despite its pervasiveness, few effort\nhas been devoted to uncovering the reason of transferability in deep feature\nrepresentations. This paper tries to understand transferability from the\nperspectives of improved generalization, optimization and the feasibility of\ntransferability. We demonstrate that 1) Transferred models tend to find flatter\nminima, since their weight matrices stay close to the original flat region of\npretrained parameters when transferred to a similar target dataset; 2)\nTransferred representations make the loss landscape more favorable with\nimproved Lipschitzness, which accelerates and stabilizes training\nsubstantially. The improvement largely attributes to the fact that the\nprincipal component of gradient is suppressed in the pretrained parameters,\nthus stabilizing the magnitude of gradient in back-propagation. 3) The\nfeasibility of transferability is related to the similarity of both input and\nlabel. And a surprising discovery is that the feasibility is also impacted by\nthe training stages in that the transferability first increases during\ntraining, and then declines. We further provide a theoretical analysis to\nverify our observations.",
    "While the conditional sequence modeling with the transformer architecture has\ndemonstrated its effectiveness in dealing with offline reinforcement learning\n(RL) tasks, it is struggle to handle out-of-distribution states and actions.\nExisting work attempts to address this issue by data augmentation with the\nlearned policy or adding extra constraints with the value-based RL algorithm.\nHowever, these studies still fail to overcome the following challenges: (1)\ninsufficiently utilizing the historical temporal information among inter-steps,\n(2) overlooking the local intrastep relationships among return-to-gos (RTGs),\nstates, and actions, (3) overfitting suboptimal trajectories with noisy labels.\nTo address these challenges, we propose Decision Mamba (DM), a novel\nmulti-grained state space model (SSM) with a self-evolving policy learning\nstrategy. DM explicitly models the historical hidden state to extract the\ntemporal information by using the mamba architecture. To capture the\nrelationship among RTG-state-action triplets, a fine-grained SSM module is\ndesigned and integrated into the original coarse-grained SSM in mamba,\nresulting in a novel mamba architecture tailored for offline RL. Finally, to\nmitigate the overfitting issue on noisy trajectories, a self-evolving policy is\nproposed by using progressive regularization. The policy evolves by using its\nown past knowledge to refine the suboptimal actions, thus enhancing its\nrobustness on noisy demonstrations. Extensive experiments on various tasks show\nthat DM outperforms other baselines substantially.",
    "This paper explores using a Long short-term memory (LSTM) based sequence\nautoencoder to learn interesting features for detecting surveillance aircraft\nusing ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic\nDependent Surveillance - Broadcast) data to ground receivers. The ability of\nLSTM networks to model varying length time series data and remember\ndependencies that span across events makes it an ideal candidate for\nimplementing a sequence autoencoder for ADS-B data because of its possible\nvariable length time series, irregular sampling and dependencies that span\nacross events.",
    "Successful analytics solutions that provide valuable insights often hinge on\nthe connection of various data sources. While it is often feasible to generate\nlarger data pools within organizations, the application of analytics within\n(inter-organizational) business networks is still severely constrained. As data\nis distributed across several legal units, potentially even across countries,\nthe fear of disclosing sensitive information as well as the sheer volume of the\ndata that would need to be exchanged are key inhibitors for the creation of\neffective system-wide solutions -- all while still reaching superior prediction\nperformance. In this work, we propose a meta machine learning method that deals\nwith these obstacles to enable comprehensive analyses within a business\nnetwork. We follow a design science research approach and evaluate our method\nwith respect to feasibility and performance in an industrial use case. First,\nwe show that it is feasible to perform network-wide analyses that preserve data\nconfidentiality as well as limit data transfer volume. Second, we demonstrate\nthat our method outperforms a conventional isolated analysis and even gets\nclose to a (hypothetical) scenario where all data could be shared within the\nnetwork. Thus, we provide a fundamental contribution for making business\nnetworks more effective, as we remove a key obstacle to tap the huge potential\nof learning from data that is scattered throughout the network.",
    "Using neural networks in practical settings would benefit from the ability of\nthe networks to learn new tasks throughout their lifetimes without forgetting\nthe previous tasks. This ability is limited in the current deep neural networks\nby a problem called catastrophic forgetting, where training on new tasks tends\nto severely degrade performance on previous tasks. One way to lessen the impact\nof the forgetting problem is to constrain parameters that are important to\nprevious tasks to stay close to the optimal parameters. Recently, multiple\ncompetitive approaches for computing the importance of the parameters with\nrespect to the previous tasks have been presented. In this paper, we propose a\nlearning to optimize algorithm for mitigating catastrophic forgetting. Instead\nof trying to formulate a new constraint function ourselves, we propose to train\nanother neural network to predict parameter update steps that respect the\nimportance of parameters to the previous tasks. In the proposed meta-training\nscheme, the update predictor is trained to minimize loss on a combination of\ncurrent and past tasks. We show experimentally that the proposed approach works\nin the continual learning setting.",
    "Classic algorithms and machine learning systems like neural networks are both\nabundant in everyday life. While classic computer science algorithms are\nsuitable for precise execution of exactly defined tasks such as finding the\nshortest path in a large graph, neural networks allow learning from data to\npredict the most likely answer in more complex tasks such as image\nclassification, which cannot be reduced to an exact algorithm. To get the best\nof both worlds, this thesis explores combining both concepts leading to more\nrobust, better performing, more interpretable, more computationally efficient,\nand more data efficient architectures. The thesis formalizes the idea of\nalgorithmic supervision, which allows a neural network to learn from or in\nconjunction with an algorithm. When integrating an algorithm into a neural\narchitecture, it is important that the algorithm is differentiable such that\nthe architecture can be trained end-to-end and gradients can be propagated back\nthrough the algorithm in a meaningful way. To make algorithms differentiable,\nthis thesis proposes a general method for continuously relaxing algorithms by\nperturbing variables and approximating the expectation value in closed form,\ni.e., without sampling. In addition, this thesis proposes differentiable\nalgorithms, such as differentiable sorting networks, differentiable renderers,\nand differentiable logic gate networks. Finally, this thesis presents\nalternative training strategies for learning with algorithms.",
    "We consider the problem of representing collective behavior of large\npopulations and predicting the evolution of a population distribution over a\ndiscrete state space. A discrete time mean field game (MFG) is motivated as an\ninterpretable model founded on game theory for understanding the aggregate\neffect of individual actions and predicting the temporal evolution of\npopulation distributions. We achieve a synthesis of MFG and Markov decision\nprocesses (MDP) by showing that a special MFG is reducible to an MDP. This\nenables us to broaden the scope of mean field game theory and infer MFG models\nof large real-world systems via deep inverse reinforcement learning. Our method\nlearns both the reward function and forward dynamics of an MFG from real data,\nand we report the first empirical test of a mean field game model of a\nreal-world social media population.",
    "Semi-supervised learning (SSL) plays an increasingly important role in the\nbig data era because a large number of unlabeled samples can be used\neffectively to improve the performance of the classifier. Semi-supervised\nsupport vector machine (S$^3$VM) is one of the most appealing methods for SSL,\nbut scaling up S$^3$VM for kernel learning is still an open problem. Recently,\na doubly stochastic gradient (DSG) algorithm has been proposed to achieve\nefficient and scalable training for kernel methods. However, the algorithm and\ntheoretical analysis of DSG are developed based on the convexity assumption\nwhich makes them incompetent for non-convex problems such as S$^3$VM. To\naddress this problem, in this paper, we propose a triply stochastic gradient\nalgorithm for S$^3$VM, called TSGS$^3$VM. Specifically, to handle two types of\ndata instances involved in S$^3$VM, TSGS$^3$VM samples a labeled instance and\nan unlabeled instance as well with the random features in each iteration to\ncompute a triply stochastic gradient. We use the approximated gradient to\nupdate the solution. More importantly, we establish new theoretic analysis for\nTSGS$^3$VM which guarantees that TSGS$^3$VM can converge to a stationary point.\nExtensive experimental results on a variety of datasets demonstrate that\nTSGS$^3$VM is much more efficient and scalable than existing S$^3$VM\nalgorithms.",
    "The mobile gaming industry, particularly the free-to-play sector, has been\naround for more than a decade, yet it still experiences rapid growth. The\nconcept of games-as-service requires game developers to pay much more attention\nto recommendations of content in their games. With recommender systems (RS),\nthe inevitable problem of bias in the data comes hand in hand. A lot of\nresearch has been done on the case of bias in RS for online retail or services,\nbut much less is available for the specific case of the game industry. Also, in\nprevious works, various debiasing techniques were tested on explicit feedback\ndatasets, while it is much more common in mobile gaming data to only have\nimplicit feedback. This case study aims to identify and categorize potential\nbias within datasets specific to model-based recommendations in mobile games,\nreview debiasing techniques in the existing literature, and assess their\neffectiveness on real-world data gathered through implicit feedback. The\neffectiveness of these methods is then evaluated based on their debiasing\nquality, data requirements, and computational demands.",
    "Clustering and classification critically rely on distance metrics that\nprovide meaningful comparisons between data points. We present mixed-integer\noptimization approaches to find optimal distance metrics that generalize the\nMahalanobis metric extensively studied in the literature. Additionally, we\ngeneralize and improve upon leading methods by removing reliance on\npre-designated \"target neighbors,\" \"triplets,\" and \"similarity pairs.\" Another\nsalient feature of our method is its ability to enable active learning by\nrecommending precise regions to sample after an optimal metric is computed to\nimprove classification performance. This targeted acquisition can significantly\nreduce computational burden by ensuring training data completeness,\nrepresentativeness, and economy. We demonstrate classification and\ncomputational performance of the algorithms through several simple and\nintuitive examples, followed by results on real image and medical datasets.",
    "Classifiers that can be implemented on chip with minimal computational and\nmemory resources are essential for edge computing in emerging applications such\nas medical and IoT devices. This paper introduces a machine learning model\nbased on oblique decision trees to enable resource-efficient classification on\na neural implant. By integrating model compression with probabilistic routing\nand implementing cost-aware learning, our proposed model could significantly\nreduce the memory and hardware cost compared to state-of-the-art models, while\nmaintaining the classification accuracy. We trained the resource-efficient\noblique tree with power-efficient regularization (ResOT-PE) on three neural\nclassification tasks to evaluate the performance, memory, and hardware\nrequirements. On seizure detection task, we were able to reduce the model size\nby 3.4X and the feature extraction cost by 14.6X compared to the ensemble of\nboosted trees, using the intracranial EEG from 10 epilepsy patients. In a\nsecond experiment, we tested the ResOT-PE model on tremor detection for\nParkinson's disease, using the local field potentials from 12 patients\nimplanted with a deep-brain stimulation (DBS) device. We achieved a comparable\nclassification performance as the state-of-the-art boosted tree ensemble, while\nreducing the model size and feature extraction cost by 10.6X and 6.8X,\nrespectively. We also tested on a 6-class finger movement detection task using\nECoG recordings from 9 subjects, reducing the model size by 17.6X and feature\ncomputation cost by 5.1X. The proposed model can enable a low-power and\nmemory-efficient implementation of classifiers for real-time neurological\ndisease detection and motor decoding.",
    "The probabilistic classification vector machine (PCVM) synthesizes the\nadvantages of both the support vector machine and the relevant vector machine,\ndelivering a sparse Bayesian solution to classification problems. However, the\nPCVM is currently only applicable to binary cases. Extending the PCVM to\nmulti-class cases via heuristic voting strategies such as one-vs-rest or\none-vs-one often results in a dilemma where classifiers make contradictory\npredictions, and those strategies might lose the benefits of probabilistic\noutputs. To overcome this problem, we extend the PCVM and propose a multi-class\nprobabilistic classification vector machine (mPCVM). Two learning algorithms,\ni.e., one top-down algorithm and one bottom-up algorithm, have been implemented\nin the mPCVM. The top-down algorithm obtains the maximum a posteriori (MAP)\npoint estimates of the parameters based on an expectation-maximization\nalgorithm, and the bottom-up algorithm is an incremental paradigm by maximizing\nthe marginal likelihood. The superior performance of the mPCVMs, especially\nwhen the investigated problem has a large number of classes, is extensively\nevaluated on synthetic and benchmark data sets.",
    "We propose a novel output layer activation function, which we name ASTra\n(Asymmetric Sigmoid Transfer function), which makes the classification of\nminority examples, in scenarios of high imbalance, more tractable. We combine\nthis with a loss function that helps to effectively target minority\nmisclassification. These two methods can be used together or separately, with\ntheir combination recommended for the most severely imbalanced cases. The\nproposed approach is tested on datasets with IRs from 588.24 to 4000 and very\nfew minority examples (in some datasets, as few as five). Results using neural\nnetworks with from two to 12 hidden units are demonstrated to be comparable to,\nor better than, equivalent results obtained in a recent study that deployed a\nwide range of complex, hybrid data-level ensemble classifiers.",
    "Data-driven black-box model-based optimization (MBO) problems arise in a\ngreat number of practical application scenarios, where the goal is to find a\ndesign over the whole space maximizing a black-box target function based on a\nstatic offline dataset. In this work, we consider a more general but\nchallenging MBO setting, named constrained MBO (CoMBO), where only part of the\ndesign space can be optimized while the rest is constrained by the environment.\nA new challenge arising from CoMBO is that most observed designs that satisfy\nthe constraints are mediocre in evaluation. Therefore, we focus on optimizing\nthese mediocre designs in the offline dataset while maintaining the given\nconstraints rather than further boosting the best observed design in the\ntraditional MBO setting. We propose retrieval-enhanced offline model-based\noptimization (ROMO), a new derivable forward approach that retrieves the\noffline dataset and aggregates relevant samples to provide a trusted\nprediction, and use it for gradient-based optimization. ROMO is simple to\nimplement and outperforms state-of-the-art approaches in the CoMBO setting.\nEmpirically, we conduct experiments on a synthetic Hartmann (3D) function\ndataset, an industrial CIO dataset, and a suite of modified tasks in the\nDesign-Bench benchmark. Results show that ROMO performs well in a wide range of\nconstrained optimization tasks.",
    "Large language models (LLMs) have demonstrated their prowess in generating\nsynthetic text and images; however, their potential for generating tabular data\n-- arguably the most common data type in business and scientific applications\n-- is largely underexplored. This paper demonstrates that LLMs, used as-is, or\nafter traditional fine-tuning, are severely inadequate as synthetic table\ngenerators. Due to the autoregressive nature of LLMs, fine-tuning with random\norder permutation runs counter to the importance of modeling functional\ndependencies, and renders LLMs unable to model conditional mixtures of\ndistributions (key to capturing real world constraints). We showcase how LLMs\ncan be made to overcome some of these deficiencies by making them\npermutation-aware.",
    "Solving the Goal-Conditioned Reward Sparse (GCRS) task is a challenging\nreinforcement learning problem due to the sparsity of reward signals. In this\nwork, we propose a new formulation of GCRS tasks from the perspective of the\ndrifted random walk on the state space, and design a novel method called\nEvolutionary Stochastic Policy Distillation (ESPD) to solve them based on the\ninsight of reducing the First Hitting Time of the stochastic process. As a\nself-imitate approach, ESPD enables a target policy to learn from a series of\nits stochastic variants through the technique of policy distillation (PD). The\nlearning mechanism of ESPD can be considered as an Evolution Strategy (ES) that\napplies perturbations upon policy directly on the action space, with a SELECT\nfunction to check the superiority of stochastic variants and then use PD to\nupdate the policy. The experiments based on the MuJoCo robotics control suite\nshow the high learning efficiency of the proposed method.",
    "The focus in machine learning has branched beyond training classifiers on a\nsingle task to investigating how previously acquired knowledge in a source\ndomain can be leveraged to facilitate learning in a related target domain,\nknown as inductive transfer learning. Three active lines of research have\nindependently explored transfer learning using neural networks. In weight\ntransfer, a model trained on the source domain is used as an initialization\npoint for a network to be trained on the target domain. In deep metric\nlearning, the source domain is used to construct an embedding that captures\nclass structure in both the source and target domains. In few-shot learning,\nthe focus is on generalizing well in the target domain based on a limited\nnumber of labeled examples. We compare state-of-the-art methods from these\nthree paradigms and also explore hybrid adapted-embedding methods that use\nlimited target-domain data to fine tune embeddings constructed from\nsource-domain data. We conduct a systematic comparison of methods in a variety\nof domains, varying the number of labeled instances available in the target\ndomain ($k$), as well as the number of target-domain classes. We reach three\nprincipal conclusions: (1) Deep embeddings are far superior, compared to weight\ntransfer, as a starting point for inter-domain transfer or model re-use (2) Our\nhybrid methods robustly outperform every few-shot learning and every deep\nmetric learning method previously proposed, with a mean error reduction of 34%\nover state-of-the-art. (3) Among loss functions for discovering embeddings, the\nhistogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results\nwill motivate a unification of research in weight transfer, deep metric\nlearning, and few-shot learning.",
    "Mining data streams is a challenge per se. It must be ready to deal with an\nenormous amount of data and with problems not present in batch machine\nlearning, such as concept drift. Therefore, applying a batch-designed\ntechnique, such as dynamic selection of classifiers (DCS) also presents a\nchallenge. The dynamic characteristic of ensembles that deal with streams\npresents barriers to the application of traditional DCS techniques in such\nclassifiers. scikit-dyn2sel is an open-source python library tailored for\ndynamic selection techniques in streaming data. scikit-dyn2sel's development\nfollows code quality and testing standards, including PEP8 compliance and\nautomated high test coverage using codecov.io and circleci.com. Source code,\ndocumentation, and examples are made available on GitHub at\nhttps://github.com/luccaportes/Scikit-DYN2SEL.",
    "We describe novel subgradient methods for a broad class of matrix\noptimization problems involving nuclear norm regularization. Unlike existing\napproaches, our method executes very cheap iterations by combining low-rank\nstochastic subgradients with efficient incremental SVD updates, made possible\nby highly optimized and parallelizable dense linear algebra operations on small\nmatrices. Our practical algorithms always maintain a low-rank factorization of\niterates that can be conveniently held in memory and efficiently multiplied to\ngenerate predictions in matrix completion settings. Empirical comparisons\nconfirm that our approach is highly competitive with several recently proposed\nstate-of-the-art solvers for such problems.",
    "Semi-supervised multi-label feature selection has recently been developed to\nsolve the curse of dimensionality problem in high-dimensional multi-label data\nwith certain samples missing labels. Although many efforts have been made, most\nexisting methods use a predefined graph approach to capture the sample\nsimilarity or the label correlation. In this manner, the presence of noise and\noutliers within the original feature space can undermine the reliability of the\nresulting sample similarity graph. It also fails to precisely depict the label\ncorrelation due to the existence of unknown labels. Besides, these methods only\nconsider the discriminative power of selected features, while neglecting their\nredundancy. In this paper, we propose an Adaptive Collaborative Correlation\nlEarning-based Semi-Supervised Multi-label Feature Selection (Access-MFS)\nmethod to address these issues. Specifically, a generalized regression model\nequipped with an extended uncorrelated constraint is introduced to select\ndiscriminative yet irrelevant features and maintain consistency between\npredicted and ground-truth labels in labeled data, simultaneously. Then, the\ninstance correlation and label correlation are integrated into the proposed\nregression model to adaptively learn both the sample similarity graph and the\nlabel similarity graph, which mutually enhance feature selection performance.\nExtensive experimental results demonstrate the superiority of the proposed\nAccess-MFS over other state-of-the-art methods.",
    "Federated learning is a technique that enables a centralized server to learn\nfrom distributed clients via communications without accessing the client local\ndata. However, existing federated learning works mainly focus on a single task\nscenario with static data. In this paper, we introduce the problem of continual\nfederated learning, where clients incrementally learn new tasks and history\ndata cannot be stored due to certain reasons, such as limited storage and data\nretention policy. Generative replay based methods are effective for continual\nlearning without storing history data, but adapting them for this setting is\nchallenging. By analyzing the behaviors of clients during training, we find\nthat the unstable training process caused by distributed training on non-IID\ndata leads to a notable performance degradation. To address this problem, we\npropose our FedCIL model with two simple but effective solutions: model\nconsolidation and consistency enforcement. Our experimental results on multiple\nbenchmark datasets demonstrate that our method significantly outperforms\nbaselines.",
    "Time Series Motif Discovery (TSMD) refers to the task of identifying patterns\nthat occur multiple times (possibly with minor variations) in a time series.\nAll existing methods for TSMD have one or more of the following limitations:\nthey only look for the two most similar occurrences of a pattern; they only\nlook for patterns of a pre-specified, fixed length; they cannot handle\nvariability along the time axis; and they only handle univariate time series.\nIn this paper, we present a new method, LoCoMotif, that has none of these\nlimitations. The method is motivated by a concrete use case from physiotherapy.\nWe demonstrate the value of the proposed method on this use case. We also\nintroduce a new quantitative evaluation metric for motif discovery, and\nbenchmark data for comparing TSMD methods. LoCoMotif substantially outperforms\nthe existing methods, on top of being more broadly applicable.",
    "The dominant paradigm for learning on graph-structured data is message\npassing. Despite being a strong inductive bias, the local message passing\nmechanism suffers from pathological issues such as over-smoothing,\nover-squashing, and limited node-level expressivity. To address these\nlimitations we propose Bundle Neural Networks (BuNN), a new type of GNN that\noperates via message diffusion over flat vector bundles - structures analogous\nto connections on Riemannian manifolds that augment the graph by assigning to\neach node a vector space and an orthogonal map. A BuNN layer evolves the\nfeatures according to a diffusion-type partial differential equation. When\ndiscretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a\nrecently proposed MPNN capable of mitigating over-smoothing. The continuous\nnature of message diffusion enables BuNNs to operate on larger scales of the\ngraph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN\ncan approximate any feature transformation over nodes on any (potentially\ninfinite) family of graphs given injective positional encodings, resulting in\nuniversal node-level expressivity. We support our theory via synthetic\nexperiments and showcase the strong empirical performance of BuNNs over a range\nof real-world tasks, achieving state-of-the-art results on several standard\nbenchmarks in transductive and inductive settings.",
    "The message-passing scheme is the core of graph representation learning.\nWhile most existing message-passing graph neural networks (MPNNs) are\npermutation-invariant in graph-level representation learning and\npermutation-equivariant in node- and edge-level representation learning, their\nexpressive power is commonly limited by the 1-Weisfeiler-Lehman (1-WL) graph\nisomorphism test. Recently proposed expressive graph neural networks (GNNs)\nwith specially designed complex message-passing mechanisms are not practical.\nTo bridge the gap, we propose a plug-in Equivariant Distance ENcoding (EDEN)\nfor MPNNs. EDEN is derived from a series of interpretable transformations on\nthe graph's distance matrix. We theoretically prove that EDEN is\npermutation-equivariant for all level graph representation learning, and we\nempirically illustrate that EDEN's expressive power can reach up to the 3-WL\ntest. Extensive experiments on real-world datasets show that combining EDEN\nwith conventional GNNs surpasses recent advanced GNNs.",
    "We consider the Domain Adaptation problem, also known as the covariate shift\nproblem, where the distributions that generate the training and test data\ndiffer while retaining the same labeling function. This problem occurs across a\nlarge range of practical applications, and is related to the more general\nchallenge of transfer learning. Most recent work on the topic focuses on\noptimization techniques that are specific to an algorithm or practical use case\nrather than a more general approach. The sparse literature attempting to\nprovide general bounds seems to suggest that efficient learning even under\nstrong assumptions is not possible for covariate shift. Our main contribution\nis to recontextualize these results by showing that any Probably Approximately\nCorrect (PAC) learnable concept class is still PAC learnable under covariate\nshift conditions with only a polynomial increase in the number of training\nsamples. This approach essentially demonstrates that the Domain Adaptation\nlearning problem is as hard as the underlying PAC learning problem, provided\nsome conditions over the training and test distributions. We also present\nbounds for the rejection sampling algorithm, justifying it as a solution to the\nDomain Adaptation problem in certain scenarios.",
    "Computational imaging methods that can exploit multiple modalities have the\npotential to enhance the capabilities of traditional sensing systems. In this\npaper, we propose a new method that reconstructs multimodal images from their\nlinear measurements by exploiting redundancies across different modalities. Our\nmethod combines a convolutional group-sparse representation of images with\ntotal variation (TV) regularization for high-quality multimodal imaging. We\ndevelop an online algorithm that enables the unsupervised learning of\nconvolutional dictionaries on large-scale datasets that are typical in such\napplications. We illustrate the benefit of our approach in the context of joint\nintensity-depth imaging.",
    "Camouflaged object detection (COD) is a challenging task due to the low\nboundary contrast between the object and its surroundings. In addition, the\nappearance of camouflaged objects varies significantly, e.g., object size and\nshape, aggravating the difficulties of accurate COD. In this paper, we propose\na novel Context-aware Cross-level Fusion Network (C2F-Net) to address the\nchallenging COD task. Specifically, we propose an Attention-induced Cross-level\nFusion Module (ACFM) to integrate the multi-level features with informative\nattention coefficients. The fused features are then fed to the proposed\nDual-branch Global Context Module (DGCM), which yields multi-scale feature\nrepresentations for exploiting rich global context information. In C2F-Net, the\ntwo modules are conducted on high-level features using a cascaded manner.\nExtensive experiments on three widely used benchmark datasets demonstrate that\nour C2F-Net is an effective COD model and outperforms state-of-the-art models\nremarkably. Our code is publicly available at:\nhttps://github.com/thograce/C2FNet.",
    "Nowadays, driven by the increasing concern on diet and health, food computing\nhas attracted enormous attention from both industry and research community. One\nof the most popular research topics in this domain is Food Retrieval, due to\nits profound influence on health-oriented applications. In this paper, we focus\non the task of cross-modal retrieval between food images and cooking recipes.\nWe present Modality-Consistent Embedding Network (MCEN) that learns\nmodality-invariant representations by projecting images and texts to the same\nembedding space. To capture the latent alignments between modalities, we\nincorporate stochastic latent variables to explicitly exploit the interactions\nbetween textual and visual features. Importantly, our method learns the\ncross-modal alignments during training but computes embeddings of different\nmodalities independently at inference time for the sake of efficiency.\nExtensive experimental results clearly demonstrate that the proposed MCEN\noutperforms all existing approaches on the benchmark Recipe1M dataset and\nrequires less computational cost.",
    "The ubiquitous multi-camera setup on modern autonomous vehicles provides an\nopportunity to construct surround-view depth. Existing methods, however, either\nperform independent monocular depth estimations on each camera or rely on\ncomputationally heavy self attention mechanisms. In this paper, we propose a\nnovel guided attention architecture, EGA-Depth, which can improve both the\nefficiency and accuracy of self-supervised multi-camera depth estimation. More\nspecifically, for each camera, we use its perspective view as the query to\ncross-reference its neighboring views to derive informative features for this\ncamera view. This allows the model to perform attention only across views with\nconsiderable overlaps and avoid the costly computations of standard\nself-attention. Given its efficiency, EGA-Depth enables us to exploit\nhigher-resolution visual features, leading to improved accuracy. Furthermore,\nEGA-Depth can incorporate more frames from previous time steps as it scales\nlinearly w.r.t. the number of views and frames. Extensive experiments on two\nchallenging autonomous driving benchmarks nuScenes and DDAD demonstrate the\nefficacy of our proposed EGA-Depth and show that it achieves the new\nstate-of-the-art in self-supervised multi-camera depth estimation.",
    "The paper presents a Traffic Sign Recognition (TSR) system, which can fast\nand accurately recognize traffic signs of different sizes in images. The system\nconsists of two well-designed Convolutional Neural Networks (CNNs), one for\nregion proposals of traffic signs and one for classification of each region. In\nthe proposal CNN, a Fully Convolutional Network (FCN) with a dual multi-scale\narchitecture is proposed to achieve scale invariant detection. In training the\nproposal network, a modified \"Online Hard Example Mining\" (OHEM) scheme is\nadopted to suppress false positives. The classification network fuses\nmulti-scale features as representation and adopts an \"Inception\" module for\nefficiency. We evaluate the proposed TSR system and its components with\nextensive experiments. Our method obtains $99.88\\%$ precision and $96.61\\%$\nrecall on the Swedish Traffic Signs Dataset (STSD), higher than\nstate-of-the-art methods. Besides, our system is faster and more lightweight\nthan state-of-the-art deep learning networks for traffic sign recognition.",
    "Human pose estimation aims to accurately estimate a wide variety of human\nposes. However, existing datasets often follow a long-tailed distribution that\nunusual poses only occupy a small portion, which further leads to the lack of\ndiversity of rare poses. These issues result in the inferior generalization\nability of current pose estimators. In this paper, we present a simple yet\neffective data augmentation method, termed Pose Transformation (PoseTrans), to\nalleviate the aforementioned problems. Specifically, we propose Pose\nTransformation Module (PTM) to create new training samples that have diverse\nposes and adopt a pose discriminator to ensure the plausibility of the\naugmented poses. Besides, we propose Pose Clustering Module (PCM) to measure\nthe pose rarity and select the \"rarest\" poses to help balance the long-tailed\ndistribution. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of our method, especially on rare poses. Also, our method is\nefficient and simple to implement, which can be easily integrated into the\ntraining pipeline of existing pose estimation models.",
    "We present a convolutional approach to reflection symmetry detection in 2D.\nOur model, built on the products of complex-valued wavelet convolutions,\nsimplifies previous edge-based pairwise methods. Being parameter-centered, as\nopposed to feature-centered, it has certain computational advantages when the\nobject sizes are known a priori, as demonstrated in an ellipse detection\napplication. The method outperforms the best-performing algorithm on the CVPR\n2013 Symmetry Detection Competition Database in the single-symmetry case. Code\nand a new database for 2D symmetry detection is available.",
    "Generic visual tracking is difficult due to many challenge factors (e.g.,\nocclusion, blur, etc.). Each of these factors may cause serious problems for a\ntracking algorithm, and when they work together can make things even more\ncomplicated. Despite a great amount of efforts devoted to understanding the\nbehavior of tracking algorithms, reliable and quantifiable ways for studying\nthe per factor tracking behavior remain barely available. Addressing this\nissue, in this paper we contribute to the community a tracking diagnosis\ntoolkit, TracKlinic, for diagnosis of challenge factors of tracking algorithms.\n  TracKlinic consists of two novel components focusing on the data and analysis\naspects, respectively. For the data component, we carefully prepare a set of\n2,390 annotated videos, each involving one and only one major challenge factor.\nWhen analyzing an algorithm for a specific challenge factor, such\none-factor-per-sequence rule greatly inhibits the disturbance from other\nfactors and consequently leads to more faithful analysis. For the analysis\ncomponent, given the tracking results on all sequences, it investigates the\nbehavior of the tracker under each individual factor and generates the report\nautomatically. With TracKlinic, a thorough study is conducted on ten\nstate-of-the-art trackers on nine challenge factors (including two compound\nones). The results suggest that, heavy shape variation and occlusion are the\ntwo most challenging factors faced by most trackers. Besides, out-of-view,\nthough does not happen frequently, is often fatal. By sharing TracKlinic, we\nexpect to make it much easier for diagnosing tracking algorithms, and to thus\nfacilitate developing better ones.",
    "Recently, directly detecting 3D objects from 3D point clouds has received\nincreasing attention. To extract object representation from an irregular point\ncloud, existing methods usually take a point grouping step to assign the points\nto an object candidate so that a PointNet-like network could be used to derive\nobject features from the grouped points. However, the inaccurate point\nassignments caused by the hand-crafted grouping scheme decrease the performance\nof 3D object detection.\n  In this paper, we present a simple yet effective method for directly\ndetecting 3D objects from the 3D point cloud. Instead of grouping local points\nto each object candidate, our method computes the feature of an object from all\nthe points in the point cloud with the help of an attention mechanism in the\nTransformers \\cite{vaswani2017attention}, where the contribution of each point\nis automatically learned in the network training. With an improved attention\nstacking scheme, our method fuses object features in different stages and\ngenerates more accurate object detection results. With few bells and whistles,\nthe proposed method achieves state-of-the-art 3D object detection performance\non two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models\nare publicly available at \\url{https://github.com/zeliu98/Group-Free-3D}",
    "Hyperspectral cameras can provide unique spectral signatures for consistently\ndistinguishing materials that can be used to solve surveillance tasks. In this\npaper, we propose a novel real-time hyperspectral likelihood maps-aided\ntracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving\nobject tracking system generally consists of registration, object detection,\nand tracking modules. We focus on the target detection part and remove the\nnecessity to build any offline classifiers and tune a large amount of\nhyperparameters, instead learning a generative target model in an online manner\nfor hyperspectral channels ranging from visible to infrared wavelengths. The\nkey idea is that, our adaptive fusion method can combine likelihood maps from\nmultiple bands of hyperspectral imagery into one single more distinctive\nrepresentation increasing the margin between mean value of foreground and\nbackground pixels in the fused map. Experimental results show that the HLT not\nonly outperforms all established fusion methods but is on par with the current\nstate-of-the-art hyperspectral target tracking frameworks.",
    "Scarcity of labeled data has motivated the development of semi-supervised\nlearning methods, which learn from large portions of unlabeled data alongside a\nfew labeled samples. Consistency Regularization between model's predictions\nunder different input perturbations, particularly has shown to provide\nstate-of-the art results in a semi-supervised framework. However, most of these\nmethod have been limited to classification and segmentation applications. We\npropose Transformation Consistency Regularization, which delves into a more\nchallenging setting of image-to-image translation, which remains unexplored by\nsemi-supervised algorithms. The method introduces a diverse set of geometric\ntransformations and enforces the model's predictions for unlabeled data to be\ninvariant to those transformations. We evaluate the efficacy of our algorithm\non three different applications: image colorization, denoising and\nsuper-resolution. Our method is significantly data efficient, requiring only\naround 10 - 20% of labeled samples to achieve similar image reconstructions to\nits fully-supervised counterpart. Furthermore, we show the effectiveness of our\nmethod in video processing applications, where knowledge from a few frames can\nbe leveraged to enhance the quality of the rest of the movie.",
    "Structured illumination microscopy (SIM) is a very important super-resolution\nmicroscopy technique, which provides high speed super-resolution with about\ntwo-fold spatial resolution enhancement. Several attempts aimed at improving\nthe performance of SIM reconstruction algorithm have been reported. However,\nmost of these highlight only one specific aspect of the SIM reconstruction --\nsuch as the determination of the illumination pattern phase shift accurately --\nwhereas other key elements -- such as determination of modulation factor,\nestimation of object power spectrum, Wiener filtering frequency components with\ninclusion of object power spectrum information, translocating and the merging\nof the overlapping frequency components -- are usually glossed over\nsuperficially. In addition, most of the work reported lie scattered throughout\nthe literature and a comprehensive review of the theoretical background is\nfound lacking. The purpose of the present work is two-fold: 1) to collect the\nessential theoretical details of SIM algorithm at one place, thereby making\nthem readily accessible to readers for the first time; and 2) to provide an\nopen source SIM reconstruction code (named OpenSIM), which enables users to\ninteractively vary the code parameters and study it's effect on reconstructed\nSIM image.",
    "Current talking face generation methods mainly focus on speech-lip\nsynchronization. However, insufficient investigation on the facial talking\nstyle leads to a lifeless and monotonous avatar. Most previous works fail to\nimitate expressive styles from arbitrary video prompts and ensure the\nauthenticity of the generated video. This paper proposes an unsupervised\nvariational style transfer model (VAST) to vivify the neutral photo-realistic\navatars. Our model consists of three key components: a style encoder that\nextracts facial style representations from the given video prompts; a hybrid\nfacial expression decoder to model accurate speech-related movements; a\nvariational style enhancer that enhances the style space to be highly\nexpressive and meaningful. With our essential designs on facial style learning,\nour model is able to flexibly capture the expressive facial style from\narbitrary video prompts and transfer it onto a personalized image renderer in a\nzero-shot manner. Experimental results demonstrate the proposed approach\ncontributes to a more vivid talking avatar with higher authenticity and richer\nexpressiveness.",
    "In this work, we propose to detect the iris and periocular regions\nsimultaneously using coarse annotations and two well-known object detectors:\nYOLOv2 and Faster R-CNN. We believe coarse annotations can be used in\nrecognition systems based on the iris and periocular regions, given the much\nsmaller engineering effort required to manually annotate the training images.\nWe manually made coarse annotations of the iris and periocular regions (122K\nimages from the visible (VIS) spectrum and 38K images from the near-infrared\n(NIR) spectrum). The iris annotations in the NIR databases were generated\nsemi-automatically by first applying an iris segmentation CNN and then\nperforming a manual inspection. These annotations were made for 11 well-known\npublic databases (3 NIR and 8 VIS) designed for the iris-based recognition\nproblem and are publicly available to the research community. Experimenting our\nproposal on these databases, we highlight two results. First, the Faster R-CNN\n+ Feature Pyramid Network (FPN) model reported an Intersection over Union (IoU)\nhigher than YOLOv2 (91.86% vs 85.30%). Second, the detection of the iris and\nperiocular regions being performed simultaneously is as accurate as performed\nseparately, but with a lower computational cost, i.e., two tasks were carried\nout at the cost of one.",
    "We tackle the problem of getting a full 6-DOF pose estimation of a query\nimage inside a given point cloud. This technical report re-evaluates the\nalgorithms proposed by Y. Li et al. \"Worldwide Pose Estimation using 3D Point\nCloud\". Our code computes poses from 3 or 4 points, with both known and unknown\nfocal length. The results can easily be displayed and analyzed with Meshlab. We\nfound both advantages and shortcomings of the methods proposed. Furthermore,\nadditional priors and parameters for point selection, RANSAC and pose quality\nestimate (inlier test) are proposed and applied.",
    "Recently, there has been a growing interest in constructing deep learning\nschemes for Low-Light Vision (LLV). Existing techniques primarily focus on\ndesigning task-specific and data-dependent vision models on the standard RGB\ndomain, which inherently contain latent data associations. In this study, we\npropose a generic low-light vision solution by introducing a generative block\nto convert data from the RAW to the RGB domain. This novel approach connects\ndiverse vision problems by explicitly depicting data generation, which is the\nfirst in the field. To precisely characterize the latent correspondence between\nthe generative procedure and the vision task, we establish a bilevel model with\nthe parameters of the generative block defined as the upper level and the\nparameters of the vision task defined as the lower level. We further develop\ntwo types of learning strategies targeting different goals, namely low cost and\nhigh accuracy, to acquire a new bilevel generative learning paradigm. The\ngenerative blocks embrace a strong generalization ability in other low-light\nvision tasks through the bilevel optimization on enhancement tasks. Extensive\nexperimental evaluations on three representative low-light vision tasks, namely\nenhancement, detection, and segmentation, fully demonstrate the superiority of\nour proposed approach. The code will be available at\nhttps://github.com/Yingchi1998/BGL.",
    "Memorability is considered to be an important characteristic of visual\ncontent, whereas for advertisement and educational purposes it is often\ncrucial. Despite numerous studies on understanding and predicting image\nmemorability, there are almost no achievements in memorability modification. In\nthis work, we study two approaches to image editing - GAN and classical image\nprocessing - and show their impact on memorability. The visual features which\ninfluence memorability directly stay unknown till now, hence it is impossible\nto control it manually. As a solution, we let GAN learn it deeply using labeled\ndata, and then use it for conditional generation of new images. By analogy with\nalgorithms which edit facial attributes, we consider memorability as yet\nanother attribute and operate with it in the same way. Obtained data is also\ninteresting for analysis, simply because there are no real-world examples of\nsuccessful change of image memorability while preserving its other attributes.\nWe believe this may give many new answers to the question \"what makes an image\nmemorable?\" Apart from that we also study the influence of conventional\nphoto-editing tools (Photoshop, Instagram, etc.) used daily by a wide audience\non memorability. In this case, we start from real practical methods and study\nit using statistics and recent advances in memorability prediction.\nPhotographers, designers, and advertisers will benefit from the results of this\nstudy directly.",
    "In this paper, we propose a pipeline for multi-target visual tracking under\nmulti-camera system. For multi-camera system tracking problem, efficient data\nassociation across cameras, and at the same time, across frames becomes more\nimportant than single-camera system tracking. However, most of the multi-camera\ntracking algorithms emphasis on single camera across frame data association.\nThus in our work, we model our tracking problem as a global graph, and adopt\nGeneralized Maximum Multi Clique optimization problem as our core algorithm to\ntake both across frame and across camera data correlation into account all\ntogether. Furthermore, in order to compute good similarity scores as the input\nof our graph model, we extract both appearance and dynamic motion similarities.\nFor appearance feature, Local Maximal Occurrence Representation(LOMO) feature\nextraction algorithm for ReID is conducted. When it comes to capturing the\ndynamic information, we build Hankel matrix for each tracklet of target and\napply rank estimation with Iterative Hankel Total Least Squares(IHTLS)\nalgorithm to it. We evaluate our tracker on the challenging Terrace Sequences\nfrom EPFL CVLAB as well as recently published Duke MTMC dataset.",
    "In this paper, we aim to segment an image degraded by blur and Poisson noise.\nWe adopt a smoothing-and-thresholding (SaT) segmentation framework that finds a\npiecewise-smooth solution, followed by $k$-means clustering to segment the\nimage. Specifically for the image smoothing step, we replace the least-squares\nfidelity for Gaussian noise in the Mumford-Shah model with a maximum posterior\n(MAP) term to deal with Poisson noise and we incorporate the weighted\ndifference of anisotropic and isotropic total variation (AITV) as a\nregularization to promote the sparsity of image gradients. For such a nonconvex\nmodel, we develop a specific splitting scheme and utilize a proximal operator\nto apply the alternating direction method of multipliers (ADMM). Convergence\nanalysis is provided to validate the efficacy of the ADMM scheme. Numerical\nexperiments on various segmentation scenarios (grayscale/color and multiphase)\nshowcase that our proposed method outperforms a number of segmentation methods,\nincluding the original SaT.",
    "The success and generalisation of deep learning algorithms heavily depend on\nlearning good feature representations. In medical imaging this entails\nrepresenting anatomical information, as well as properties related to the\nspecific imaging setting. Anatomical information is required to perform further\nanalysis, whereas imaging information is key to disentangle scanner variability\nand potential artefacts. The ability to factorise these would allow for\ntraining algorithms only on the relevant information according to the task. To\ndate, such factorisation has not been attempted. In this paper, we propose a\nmethodology of latent space factorisation relying on the cycle-consistency\nprinciple. As an example application, we consider cardiac MR segmentation,\nwhere we separate information related to the myocardium from other features\nrelated to imaging and surrounding substructures. We demonstrate the proposed\nmethod's utility in a semi-supervised setting: we use very few labelled images\ntogether with many unlabelled images to train a myocardium segmentation neural\nnetwork. Specifically, we achieve comparable performance to fully supervised\nnetworks using a fraction of labelled images in experiments on ACDC and a\ndataset from Edinburgh Imaging Facility QMRI. Code will be made available at\nhttps://github.com/agis85/spatial_factorisation.",
    "Deep learning has shown state-of-art classification performance on datasets\nsuch as ImageNet, which contain a single object in each image. However,\nmulti-object classification is far more challenging. We present a unified\nframework which leverages the strengths of multiple machine learning methods,\nviz deep learning, probabilistic models and kernel methods to obtain\nstate-of-art performance on Microsoft COCO, consisting of non-iconic images. We\nincorporate contextual information in natural images through a conditional\nlatent tree probabilistic model (CLTM), where the object co-occurrences are\nconditioned on the extracted fc7 features from pre-trained Imagenet CNN as\ninput. We learn the CLTM tree structure using conditional pairwise\nprobabilities for object co-occurrences, estimated through kernel methods, and\nwe learn its node and edge potentials by training a new 3-layer neural network,\nwhich takes fc7 features as input. Object classification is carried out via\ninference on the learnt conditional tree model, and we obtain significant gain\nin precision-recall and F-measures on MS-COCO, especially for difficult object\ncategories. Moreover, the latent variables in the CLTM capture scene\ninformation: the images with top activations for a latent node have common\nthemes such as being a grasslands or a food scene, and on on. In addition, we\nshow that a simple k-means clustering of the inferred latent nodes alone\nsignificantly improves scene classification performance on the MIT-Indoor\ndataset, without the need for any retraining, and without using scene labels\nduring training. Thus, we present a unified framework for multi-object\nclassification and unsupervised scene understanding.",
    "Pedestrian tracking has long been considered an important problem, especially\nin security applications. Previously,many approaches have been proposed with\nvarious types of sensors. One popular method is Pedestrian Dead Reckoning(PDR)\n[1] which is based on the inertial measurement unit(IMU) sensor. However PDR is\nan integration and threshold based method, which suffers from accumulation\nerrors and low accuracy. In this paper, we propose a novel method in which the\nsensor data is fed into a deep learning model to predict the displacements and\norientations of the pedestrian. We also devise a new apparatus to collect and\nconstruct databases containing synchronized IMU sensor data and precise\nlocations measured by a LIDAR. The preliminary results are promising, and we\nplan to push this forward by collecting more data and adapting the deep\nlearning model for all general pedestrian motions.",
    "Image to image translation is the problem of transferring an image from a\nsource domain to a different (but related) target domain. We present a new\nunsupervised image to image translation technique that leverages the underlying\nsemantic information for object transfiguration and domain transfer tasks.\nSpecifically, we present a generative adversarial learning approach that\njointly translates images and labels from a source domain to a target domain.\nOur main technical contribution is an encoder-decoder based network\narchitecture that jointly encodes the image and its underlying semantics and\ntranslates both individually to the target domain. Additionally, we propose\nobject transfiguration and cross-domain semantic consistency losses that\npreserve semantic labels. Through extensive experimental evaluation, we\ndemonstrate the effectiveness of our approach as compared to the\nstate-of-the-art methods on unsupervised image-to-image translation, domain\nadaptation, and object transfiguration.",
    "We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].",
    "In computed tomography (CT), metal implants increase the inconsistencies\nbetween the measured data and the linear attenuation assumption made by\nanalytic CT reconstruction algorithms. The inconsistencies give rise to dark\nand bright bands and streaks in the reconstructed image, collectively called\nmetal artifacts. These artifacts make it difficult for radiologists to render\ncorrect diagnostic decisions. We describe a data-driven metal artifact\nreduction (MAR) algorithm for image-guided spine surgery that applies to\nscenarios in which a prior CT scan of the patient is available. We tested the\nproposed method with two clinical datasets that were both obtained during spine\nsurgery. Using the proposed method, we were not only able to remove the dark\nand bright streaks caused by the implanted screws but we also recovered the\nanatomical structures hidden by these artifacts. This results in an improved\ncapability of surgeons to confirm the correctness of the implanted pedicle\nscrew placements.",
    "We propose a filtering feature selection framework that considers subsets of\nfeatures as paths in a graph, where a node is a feature and an edge indicates\npairwise (customizable) relations among features, dealing with relevance and\nredundancy principles. By two different interpretations (exploiting properties\nof power series of matrices and relying on Markov chains fundamentals) we can\nevaluate the values of paths (i.e., feature subsets) of arbitrary lengths,\neventually go to infinite, from which we dub our framework Infinite Feature\nSelection (Inf-FS). Going to infinite allows to constrain the computational\ncomplexity of the selection process, and to rank the features in an elegant\nway, that is, considering the value of any path (subset) containing a\nparticular feature. We also propose a simple unsupervised strategy to cut the\nranking, so providing the subset of features to keep. In the experiments, we\nanalyze diverse settings with heterogeneous features, for a total of 11\nbenchmarks, comparing against 18 widely-known comparative approaches. The\nresults show that Inf-FS behaves better in almost any situation, that is, when\nthe number of features to keep are fixed a priori, or when the decision of the\nsubset cardinality is part of the process.",
    "Global optimization algorithms have shown impressive performance in\ndata-association based multi-object tracking, but handling online data remains\na difficult hurdle to overcome. In this paper, we present a hybrid data\nassociation framework with a min-cost multi-commodity network flow for robust\nonline multi-object tracking. We build local target-specific models interleaved\nwith global optimization of the optimal data association over multiple video\nframes. More specifically, in the min-cost multi-commodity network flow, the\ntarget-specific similarities are online learned to enforce the local\nconsistency for reducing the complexity of the global data association.\nMeanwhile, the global data association taking multiple video frames into\naccount alleviates irrecoverable errors caused by the local data association\nbetween adjacent frames. To ensure the efficiency of online tracking, we give\nan efficient near-optimal solution to the proposed min-cost multi-commodity\nflow problem, and provide the empirical proof of its sub-optimality. The\ncomprehensive experiments on real data demonstrate the superior tracking\nperformance of our approach in various challenging situations.",
    "Pose variation is one of the key factors which prevents the network from\nlearning a robust person re-identification (Re-ID) model. To address this\nissue, we propose a novel person pose-guided image generation method, which is\ncalled the semantic attention network. The network consists of several semantic\nattention blocks, where each block attends to preserve and update the pose code\nand the clothing textures. The introduction of the binary segmentation mask and\nthe semantic parsing is important for seamlessly stitching foreground and\nbackground in the pose-guided image generation. Compared with other methods,\nour network can characterize better body shape and keep clothing attributes,\nsimultaneously. Our synthesized image can obtain better appearance and shape\nconsistency related to the original image. Experimental results show that our\napproach is competitive with respect to both quantitative and qualitative\nresults on Market-1501 and DeepFashion. Furthermore, we conduct extensive\nevaluations by using person re-identification (Re-ID) systems trained with the\npose-transferred person based augmented data. The experiment shows that our\napproach can significantly enhance the person Re-ID accuracy.",
    "Video generation is an inherently challenging task, as it requires modeling\nrealistic temporal dynamics as well as spatial content. Existing methods\nentangle the two intrinsically different tasks of motion and content creation\nin a single generator network, but this approach struggles to simultaneously\ngenerate plausible motion and content. To im-prove motion modeling in video\ngeneration tasks, we propose a two-stream model that disentangles motion\ngeneration from content generation, called a Two-Stream Variational Adversarial\nNetwork (TwoStreamVAN). Given an action label and a noise vector, our model is\nable to create clear and consistent motion, and thus yields photorealistic\nvideos. The key idea is to progressively generate and fuse multi-scale motion\nwith its corresponding spatial content. Our model significantly outperforms\nexisting methods on the standard Weizmann Human Action, MUG Facial Expression,\nand VoxCeleb datasets, as well as our new dataset of diverse human actions with\nchallenging and complex motion. Our code is available at\nhttps://github.com/sunxm2357/TwoStreamVAN/.",
    "Pavement damage segmentation has benefited enormously from deep learning. %\nand large-scale datasets. However, few current public datasets limit the\npotential exploration of deep learning in the application of pavement damage\nsegmentation. To address this problem, this study has proposed Pavementscapes,\na large-scale dataset to develop and evaluate methods for pavement damage\nsegmentation. Pavementscapes is comprised of 4,000 images with a resolution of\n$1024 \\times 2048$, which have been recorded in the real-world pavement\ninspection projects with 15 different pavements. A total of 8,680 damage\ninstances are manually labeled with six damage classes at the pixel level. The\nstatistical study gives a thorough investigation and analysis of the proposed\ndataset. The numeral experiments propose the top-performing deep neural\nnetworks capable of segmenting pavement damages, which provides the baselines\nof the open challenge for pavement inspection. The experiment results also\nindicate the existing problems for damage segmentation using deep learning, and\nthis study provides potential solutions.",
    "The fusion techniques that utilize multiple feature sets to form new features\nthat are often more robust and contain useful information for future processing\nare referred to as feature fusion. The term data fusion is applied to the class\nof techniques used for combining decisions obtained from multiple feature sets\nto form global decisions. Feature and data fusion interchangeably represent two\nimportant classes of techniques that have proved to be of practical importance\nin a wide range of medical imaging problems",
    "This paper introduces a novel methodology that combines the multi-resolution\nfeature of the Gabor wavelet transformation (GWT) with the local interactions\nof the facial structures expressed through the Pseudo Hidden Markov model\n(PHMM). Unlike the traditional zigzag scanning method for feature extraction a\ncontinuous scanning method from top-left corner to right then top-down and\nright to left and so on until right-bottom of the image i.e. a spiral scanning\ntechnique has been proposed for better feature selection. Unlike traditional\nHMMs, the proposed PHMM does not perform the state conditional independence of\nthe visible observation sequence assumption. This is achieved via the concept\nof local structures introduced by the PHMM used to extract facial bands and\nautomatically select the most informative features of a face image. Thus, the\nlong-range dependency problem inherent to traditional HMMs has been drastically\nreduced. Again with the use of most informative pixels rather than the whole\nimage makes the proposed method reasonably faster for face recognition. This\nmethod has been successfully tested on frontal face images from the ORL, FRAV2D\nand FERET face databases where the images vary in pose, illumination,\nexpression, and scale. The FERET data set contains 2200 frontal face images of\n200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects\nand the full ORL database is considered. The results reported in this\napplication are far better than the recent and most referred systems.",
    "Learning transformation invariant representations of visual data is an\nimportant problem in computer vision. Deep convolutional networks have\ndemonstrated remarkable results for image and video classification tasks.\nHowever, they have achieved only limited success in the classification of\nimages that undergo geometric transformations. In this work we present a novel\nTransformation Invariant Graph-based Network (TIGraNet), which learns\ngraph-based features that are inherently invariant to isometric transformations\nsuch as rotation and translation of input images. In particular, images are\nrepresented as signals on graphs, which permits to replace classical\nconvolution and pooling layers in deep networks with graph spectral convolution\nand dynamic graph pooling layers that together contribute to invariance to\nisometric transformation. Our experiments show high performance on rotated and\ntranslated images from the test set compared to classical architectures that\nare very sensitive to transformations in the data. The inherent invariance\nproperties of our framework provide key advantages, such as increased\nresiliency to data variability and sustained performance with limited training\nsets. Our code is available online.",
    "Conventional object detection methods essentially suppose that the training\nand testing data are collected from a restricted target domain with expensive\nlabeling cost. For alleviating the problem of domain dependency and cumbersome\nlabeling, this paper proposes to detect objects in an unrestricted environment\nby leveraging domain knowledge trained from an auxiliary source domain with\nsufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN\n(MAF) framework for unrestricted object detection, which inherently addresses\ndomain disparity minimization for domain adaptation in feature representation.\nThe paper merits are in three-fold: 1) With the idea that object detectors\noften becomes domain incompatible when image distribution resulted domain\ndisparity appears, we propose a hierarchical domain feature alignment module,\nin which multiple adversarial domain classifier submodules for layer-wise\ndomain feature confusion are designed; 2) An information invariant scale\nreduction module (SRM) for hierarchical feature map resizing is proposed for\npromoting the training efficiency of adversarial domain adaptation; 3) In order\nto improve the domain adaptability, the aggregated proposal features with\ndetection results are feed into a proposed weighted gradient reversal layer\n(WGRL) for characterizing hard confused domain samples. We evaluate our MAF on\nunrestricted tasks, including Cityscapes, KITTI, Sim10k, etc. and the\nexperiments show the state-of-the-art performance over the existing detectors.",
    "Remote sensing image scene classification plays an important role in a wide\nrange of applications and hence has been receiving remarkable attention. During\nthe past years, significant efforts have been made to develop various datasets\nor present a variety of approaches for scene classification from remote sensing\nimages. However, a systematic review of the literature concerning datasets and\nmethods for scene classification is still lacking. In addition, almost all\nexisting datasets have a number of limitations, including the small scale of\nscene classes and the image numbers, the lack of image variations and\ndiversity, and the saturation of accuracy. These limitations severely limit the\ndevelopment of new approaches especially deep learning-based methods. This\npaper first provides a comprehensive review of the recent progress. Then, we\npropose a large-scale dataset, termed \"NWPU-RESISC45\", which is a publicly\navailable benchmark for REmote Sensing Image Scene Classification (RESISC),\ncreated by Northwestern Polytechnical University (NWPU). This dataset contains\n31,500 images, covering 45 scene classes with 700 images in each class. The\nproposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total\nimage number, (ii) holds big variations in translation, spatial resolution,\nviewpoint, object pose, illumination, background, and occlusion, and (iii) has\nhigh within-class diversity and between-class similarity. The creation of this\ndataset will enable the community to develop and evaluate various data-driven\nalgorithms. Finally, several representative methods are evaluated using the\nproposed dataset and the results are reported as a useful baseline for future\nresearch.",
    "We propose a training and evaluation approach for autoencoder Generative\nAdversarial Networks (GANs), specifically the Boundary Equilibrium Generative\nAdversarial Network (BEGAN), based on methods from the image quality assessment\nliterature. Our approach explores a multidimensional evaluation criterion that\nutilizes three distance functions: an $l_1$ score, the Gradient Magnitude\nSimilarity Mean (GMSM) score, and a chrominance score. We show that each of the\ndifferent distance functions captures a slightly different set of properties in\nimage space and, consequently, requires its own evaluation criterion to\nproperly assess whether the relevant property has been adequately learned. We\nshow that models using the new distance functions are able to produce better\nimages than the original BEGAN model in predicted ways.",
    "Sign Language Recognition has emerged as one of the important area of\nresearch in Computer Vision. The difficulty faced by the researchers is that\nthe instances of signs vary with both motion and appearance. Thus, in this\npaper a novel approach for recognizing various alphabets of Indian Sign\nLanguage is proposed where continuous video sequences of the signs have been\nconsidered. The proposed system comprises of three stages: Preprocessing stage,\nFeature Extraction and Classification. Preprocessing stage includes skin\nfiltering, histogram matching. Eigen values and Eigen Vectors were considered\nfor feature extraction stage and finally Eigen value weighted Euclidean\ndistance is used to recognize the sign. It deals with bare hands, thus allowing\nthe user to interact with the system in natural way. We have considered 24\ndifferent alphabets in the video sequences and attained a success rate of\n96.25%.",
    "A common strategy for improving model robustness is through data\naugmentations. Data augmentations encourage models to learn desired\ninvariances, such as invariance to horizontal flipping or small changes in\ncolor. Recent work has shown that arbitrary style transfer can be used as a\nform of data augmentation to encourage invariance to textures by creating\npainting-like images from photographs. However, a stylized photograph is not\nquite the same as an artist-created painting. Artists depict perceptually\nmeaningful cues in paintings so that humans can recognize salient components in\nscenes, an emphasis which is not enforced in style transfer. Therefore, we\nstudy how style transfer and paintings differ in their impact on model\nrobustness. First, we investigate the role of paintings as style images for\nstylization-based data augmentation. We find that style transfer functions well\neven without paintings as style images. Second, we show that learning from\npaintings as a form of perceptual data augmentation can improve model\nrobustness. Finally, we investigate the invariances learned from stylization\nand from paintings, and show that models learn different invariances from these\ndiffering forms of data. Our results provide insights into how stylization\nimproves model robustness, and provide evidence that artist-created paintings\ncan be a valuable source of data for model robustness.",
    "The non-stationary nature of image characteristics calls for adaptive\nprocessing, based on the local image content. We propose a simple and flexible\nmethod to learn local tuning of parameters in adaptive image processing: we\nextract simple local features from an image and learn the relation between\nthese features and the optimal filtering parameters. Learning is performed by\noptimizing a user defined cost function (any image quality metric) on a\ntraining set. We apply our method to three classical problems (denoising,\ndemosaicing and deblurring) and we show the effectiveness of the learned\nparameter modulation strategies. We also show that these strategies are\nconsistent with theoretical results from the literature.",
    "Data of different modalities generally convey complimentary but heterogeneous\ninformation, and a more discriminative representation is often preferred by\ncombining multiple data modalities like the RGB and infrared features. However\nin reality, obtaining both data channels is challenging due to many\nlimitations. For example, the RGB surveillance cameras are often restricted\nfrom private spaces, which is in conflict with the need of abnormal activity\ndetection for personal security. As a result, using partial data channels to\nbuild a full representation of multi-modalities is clearly desired. In this\npaper, we propose a novel Partial-modal Generative Adversarial Networks\n(PM-GANs) that learns a full-modal representation using data from only partial\nmodalities. The full representation is achieved by a generated representation\nin place of the missing data channel. Extensive experiments are conducted to\nverify the performance of our proposed method on action recognition, compared\nwith four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset\nfor action recognition is introduced, and will be the first publicly available\naction dataset that contains paired infrared and visible spectrum.",
    "Medical image segmentation has made significant progress in recent years.\nDeep learning-based methods are recognized as data-hungry techniques, requiring\nlarge amounts of data with manual annotations. However, manual annotation is\nexpensive in the field of medical image analysis, which requires\ndomain-specific expertise. To address this challenge, few-shot learning has the\npotential to learn new classes from only a few examples. In this work, we\npropose a novel framework for few-shot medical image segmentation, termed\nCAT-Net, based on cross masked attention Transformer. Our proposed network\nmines the correlations between the support image and query image, limiting them\nto focus only on useful foreground information and boosting the representation\ncapacity of both the support prototype and query features. We further design an\niterative refinement framework that refines the query image segmentation\niteratively and promotes the support feature in turn. We validated the proposed\nmethod on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental\nresults demonstrate the superior performance of our method compared to\nstate-of-the-art methods and the effectiveness of each component. Code:\nhttps://github.com/hust-linyi/CAT-Net.",
    "Recently, contrastive self-supervised learning has become a key component for\nlearning visual representations across many computer vision tasks and\nbenchmarks. However, contrastive learning in the context of domain adaptation\nremains largely underexplored. In this paper, we propose to extend contrastive\nlearning to a new domain adaptation setting, a particular situation occurring\nwhere the similarity is learned and deployed on samples following different\nprobability distributions without access to labels. Contrastive learning learns\nby comparing and contrasting positive and negative pairs of samples in an\nunsupervised setting without access to source and target labels. We have\ndeveloped a variation of a recently proposed contrastive learning framework\nthat helps tackle the domain adaptation problem, further identifying and\nremoving possible negatives similar to the anchor to mitigate the effects of\nfalse negatives. Extensive experiments demonstrate that the proposed method\nadapts well, and improves the performance on the downstream domain adaptation\ntask.",
    "We present a simple and effective deep convolutional neural network (CNN)\nmodel for video deblurring. The proposed algorithm mainly consists of optical\nflow estimation from intermediate latent frames and latent frame restoration\nsteps. It first develops a deep CNN model to estimate optical flow from\nintermediate latent frames and then restores the latent frames based on the\nestimated optical flow. To better explore the temporal information from videos,\nwe develop a temporal sharpness prior to constrain the deep CNN model to help\nthe latent frame restoration. We develop an effective cascaded training\napproach and jointly train the proposed CNN model in an end-to-end manner. We\nshow that exploring the domain knowledge of video deblurring is able to make\nthe deep CNN model more compact and efficient. Extensive experimental results\nshow that the proposed algorithm performs favorably against state-of-the-art\nmethods on the benchmark datasets as well as real-world videos.",
    "In this paper, we study a novel problem in egocentric action recognition,\nwhich we term as \"Multimodal Generalization\" (MMG). MMG aims to study how\nsystems can generalize when data from certain modalities is limited or even\ncompletely missing. We thoroughly investigate MMG in the context of standard\nsupervised action recognition and the more challenging few-shot setting for\nlearning new action categories. MMG consists of two novel scenarios, designed\nto support security, and efficiency considerations in real-world applications:\n(1) missing modality generalization where some modalities that were present\nduring the train time are missing during the inference time, and (2)\ncross-modal zero-shot generalization, where the modalities present during the\ninference time and the training time are disjoint. To enable this\ninvestigation, we construct a new dataset MMG-Ego4D containing data points with\nvideo, audio, and inertial motion sensor (IMU) modalities. Our dataset is\nderived from Ego4D dataset, but processed and thoroughly re-annotated by human\nexperts to facilitate research in the MMG problem. We evaluate a diverse array\nof models on MMG-Ego4D and propose new methods with improved generalization\nability. In particular, we introduce a new fusion module with modality dropout\ntraining, contrastive-based alignment training, and a novel cross-modal\nprototypical loss for better few-shot performance. We hope this study will\nserve as a benchmark and guide future research in multimodal generalization\nproblems. The benchmark and code will be available at\nhttps://github.com/facebookresearch/MMG_Ego4D.",
    "Unsupervised learning of visual similarities is of paramount importance to\ncomputer vision, particularly due to lacking training data for fine-grained\nsimilarities. Deep learning of similarities is often based on relationships\nbetween pairs or triplets of samples. Many of these relations are unreliable\nand mutually contradicting, implying inconsistencies when trained without\nsupervision information that relates different tuples or triplets to each\nother. To overcome this problem, we use local estimates of reliable\n(dis-)similarities to initially group samples into compact surrogate classes\nand use local partial orders of samples to classes to link classes to each\nother. Similarity learning is then formulated as a partial ordering task with\nsoft correspondences of all samples to classes. Adopting a strategy of\nself-supervision, a CNN is trained to optimally represent samples in a mutually\nconsistent manner while updating the classes. The similarity learning and\ngrouping procedure are integrated in a single model and optimized jointly. The\nproposed unsupervised approach shows competitive performance on detailed pose\nestimation and object classification.",
    "In this paper, we propose a lossless data hiding scheme in JPEG images. After\nquantified DCT transform, coefficients have characteristics that distribution\nin high frequencies is relatively sparse and absolute values are small. To\nimprove encoding efficiency, we put forward an encoding algorithm that searches\nfor a high frequency as terminate point and recode the coefficients above, so\nspare space is reserved to embed secret data and appended data with no file\nexpansion. Receiver can obtain terminate point through data analysis, extract\nadditional data and recover original JPEG images lossless. Experimental results\nshow that the proposed method has a larger capacity than state-of-the-art\nworks.",
    "3D reconstruction of depth and motion from monocular video in dynamic\nenvironments is a highly ill-posed problem due to scale ambiguities when\nprojecting to the 2D image domain. In this work, we investigate the performance\nof the current State-of-the-Art (SotA) deep multi-view systems in such\nenvironments. We find that current supervised methods work surprisingly well\ndespite not modelling individual object motions, but make systematic errors due\nto a lack of dense ground truth data. To detect such errors during usage, we\nextend the cost volume based Deep Video to Depth (DeepV2D) framework\n\\cite{teed2018deepv2d} with a learned uncertainty. Our Deep Video to certain\nDepth (DeepV2cD) model allows i) to perform en par or better with current SotA\nand ii) achieve a better uncertainty measure than the naive Shannon entropy.\nOur experiments show that a simple filter strategy based on the uncertainty can\nsignificantly reduce systematic errors. This results in cleaner reconstructions\nboth on static and dynamic parts of the scene.",
    "Multiview super-resolution image reconstruction (SRIR) is often cast as a\nresampling problem by merging non-redundant data from multiple low-resolution\n(LR) images on a finer high-resolution (HR) grid, while inverting the effect of\nthe camera point spread function (PSF). One main problem with multiview methods\nis that resampling from nonuniform samples (provided by LR images) and the\ninversion of the PSF are highly nonlinear and ill-posed problems. Non-linearity\nand ill-posedness are typically overcome by linearization and regularization,\noften through an iterative optimization process, which essentially trade off\nthe very same information (i.e. high frequency) that we want to recover. We\npropose a novel point of view for multiview SRIR: Unlike existing multiview\nmethods that reconstruct the entire spectrum of the HR image from the multiple\ngiven LR images, we derive explicit expressions that show how the\nhigh-frequency spectra of the unknown HR image are related to the spectra of\nthe LR images. Therefore, by taking any of the LR images as the reference to\nrepresent the low-frequency spectra of the HR image, one can reconstruct the\nsuper-resolution image by focusing only on the reconstruction of the\nhigh-frequency spectra. This is very much like single-image methods, which\nextrapolate the spectrum of one image, except that we rely on information\nprovided by all other views, rather than by prior constraints as in\nsingle-image methods (which may not be an accurate source of information). This\nis made possible by deriving and applying explicit closed-form expressions that\ndefine how the local high frequency information that we aim to recover for the\nreference high resolution image is related to the local low frequency\ninformation in the sequence of views. Results and comparisons with recently\npublished state-of-the-art methods show the superiority of the proposed\nsolution.",
    "In 3D shape recognition, multi-view based methods leverage human's\nperspective to analyze 3D shapes and have achieved significant outcomes. Most\nexisting research works in deep learning adopt handcrafted networks as\nbackbones due to their high capacity of feature extraction, and also benefit\nfrom ImageNet pretraining. However, whether these network architectures are\nsuitable for 3D analysis or not remains unclear. In this paper, we propose a\nneural architecture search method named Auto-MVCNN which is particularly\ndesigned for optimizing architecture in multi-view 3D shape recognition.\nAuto-MVCNN extends gradient-based frameworks to process multi-view images, by\nautomatically searching the fusion cell to explore intrinsic correlation among\nview features. Moreover, we develop an end-to-end scheme to enhance retrieval\nperformance through the trade-off parameter search. Extensive experimental\nresults show that the searched architectures significantly outperform manually\ndesigned counterparts in various aspects, and our method achieves\nstate-of-the-art performance at the same time.",
    "In this paper, we present a new approach to estimate the layout of a room\nfrom its single image. While recent approaches for this task use robust\nfeatures learnt from data, they resort to optimization for detecting the final\nlayout. In addition to using learnt robust features, our approach learns an\nadditional ranking function to estimate the final layout instead of using\noptimization. To learn this ranking function, we propose a framework to train a\nCNN using max-margin structure cost. Also, while most approaches aim at\ndetecting cuboidal layouts, our approach detects non-cuboidal layouts for which\nwe explicitly estimates layout complexity parameters. We use these parameters\nto propose layout candidates in a novel way. Our approach shows\nstate-of-the-art results on standard datasets with mostly cuboidal layouts and\nalso performs well on a dataset containing rooms with non-cuboidal layouts.",
    "Learning discriminative representation using large-scale face datasets in the\nwild is crucial for real-world applications, yet it remains challenging. The\ndifficulties lie in many aspects and this work focus on computing resource\nconstraint and long-tailed class distribution. Recently, classification-based\nrepresentation learning with deep neural networks and well-designed losses have\ndemonstrated good recognition performance. However, the computing and memory\ncost linearly scales up to the number of identities (classes) in the training\nset, and the learning process suffers from unbalanced classes. In this work, we\npropose a dynamic class queue (DCQ) to tackle these two problems. Specifically,\nfor each iteration during training, a subset of classes for recognition are\ndynamically selected and their class weights are dynamically generated\non-the-fly which are stored in a queue. Since only a subset of classes is\nselected for each iteration, the computing requirement is reduced. By using a\nsingle server without model parallel, we empirically verify in large-scale\ndatasets that 10% of classes are sufficient to achieve similar performance as\nusing all classes. Moreover, the class weights are dynamically generated in a\nfew-shot manner and therefore suitable for tail classes with only a few\ninstances. We show clear improvement over a strong baseline in the largest\npublic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%\nof them have less than 10 instances. Code is available at\nhttps://github.com/bilylee/DCQ",
    "This paper tackles the problem of novel view synthesis from a single image.\nIn particular, we target real-world scenes with rich geometric structure, a\nchallenging task due to the large appearance variations of such scenes and the\nlack of simple 3D models to represent them. Modern, learning-based approaches\nmostly focus on appearance to synthesize novel views and thus tend to generate\npredictions that are inconsistent with the underlying scene structure. By\ncontrast, in this paper, we propose to exploit the 3D geometry of the scene to\nsynthesize a novel view. Specifically, we approximate a real-world scene by a\nfixed number of planes, and learn to predict a set of homographies and their\ncorresponding region masks to transform the input image into a novel view. To\nthis end, we develop a new region-aware geometric transform network that\nperforms these multiple tasks in a common framework. Our results on the outdoor\nKITTI and the indoor ScanNet datasets demonstrate the effectiveness of our\nnetwork in generating high quality synthetic views that respect the scene\ngeometry, thus outperforming the state-of-the-art methods.",
    "The success of deep neural networks (DNNs) is attributable to three factors:\nincreased compute capacity, more complex models, and more data. These factors,\nhowever, are not always present, especially for edge applications such as\nautonomous driving, augmented reality, and internet-of-things. Training DNNs\nrequires a large amount of data, which is difficult to obtain. Edge devices\nsuch as mobile phones have limited compute capacity, and therefore, require\nspecialized and efficient DNNs. However, due to the enormous design space and\nprohibitive training costs, designing efficient DNNs for different target\ndevices is challenging. So the question is, with limited data, compute\ncapacity, and model complexity, can we still successfully apply deep neural\nnetworks?\n  This dissertation focuses on the above problems and improving the efficiency\nof deep neural networks at four levels. Model efficiency: we designed neural\nnetworks for various computer vision tasks and achieved more than 10x faster\nspeed and lower energy. Data efficiency: we developed an advanced tool that\nenables 6.2x faster annotation of a LiDAR point cloud. We also leveraged domain\nadaptation to utilize simulated data, bypassing the need for real data.\nHardware efficiency: we co-designed neural networks and hardware accelerators\nand achieved 11.6x faster inference. Design efficiency: the process of finding\nthe optimal neural networks is time-consuming. Our automated neural\narchitecture search algorithms discovered, using 421x lower computational cost\nthan previous search methods, models with state-of-the-art accuracy and\nefficiency.",
    "In this paper, we propose a way of synthesizing realistic images directly\nwith natural language description, which has many useful applications, e.g.\nintelligent image manipulation. We attempt to accomplish such synthesis: given\na source image and a target text description, our model synthesizes images to\nmeet two requirements: 1) being realistic while matching the target text\ndescription; 2) maintaining other image features that are irrelevant to the\ntext description. The model should be able to disentangle the semantic\ninformation from the two modalities (image and text), and generate new images\nfrom the combined semantics. To achieve this, we proposed an end-to-end neural\narchitecture that leverages adversarial learning to automatically learn\nimplicit loss functions, which are optimized to fulfill the aforementioned two\nrequirements. We have evaluated our model by conducting experiments on\nCaltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated\nthat our model is capable of synthesizing realistic images that match the given\ndescriptions, while still maintain other features of original images.",
    "Fully autonomous drones have been demonstrated to find lost or injured\npersons under strongly occluding forest canopy. Airborne Optical Sectioning\n(AOS), a novel synthetic aperture imaging technique, together with\ndeep-learning-based classification enables high detection rates under realistic\nsearch-and-rescue conditions. We demonstrate that false detections can be\nsignificantly suppressed and true detections boosted by combining\nclassifications from multiple AOS rather than single integral images. This\nimproves classification rates especially in the presence of occlusion. To make\nthis possible, we modified the AOS imaging process to support large overlaps\nbetween subsequent integrals, enabling real-time and on-board scanning and\nprocessing of groundspeeds up to 10 m/s.",
    "In object detection, reducing computational cost is as important as improving\naccuracy for most practical usages. This paper proposes a novel network\nstructure, which is an order of magnitude lighter than other state-of-the-art\nnetworks while maintaining the accuracy. Based on the basic principle of more\nlayers with less channels, this new deep neural network minimizes its\nredundancy by adopting recent innovations including C.ReLU and Inception\nstructure. We also show that this network can be trained efficiently to achieve\nsolid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on\nVOC2007 and VOC2012 while the required compute is less than 10% of the recent\nResNet-101.",
    "Deep Reinforcement Learning has been successfully applied to learn robotic\ncontrol. However, the corresponding algorithms struggle when applied to\nproblems where the agent is only rewarded after achieving a complex task. In\nthis context, using demonstrations can significantly speed up the learning\nprocess, but demonstrations can be costly to acquire. In this paper, we propose\nto leverage a sequential bias to learn control policies for complex robotic\ntasks using a single demonstration. To do so, our method learns a\ngoal-conditioned policy to control a system between successive low-dimensional\ngoals. This sequential goal-reaching approach raises a problem of compatibility\nbetween successive goals: we need to ensure that the state resulting from\nreaching a goal is compatible with the achievement of the following goals. To\ntackle this problem, we present a new algorithm called DCIL-II. We show that\nDCIL-II can solve with unprecedented sample efficiency some challenging\nsimulated tasks such as humanoid locomotion and stand-up as well as fast\nrunning with a simulated Cassie robot. Our method leveraging sequentiality is a\nstep towards the resolution of complex robotic tasks under minimal\nspecification effort, a key feature for the next generation of autonomous\nrobots.",
    "The four-wheeled Mecanum robot is widely used in various industries due to\nits maneuverability and strong load capacity, which is suitable for performing\nprecise transportation tasks in a narrow environment, but while the Mecanum\nwheel robot has mobility, it also consumes more energy than ordinary robots.\nThe power consumed by the Mecanum wheel mobile robot varies enormously\ndepending on their operating regimes and environments. Therefore, only knowing\nthe working environment of the robot and the accurate power consumption model\ncan we accurately predict the power consumption of the robot. In order to\nincrease the appli-cable scenarios of energy consumption modeling for Mecanum\nwheel robots and improve the accuracy of energy consumption modeling, this\npaper focuses on various factors that affect the energy consumption of the\nMecanum wheel robot, such as motor temperature, terrain, the center of gravity\nposition, etc. The model is derived from the kinematic and kinetic model\ncombined with electrical engineering and energy flow principles. The model has\nbeen simulated in MATLAB and experimentally validated with the four-wheeled\nMecanum robot platform in our lab. Experimental results show that the model is\n90% accurate. The results of energy consumption modeling can help robots to\nsave energy by helping them to perform rational path planning and task\nplanning.",
    "A key challenge in Imitation Learning (IL) is that optimal state actions\ndemonstrations are difficult for the teacher to provide. For example in\nrobotics, providing kinesthetic demonstrations on a robotic manipulator\nrequires the teacher to control multiple degrees of freedom at once. The\ndifficulty of requiring optimal state action demonstrations limits the space of\nproblems where the teacher can provide quality feedback. As an alternative to\nstate action demonstrations, the teacher can provide corrective feedback such\nas their preferences or rewards. Prior work has created algorithms designed to\nlearn from specific types of noisy feedback, but across teachers and tasks\ndifferent forms of feedback may be required. Instead we propose that in order\nto learn from a diversity of scenarios we need to learn from a variety of\nfeedback. To learn from a variety of feedback we make the following insight:\nthe teacher's cost function is latent and we can model a stream of feedback as\na stream of loss functions. We then use any online learning algorithm to\nminimize the sum of these losses. With this insight we can learn from a\ndiversity of feedback that is weakly correlated with the teacher's true cost\nfunction. We unify prior work into a general corrective feedback meta-algorithm\nand show that regardless of feedback we can obtain the same regret bounds. We\ndemonstrate our approach by learning to perform a household navigation task on\na robotic racecar platform. Our results show that our approach can learn\nquickly from a variety of noisy feedback.",
    "In this paper we consider infinite horizon discounted dynamic programming\nproblems with finite state and control spaces, and partial state observations.\nWe discuss an algorithm that uses multistep lookahead, truncated rollout with a\nknown base policy, and a terminal cost function approximation. This algorithm\nis also used for policy improvement in an approximate policy iteration scheme,\nwhere successive policies are approximated by using a neural network\nclassifier. A novel feature of our approach is that it is well suited for\ndistributed computation through an extended belief space formulation and the\nuse of a partitioned architecture, which is trained with multiple neural\nnetworks. We apply our methods in simulation to a class of sequential repair\nproblems where a robot inspects and repairs a pipeline with potentially several\nrupture sites under partial information about the state of the pipeline.",
    "We present an integrated Task-Motion Planning framework for robot navigation\nin belief space. Autonomous robots operating in real world complex scenarios\nrequire planning in the discrete (task) space and the continuous (motion)\nspace. To this end, we propose a framework for integrating belief space\nreasoning within a hybrid task planner. The expressive power of PDDL+ combined\nwith heuristic-driven semantic attachments performs the propagated and\nposterior belief estimates while planning. The underlying methodology for the\ndevelopment of the combined hybrid planner is discussed, providing suggestions\nfor improvements and future work. Furthermore we validate key aspects of our\napproach using a realistic scenario in simulation.",
    "The real-world application of small drones is mostly hampered by energy\nlimitations. Neuromorphic computing promises extremely energy-efficient AI for\nautonomous flight, but is still challenging to train and deploy on real robots.\nIn order to reap the maximal benefits from neuromorphic computing, it is\ndesired to perform all autonomy functions end-to-end on a single neuromorphic\nchip, from low-level attitude control to high-level navigation. This research\npresents the first neuromorphic control system using a spiking neural network\n(SNN) to effectively map a drone's raw sensory input directly to motor\ncommands. We apply this method to low-level attitude estimation and control for\na quadrotor, deploying the SNN on a tiny Crazyflie. We propose a modular SNN,\nseparately training and then merging estimation and control sub-networks. The\nSNN is trained with imitation learning, using a flight dataset of sensory-motor\npairs. Post-training, the network is deployed on the Crazyflie, issuing control\ncommands from sensor inputs at $500$Hz. Furthermore, for the training procedure\nwe augmented training data by flying a controller with additional excitation\nand time-shifting the target data to enhance the predictive capabilities of the\nSNN. On the real drone the perception-to-control SNN tracks attitude commands\nwith an average error of $3$ degrees, compared to $2.5$ degrees for the regular\nflight stack. We also show the benefits of the proposed learning modifications\nfor reducing the average tracking error and reducing oscillations. Our work\nshows the feasibility of performing neuromorphic end-to-end control, laying the\nbasis for highly energy-efficient and low-latency neuromorphic autopilots.",
    "Robotic vision introduces requirements for real-time processing of\nfast-varying, noisy information in a continuously changing environment. In a\nreal-world environment, convenient assumptions, such as static camera systems\nand deep learning algorithms devouring high volumes of ideally slightly-varying\ndata are hard to survive. Leveraging on recent studies on the neural connectome\nassociated with eye movements, we designed a neuromorphic oculomotor controller\nand placed it at the heart of our in-house biomimetic robotic head prototype.\nThe controller is unique in the sense that (1) all data are encoded and\nprocessed by a spiking neural network (SNN), and (2) by mimicking the\nassociated brain areas' topology, the SNN is biologically interpretable and\nrequires no training to operate. Here, we report the robot's target tracking\nability, demonstrate that its eye kinematics are similar to those reported in\nhuman eye studies and show that a biologically-constrained learning, although\nnot required for the SNN's function, can be used to further refine its\nperformance. This work aligns with our ongoing effort to develop\nenergy-efficient neuromorphic SNNs and harness their emerging intelligence to\ncontrol biomimetic robots with versatility and robustness.",
    "Finding controllers that perform well across multiple morphologies is an\nimportant milestone for large-scale robotics, in line with recent advances via\nfoundation models in other areas of machine learning. However, the challenges\nof learning a single controller to control multiple morphologies make the `one\nrobot one task' paradigm dominant in the field. To alleviate these challenges,\nwe present a pipeline that: (1) leverages Quality Diversity algorithms like\nMAP-Elites to create a dataset of many single-task/single-morphology teacher\ncontrollers, then (2) distills those diverse controllers into a single\nmulti-morphology controller that performs well across many different body plans\nby mimicking the sensory-action patterns of the teacher controllers via\nsupervised learning. The distilled controller scales well with the number of\nteachers/morphologies and shows emergent properties. It generalizes to unseen\nmorphologies in a zero-shot manner, providing robustness to morphological\nperturbations and instant damage recovery. Lastly, the distilled controller is\nalso independent of the teacher controllers -- we can distill the teacher's\nknowledge into any controller model, making our approach synergistic with\narchitectural improvements and existing training algorithms for teacher\ncontrollers.",
    "In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for\nmotion planning under motion and sensing uncertainties. The original stochastic\nmotion planning problem is divided into a deterministic motion planning problem\nand a graph search problem. We solve the deterministic planning problem using\nsampling-based methods such as PRM or RRG to construct a graph of nominal\ntrajectories. Then, an informed cost-to-go heuristic for the original problem\nis computed based on the nominal trajectory graph. Finally, we grow a belief\ntree by searching over the graph using the proposed heuristic. IBBT interleaves\nbetween batch state sampling, nominal trajectory graph construction, heuristic\ncomputing, and search over the graph to find belief space motion plans. IBBT is\nan anytime, incremental algorithm. With an increasing number of batches of\nsamples added to the graph, the algorithm finds motion plans that converge to\nthe optimal one. IBBT is efficient by reusing results between sequential\niterations. The belief tree searching is an ordered search guided by an\ninformed heuristic. We test IBBT in different planning environments. Our\nnumerical investigation confirms that IBBT finds non-trivial motion plans and\nis faster compared with previous similar methods.",
    "Hierarchical Reinforcement Learning (HRL) algorithms have been demonstrated\nto perform well on high-dimensional decision making and robotic control tasks.\nHowever, because they solely optimize for rewards, the agent tends to search\nthe same space redundantly. This problem reduces the speed of learning and\nachieved reward. In this work, we present an Off-Policy HRL algorithm that\nmaximizes entropy for efficient exploration. The algorithm learns a temporally\nabstracted low-level policy and is able to explore broadly through the addition\nof entropy to the high-level. The novelty of this work is the theoretical\nmotivation of adding entropy to the RL objective in the HRL setting. We\nempirically show that the entropy can be added to both levels if the\nKullback-Leibler (KL) divergence between consecutive updates of the low-level\npolicy is sufficiently small. We performed an ablative study to analyze the\neffects of entropy on hierarchy, in which adding entropy to high-level emerged\nas the most desirable configuration. Furthermore, a higher temperature in the\nlow-level leads to Q-value overestimation and increases the stochasticity of\nthe environment that the high-level operates on, making learning more\nchallenging. Our method, SHIRO, surpasses state-of-the-art performance on a\nrange of simulated robotic control benchmark tasks and requires minimal tuning.",
    "Despite outstanding success in vision amongst other domains, many of the\nrecent deep learning approaches have evident drawbacks for robots. This\nmanuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a\ntool to resolve motor commands directly from raw percepts. These recent\nadvances are only a piece to the puzzle. We suggest that deep learning as a\ntool alone is insufficient in building a unified framework to acquire general\nintelligence. For this reason, we complement our survey with insights from\ncognitive development and refer to ideas from classical control theory,\nproducing an integrated direction for a lifelong learning architecture.",
    "The use of mobile robots is being popular over the world mainly for\nautonomous explorations in hazardous/ toxic or unknown environments. This\nexploration will be more effective and efficient if the explorations in unknown\nenvironment can be aided with the learning from past experiences. Currently\nreinforcement learning is getting more acceptances for implementing learning in\nrobots from the system-environment interactions. This learning can be\nimplemented using the concept of both single-agent and multiagent. This paper\ndescribes such a multiagent approach for implementing a type of reinforcement\nlearning using a priority based behaviour-based architecture. This proposed\nmethodology has been successfully tested in both indoor and outdoor\nenvironments.",
    "This paper presents a learning from demonstration approach to programming\nsafe, autonomous behaviors for uncommon driving scenarios. Simulation is used\nto re-create a targeted driving situation, one containing a road-side hazard\ncreating a significant occlusion in an urban neighborhood, and collect optimal\ndriving behaviors from 24 users. Paper employs a key-frame based approach\ncombined with an algorithm to linearly combine models in order to extend the\nbehavior to novel variations of the target situation. This approach is\ntheoretically agnostic to the kind of LfD framework used for modeling data and\nour results suggest it generalizes well to variations containing an additional\nnumber of hazards occurring in sequence. The linear combination algorithm is\ninformed by analysis of driving data, which also suggests that decision-making\nalgorithms need to consider a trade-off between road-rules and immediate\nrewards to tackle some complex cases.",
    "Motivated by the stringent requirements of unstructured real-world where a\nplethora of unknown objects reside in arbitrary locations of the surface, we\npropose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that\ngenerates feasible 6-DoF grasp poses in unrestricted workspace with\nreachability awareness. Unlike the majority of works that predict if a proposed\ngrasp pose within the restricted workspace will be successful solely based on\ngrasp pose stability, our approach further learns a reachability predictor that\nevaluates if the grasp pose is reachable or not from robot's own experience. To\navoid the laborious real training data collection, we exploit the power of\nsimulation to train our networks on a large-scale synthetic dataset. This work\nis an early attempt that simultaneously evaluates grasping reachability from\nlearned knowledge while proposing feasible grasp poses with 3D CNN.\nExperimental results in both simulation and real-world demonstrate that our\napproach outperforms several other methods and achieves 82.5% grasping success\nrate on unknown objects.",
    "We formalize decision-making problems in robotics and automated control using\ncontinuous MDPs and actions that take place over continuous time intervals. We\nthen approximate the continuous MDP using finer and finer discretizations.\nDoing this results in a family of systems, each of which has an extremely large\naction space, although only a few actions are \"interesting\". We can view the\ndecision maker as being unaware of which actions are \"interesting\". We can\nmodel this using MDPUs, MDPs with unawareness, where the action space is much\nsmaller. As we show, MDPUs can be used as a general framework for learning\ntasks in robotic problems. We prove results on the difficulty of learning a\nnear-optimal policy in an an MDPU for a continuous task. We apply these ideas\nto the problem of having a humanoid robot learn on its own how to walk.",
    "In this article, we propose a backpropagation-free approach to robotic\ncontrol through the neuro-cognitive computational framework of neural\ngenerative coding (NGC), designing an agent built completely from powerful\npredictive coding/processing circuits that facilitate dynamic, online learning\nfrom sparse rewards, embodying the principles of planning-as-inference.\nConcretely, we craft an adaptive agent system, which we call active predictive\ncoding (ActPC), that balances an internally-generated epistemic signal (meant\nto encourage intelligent exploration) with an internally-generated instrumental\nsignal (meant to encourage goal-seeking behavior) to ultimately learn how to\ncontrol various simulated robotic systems as well as a complex robotic arm\nusing a realistic robotics simulator, i.e., the Surreal Robotics Suite, for the\nblock lifting task and can pick-and-place problems. Notably, our experimental\nresults demonstrate that our proposed ActPC agent performs well in the face of\nsparse (extrinsic) reward signals and is competitive with or outperforms\nseveral powerful backprop-based RL approaches.",
    "To achieve a successful grasp, gripper attributes such as its geometry and\nkinematics play a role as important as the object geometry. The majority of\nprevious work has focused on developing grasp methods that generalize over\nnovel object geometry but are specific to a certain robot hand. We propose\nUniGrasp, an efficient data-driven grasp synthesis method that considers both\nthe object geometry and gripper attributes as inputs. UniGrasp is based on a\nnovel deep neural network architecture that selects sets of contact points from\nthe input point cloud of the object. The proposed model is trained on a large\ndataset to produce contact points that are in force closure and reachable by\nthe robot hand. By using contact points as output, we can transfer between a\ndiverse set of multifingered robotic hands. Our model produces over 90% valid\ncontact points in Top10 predictions in simulation and more than 90% successful\ngrasps in real world experiments for various known two-fingered and\nthree-fingered grippers. Our model also achieves 93%, 83% and 90% successful\ngrasps in real world experiments for an unseen two-fingered gripper and two\nunseen multi-fingered anthropomorphic robotic hands.",
    "We present a novel human-aware navigation approach, where the robot learns to\nmimic humans to navigate safely in crowds. The presented model, referred to as\nDeepMoTIon, is trained with pedestrian surveillance data to predict human\nvelocity in the environment. The robot processes LiDAR scans via the trained\nnetwork to navigate to the target location. We conduct extensive experiments to\nassess the components of our network and prove their necessity to imitate\nhumans. Our experiments show that DeepMoTIion outperforms all the benchmarks in\nterms of human imitation, achieving a 24% reduction in time series-based path\ndeviation over the next best approach. In addition, while many other approaches\noften failed to reach the target, our method reached the target in 100% of the\ntest cases while complying with social norms and ensuring human safety.",
    "Learning algorithms, like Quality-Diversity (QD), can be used to acquire\nrepertoires of diverse robotics skills. This learning is commonly done via\ncomputer simulation due to the large number of evaluations required. However,\ntraining in a virtual environment generates a gap between simulation and\nreality. Here, we build upon the Reset-Free QD (RF-QD) algorithm to learn\ncontrollers directly on a physical robot. This method uses a dynamics model,\nlearned from interactions between the robot and the environment, to predict the\nrobot's behaviour and improve sample efficiency. A behaviour selection policy\nfilters out uninteresting or unsafe policies predicted by the model. RF-QD also\nincludes a recovery policy that returns the robot to a safe zone when it has\nwalked outside of it, allowing continuous learning. We demonstrate that our\nmethod enables a physical quadruped robot to learn a repertoire of behaviours\nin two hours without human supervision. We successfully test the solution\nrepertoire using a maze navigation task. Finally, we compare our approach to\nthe MAP-Elites algorithm. We show that dynamics awareness and a recovery policy\nare required for training on a physical robot for optimal archive generation.\nVideo available at https://youtu.be/BgGNvIsRh7Q",
    "The study of dexterous manipulation has provided important insights in humans\nsensorimotor control as well as inspiration for manipulation strategies in\nrobotic hands. Previous work focused on experimental environment with\nrestrictions. Here we describe a method using the deformation and color\ndistribution of the fingernail and its surrounding skin, to estimate the\nfingertip forces, torques and contact surface curvatures for various objects,\nincluding the shape and material of the contact surfaces and the weight of the\nobjects. The proposed method circumvents limitations associated with sensorized\nobjects, gloves or fixed contact surface type. In addition, compared with\nprevious single finger estimation in an experimental environment, we extend the\napproach to multiple finger force estimation, which can be used for\napplications such as human grasping analysis. Four algorithms are used, c.q.,\nGaussian process (GP), Convolutional Neural Networks (CNN), Neural Networks\nwith Fast Dropout (NN-FD) and Recurrent Neural Networks with Fast Dropout\n(RNN-FD), to model a mapping from images to the corresponding labels. The\nresults further show that the proposed method has high accuracy to predict\nforce, torque and contact surface.",
    "It has been suggested that, when faced with large amounts of uncertainty in\nsituations of automated control, type-2 fuzzy logic based controllers will\nout-perform the simpler type-1 varieties due to the latter lacking the\nflexibility to adapt accordingly. This paper aims to investigate this problem\nin detail in order to analyse when a type-2 controller will improve upon type-1\nperformance. A robotic sailing boat is subjected to several experiments in\nwhich the uncertainty and difficulty of the sailing problem is increased in\norder to observe the effects on measured performance. Improved performance is\nobserved but not in every case. The size of the FOU is shown to be have a large\neffect on performance with potentially severe performance penalties for\nincorrectly sized footprints.",
    "This paper presents a data structure that summarizes distances between\nconfigurations across a robot configuration space, using a binary space\npartition whose cells contain parameters used for a locally linear\napproximation of the distance function. Querying the data structure is\nextremely fast, particularly when compared to the graph search required for\nquerying Probabilistic Roadmaps, and memory requirements are promising. The\npaper explores the use of the data structure constructed for a single robot to\nprovide a heuristic for challenging multi-robot motion planning problems.\nPotential applications also include the use of remote computation to analyze\nthe space of robot motions, which then might be transmitted on-demand to robots\nwith fewer computational resources.",
    "The use of multi-camera views simultaneously has been shown to improve the\ngeneralization capabilities and performance of visual policies. However, the\nhardware cost and design constraints in real-world scenarios can potentially\nmake it challenging to use multiple cameras. In this study, we present a novel\napproach to enhance the generalization performance of vision-based\nReinforcement Learning (RL) algorithms for robotic manipulation tasks. Our\nproposed method involves utilizing a technique known as knowledge distillation,\nin which a pre-trained ``teacher'' policy trained with multiple camera\nviewpoints guides a ``student'' policy in learning from a single camera\nviewpoint. To enhance the student policy's robustness against camera location\nperturbations, it is trained using data augmentation and extreme viewpoint\nchanges. As a result, the student policy learns robust visual features that\nallow it to locate the object of interest accurately and consistently,\nregardless of the camera viewpoint. The efficacy and efficiency of the proposed\nmethod were evaluated both in simulation and real-world environments. The\nresults demonstrate that the single-view visual student policy can successfully\nlearn to grasp and lift a challenging object, which was not possible with a\nsingle-view policy alone. Furthermore, the student policy demonstrates\nzero-shot transfer capability, where it can successfully grasp and lift objects\nin real-world scenarios for unseen visual configurations.",
    "Robots operating in the real world will experience a range of different\nenvironments and tasks. It is essential for the robot to have the ability to\nadapt to its surroundings to work efficiently in changing conditions.\nEvolutionary robotics aims to solve this by optimizing both the control and\nbody (morphology) of a robot, allowing adaptation to internal, as well as\nexternal factors. Most work in this field has been done in physics simulators,\nwhich are relatively simple and not able to replicate the richness of\ninteractions found in the real world. Solutions that rely on the complex\ninterplay between control, body, and environment are therefore rarely found. In\nthis paper, we rely solely on real-world evaluations and apply evolutionary\nsearch to yield combinations of morphology and control for our mechanically\nself-reconfiguring quadruped robot. We evolve solutions on two distinct\nphysical surfaces and analyze the results in terms of both control and\nmorphology. We then transition to two previously unseen surfaces to demonstrate\nthe generality of our method. We find that the evolutionary search finds\nhigh-performing and diverse morphology-controller configurations by adapting\nboth control and body to the different properties of the physical environments.\nWe additionally find that morphology and control vary with statistical\nsignificance between the environments. Moreover, we observe that our method\nallows for morphology and control parameters to transfer to previously-unseen\nterrains, demonstrating the generality of our approach.",
    "We provide a systematic analysis of levels of integration between discrete\nhigh-level reasoning and continuous low-level reasoning to address hybrid\nplanning problems in robotics. We identify four distinct strategies for such an\nintegration: (i) low-level checks are done for all possible cases in advance\nand then this information is used during plan generation, (ii) low-level checks\nare done exactly when they are needed during the search for a plan, (iii) first\nall plans are computed and then infeasible ones are filtered, and (iv) by means\nof replanning, after finding a plan, low-level checks identify whether it is\ninfeasible or not; if it is infeasible, a new plan is computed considering the\nresults of previous low- level checks. We perform experiments on hybrid\nplanning problems in robotic manipulation and legged locomotion domains\nconsidering these four methods of integration, as well as some of their\ncombinations. We analyze the usefulness of levels of integration in these\ndomains, both from the point of view of computational efficiency (in time and\nspace) and from the point of view of plan quality relative to its feasibility.\nWe discuss advantages and disadvantages of each strategy in the light of\nexperimental results and provide some guidelines on choosing proper strategies\nfor a given domain.",
    "A limitation of model-based reinforcement learning (MBRL) is the exploitation\nof errors in the learned models. Black-box models can fit complex dynamics with\nhigh fidelity, but their behavior is undefined outside of the data\ndistribution.Physics-based models are better at extrapolating, due to the\ngeneral validity of their informed structure, but underfit in the real world\ndue to the presence of unmodeled phenomena. In this work, we demonstrate\nexperimentally that for the offline model-based reinforcement learning setting,\nphysics-based models can be beneficial compared to high-capacity function\napproximators if the mechanical structure is known. Physics-based models can\nlearn to perform the ball in a cup (BiC) task on a physical manipulator using\nonly 4 minutes of sampled data using offline MBRL. We find that black-box\nmodels consistently produce unviable policies for BiC as all predicted\ntrajectories diverge to physically impossible state, despite having access to\nmore data than the physics-based model. In addition, we generalize the approach\nof physics parameter identification from modeling holonomic multi-body systems\nto systems with nonholonomic dynamics using end-to-end automatic\ndifferentiation.\n  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/",
    "Dubins tours represent a solution of the Dubins Traveling Salesman Problem\n(DTSP) that is a variant of the optimization routing problem to determine a\ncurvature-constrained shortest path to visit a set of locations such that the\npath is feasible for Dubins vehicle, which moves only forward and has a limited\nturning radius. The DTSP combines the NP-hard combinatorial optimization to\ndetermine the optimal sequence of visits to the locations, as in the regular\nTSP, with the continuous optimization of the heading angles at the locations,\nwhere the optimal heading values depend on the sequence of visits and vice\nversa. We address the computationally challenging DTSP by fast evaluation of\nthe sequence of visits by the proposed Windowing Surrogate Model (WiSM) which\nestimates the length of the optimal Dubins path connecting a sequence of\nlocations in a Dubins tour. The estimation is sped up by a regression model\ntrained using close to optimum solutions of small Dubins tours that are\ngeneralized for large-scale instances of the addressed DTSP utilizing the\nsliding window technique and a cache for already computed results. The reported\nresults support that the proposed WiSM enables a fast convergence of a\nrelatively simple evolutionary algorithm to high-quality solutions of the DTSP.\nWe show that with an increasing number of locations, our algorithm scales\nsignificantly better than other state-of-the-art DTSP solvers.",
    "Learning to control robots directly based on images is a primary challenge in\nrobotics. However, many existing reinforcement learning approaches require\niteratively obtaining millions of robot samples to learn a policy, which can\ntake significant time. In this paper, we focus on learning a realistic world\nmodel capturing the dynamics of scene changes conditioned on robot actions. Our\ndreaming model can emulate samples equivalent to a sequence of images from the\nactual environment, technically by learning an action-conditioned future\nrepresentation/scene regressor. This allows the agent to learn action policies\n(i.e., visuomotor policies) by interacting with the dreaming model rather than\nthe real-world. We experimentally confirm that our dreaming model enables robot\nlearning of policies that transfer to the real-world.",
    "Deep Reinforcement Learning (DRL) has achieved impressive performance in\nrobotics and autonomous systems (RAS). A key challenge to its deployment in\nreal-life operations is the presence of spuriously unsafe DRL policies.\nUnexplored states may lead the agent to make wrong decisions that could result\nin hazards, especially in applications where DRL-trained end-to-end controllers\ngovern the behaviour of RAS. This paper proposes a novel quantitative\nreliability assessment framework for DRL-controlled RAS, leveraging\nverification evidence generated from formal reliability analysis of neural\nnetworks. A two-level verification framework is introduced to check the safety\nproperty with respect to inaccurate observations that are due to, e.g.,\nenvironmental noise and state changes. Reachability verification tools are\nleveraged locally to generate safety evidence of trajectories. In contrast, at\nthe global level, we quantify the overall reliability as an aggregated metric\nof local safety evidence, corresponding to a set of distinct tasks and their\noccurrence probabilities. The effectiveness of the proposed verification\nframework is demonstrated and validated via experiments on real RAS.",
    "A robot performing multi-object grasping needs to sense the number of objects\nin the hand after grasping. The count plays an important role in determining\nthe robot's next move and the outcome and efficiency of the whole pick-place\nprocess. This paper presents a data-driven contrastive learning-based counting\nclassifier with a modified loss function as a simple and effective approach for\nobject counting despite significant occlusion challenges caused by robotic\nfingers and objects. The model was validated against other models with three\ndifferent common shapes (spheres, cylinders, and cubes) in simulation and in a\nreal setup. The proposed contrastive learning-based counting approach achieved\nabove 96\\% accuracy for all three objects in the real setup.",
    "We investigated the application of haptic feedback control and deep\nreinforcement learning (DRL) to robot-assisted dressing. Our method uses DRL to\nsimultaneously train human and robot control policies as separate neural\nnetworks using physics simulations. In addition, we modeled variations in human\nimpairments relevant to dressing, including unilateral muscle weakness,\ninvoluntary arm motion, and limited range of motion. Our approach resulted in\ncontrol policies that successfully collaborate in a variety of simulated\ndressing tasks involving a hospital gown and a T-shirt. In addition, our\napproach resulted in policies trained in simulation that enabled a real PR2\nrobot to dress the arm of a humanoid robot with a hospital gown. We found that\ntraining policies for specific impairments dramatically improved performance;\nthat controller execution speed could be scaled after training to reduce the\nrobot's speed without steep reductions in performance; that curriculum learning\ncould be used to lower applied forces; and that multi-modal sensing, including\na simulated capacitive sensor, improved performance.",
    "In this paper, we address the problem of adaptive path planning for accurate\nsemantic segmentation of terrain using unmanned aerial vehicles (UAVs). The\nusage of UAVs for terrain monitoring and remote sensing is rapidly gaining\nmomentum due to their high mobility, low cost, and flexible deployment.\nHowever, a key challenge is planning missions to maximize the value of acquired\ndata in large environments given flight time limitations. To address this, we\npropose an online planning algorithm which adapts the UAV paths to obtain\nhigh-resolution semantic segmentations necessary in areas on the terrain with\nfine details as they are detected in incoming images. This enables us to\nperform close inspections at low altitudes only where required, without wasting\nenergy on exhaustive mapping at maximum resolution. A key feature of our\napproach is a new accuracy model for deep learning-based architectures that\ncaptures the relationship between UAV altitude and semantic segmentation\naccuracy. We evaluate our approach on the application of crop/weed segmentation\nin precision agriculture using real-world field data.",
    "Accurate diagnosis of propeller faults is crucial for ensuring the safe and\nefficient operation of quadrotors. Training a fault classifier using simulated\ndata and deploying it on a real quadrotor is a cost-effective and safe\napproach. However, the simulation-to-reality gap often leads to poor\nperformance of the classifier when applied in real flight. In this work, we\npropose a deep learning model that addresses this issue by utilizing newly\nidentified features (NIF) as input and utilizing domain adaptation techniques\nto reduce the simulation-to-reality gap. In addition, we introduce an adjusted\nsimulation model that generates training data that more accurately reflects the\nbehavior of real quadrotors. The experimental results demonstrate that our\nproposed approach achieves an accuracy of 96\\% in detecting propeller faults.\nTo the best of our knowledge, this is the first reliable and efficient method\nfor simulation-to-reality fault diagnosis of quadrotor propellers.",
    "In this work, we examine the problem of online decision making under\nuncertainty, which we formulate as planning in the belief space. Maintaining\nbeliefs (i.e., distributions) over high-dimensional states (e.g., entire\ntrajectories) was not only shown to significantly improve accuracy, but also\nallows planning with information-theoretic objectives, as required for the\ntasks of active SLAM and information gathering. Nonetheless, planning under\nthis \"smoothing\" paradigm holds a high computational complexity, which makes it\nchallenging for online solution. Thus, we suggest the following idea: before\nplanning, perform a standalone state variable reordering procedure on the\ninitial belief, and \"push forwards\" all the predicted loop closing variables.\nSince the initial variable order determines which subset of them would be\naffected by incoming updates, such reordering allows us to minimize the total\nnumber of affected variables, and reduce the computational complexity of\ncandidate evaluation during planning. We call this approach PIVOT: Predictive\nIncremental Variable Ordering Tactic. Applying this tactic can also improve the\nstate inference efficiency; if we maintain the PIVOT order after the planning\nsession, then we should similarly reduce the cost of loop closures, when they\nactually occur. To demonstrate its effectiveness, we applied PIVOT in a\nrealistic active SLAM simulation, where we managed to significantly reduce the\ncomputation time of both the planning and inference sessions. The approach is\napplicable to general distributions, and induces no loss in accuracy.",
    "The aim of this workshop is to foster the exchange of insights on past and\nongoing research towards effective and long-lasting collaborations between\nhumans and robots. This workshop will provide a forum for representatives from\nacademia and industry communities to analyse the different aspects of HRI that\nimpact on its success. We particularly focus on AI techniques required to\nimplement autonomous and proactive interactions, on the factors that enhance,\nundermine, or recover humans' acceptance and trust in robots, and on the\npotential ethical and legal concerns related to the deployment of such robots\nin human-centred environments.\n  Website: https://sites.google.com/view/traits-hri-2022",
    "Global mobile robot localization is the problem of determining a robot's pose\nin an environment, using sensor data, when the starting position is unknown. A\nfamily of probabilistic algorithms known as Monte Carlo Localization (MCL) is\ncurrently among the most popular methods for solving this problem. MCL\nalgorithms represent a robot's belief by a set of weighted samples, which\napproximate the posterior probability of where the robot is located by using a\nBayesian formulation of the localization problem. This article presents an\nextension to the MCL algorithm, which addresses its problems when localizing in\nhighly symmetrical environments; a situation where MCL is often unable to\ncorrectly track equally probable poses for the robot. The problem arises from\nthe fact that sample sets in MCL often become impoverished, when samples are\ngenerated according to their posterior likelihood. Our approach incorporates\nthe idea of clusters of samples and modifies the proposal distribution\nconsidering the probability mass of those clusters. Experimental results are\npresented that show that this new extension to the MCL algorithm successfully\nlocalizes in symmetric environments where ordinary MCL often fails.",
    "An overview of the process to develop a safety case for an autonomous robot\ndeployment on a nuclear site in the UK is described and a safety case for a\nhypothetical robot incorporating AI is presented. This forms a first step\ntowards a deployment, showing what is possible now and what may be possible\nwith development of tools. It forms the basis for further discussion between\nnuclear site licensees, the Office for Nuclear Regulation (ONR), industry and\nacademia.",
    "Billions of people use chopsticks, a simple yet versatile tool, for fine\nmanipulation of everyday objects. The small, curved, and slippery tips of\nchopsticks pose a challenge for picking up small objects, making them a\nsuitably complex test case. This paper leverages human demonstrations to\ndevelop an autonomous chopsticks-equipped robotic manipulator. Due to the lack\nof accurate models for fine manipulation, we explore model-free imitation\nlearning, which traditionally suffers from the covariate shift phenomenon that\ncauses poor generalization. We propose two approaches to reduce covariate\nshift, neither of which requires access to an interactive expert or a model,\nunlike previous approaches. First, we alleviate single-step prediction errors\nby applying an invariant operator to increase the data support at critical\nsteps for grasping. Second, we generate synthetic corrective labels by adding\nbounded noise and combining parametric and non-parametric methods to prevent\nerror accumulation. We demonstrate our methods on a real chopstick-equipped\nrobot that we built, and observe the agent's success rate increase from 37.3%\nto 80%, which is comparable to the human expert performance of 82.6%.",
    "In this paper, we propose a novel lightweight learning from demonstration\n(LfD) model based on reservoir computing that can learn and generate multiple\nmovement trajectories with prediction intervals, which we call as Context-based\nEcho State Network with prediction confidence (CESN+). CESN+ can generate\nmovement trajectories that may go beyond the initial LfD training based on a\ndesired set of conditions while providing confidence on its generated output.\nTo assess the abilities of CESN+, we first evaluate its performance against\nConditional Neural Movement Primitives (CNMP), a comparable framework that uses\na conditional neural process to generate movement primitives. Our findings\nindicate that CESN+ not only outperforms CNMP but is also faster to train and\ndemonstrates impressive performance in generating trajectories for\nextrapolation cases. In human-robot shared control applications, the confidence\nof the machine generated trajectory is a key indicator of how to arbitrate\ncontrol sharing. To show the usability of the CESN+ for human-robot adaptive\nshared control, we have designed a proof-of-concept human-robot shared control\ntask and tested its efficacy in adapting the sharing weight between the human\nand the robot by comparing it to a fixed-weight control scheme. The simulation\nexperiments show that with CESN+ based adaptive sharing the total human load in\nshared control can be significantly reduced. Overall, the developed CESN+ model\nis a strong lightweight LfD system with desirable properties such fast training\nand ability to extrapolate to the new task parameters while producing robust\nprediction intervals for its output.",
    "The interactive partially observable Markov decision process (I-POMDP) is a\nrecently developed framework which extends the POMDP to the multi-agent setting\nby including agent models in the state space. This paper argues for formulating\nthe problem of an agent learning interactively from a human teacher as an\nI-POMDP, where the agent \\emph{programming} to be learned is captured by random\nvariables in the agent's state space, all \\emph{signals} from the human teacher\nare treated as observed random variables, and the human teacher, modeled as a\ndistinct agent, is explicitly represented in the agent's state space. The main\nbenefits of this approach are: i. a principled action selection mechanism, ii.\na principled belief update mechanism, iii. support for the most common teacher\n\\emph{signals}, and iv. the anticipated production of complex beneficial\ninteractions. The proposed formulation, its benefits, and several open\nquestions are presented.",
    "The increasing presence of robots alongside humans, such as in human-robot\nteams in manufacturing, gives rise to research questions about the kind of\nbehaviors people prefer in their robot counterparts. We term actions that\nsupport interaction by reducing future interference with others as supportive\nrobot actions and investigate their utility in a co-located manipulation\nscenario. We compare two robot modes in a shared table pick-and-place task: (1)\nTask-oriented: the robot only takes actions to further its own task objective\nand (2) Supportive: the robot sometimes prefers supportive actions to\ntask-oriented ones when they reduce future goal-conflicts. Our experiments in\nsimulation, using a simplified human model, reveal that supportive actions\nreduce the interference between agents, especially in more difficult tasks, but\nalso cause the robot to take longer to complete the task. We implemented these\nmodes on a physical robot in a user study where a human and a robot perform\nobject placement on a shared table. Our results show that a supportive robot\nwas perceived as a more favorable coworker by the human and also reduced\ninterference with the human in the more difficult of two scenarios. However, it\nalso took longer to complete the task highlighting an interesting trade-off\nbetween task-efficiency and human-preference that needs to be considered before\ndesigning robot behavior for close-proximity manipulation scenarios.",
    "Simulation-to-real transfer is an important strategy for making reinforcement\nlearning practical with real robots. Successful sim-to-real transfer systems\nhave difficulty producing policies which generalize across tasks, despite\ntraining for thousands of hours equivalent real robot time. To address this\nshortcoming, we present a novel approach to efficiently learning new robotic\nskills directly on a real robot, based on model-predictive control (MPC) and an\nalgorithm for learning task representations. In short, we show how to reuse the\nsimulation from the pre-training step of sim-to-real methods as a tool for\nforesight, allowing the sim-to-real policy adapt to unseen tasks. Rather than\nend-to-end learning policies for single tasks and attempting to transfer them,\nwe first use simulation to simultaneously learn (1) a continuous\nparameterization (i.e. a skill embedding or latent) of task-appropriate\nprimitive skills, and (2) a single policy for these skills which is conditioned\non this representation. We then directly transfer our multi-skill policy to a\nreal robot, and actuate the robot by choosing sequences of skill latents which\nactuate the policy, with each latent corresponding to a pre-learned primitive\nskill controller. We complete unseen tasks by choosing new sequences of skill\nlatents to control the robot using MPC, where our MPC model is composed of the\npre-trained skill policy executed in the simulation environment, run in\nparallel with the real robot. We discuss the background and principles of our\nmethod, detail its practical implementation, and evaluate its performance by\nusing our method to train a real Sawyer Robot to achieve motion tasks such as\ndrawing and block pushing.",
    "Due to the complex and dynamic character of intersection scenarios, the\nautonomous driving strategy at intersections has been a difficult problem and a\nhot point in the research of intelligent transportation systems in recent\nyears. This paper gives a brief summary of state-of-the-art autonomous driving\nstrategies at intersections. Firstly, we enumerate and analyze common types of\nintersection scenarios, corresponding simulation platforms, as well as related\ndatasets. Secondly, by reviewing previous studies, we have summarized\ncharacteristics of existing autonomous driving strategies and classified them\ninto several categories. Finally, we point out problems of the existing\nautonomous driving strategies and put forward several valuable research\noutlooks.",
    "Over the last several years, use cases for robotics based solutions have\ndiversified from factory floors to domestic applications. In parallel, Deep\nLearning approaches are replacing traditional techniques in Computer Vision,\nNatural Language Processing, Speech processing, etc. and are delivering robust\nresults. Our goal is to survey a number of research internship projects in the\nbroad area of 'Deep Learning as applied to Robotics' and present a concise view\nfor the benefit of aspiring student interns. In this paper, we survey the\nresearch work done by Robotic Institute Summer Scholars (RISS), CMU. We\nparticularly focus on papers that use deep learning to solve core robotic\nproblems and also robotic solutions. We trust this would be useful particularly\nfor internship aspirants for the Robotics Institute, CMU",
    "In this paper, we exploit minimal sensing information gathered from\nbiologically inspired sensor networks to perform exploration and mapping in an\nunknown environment. A probabilistic motion model of mobile sensing nodes,\ninspired by motion characteristics of cockroaches, is utilized to extract weak\nencounter information in order to build a topological representation of the\nenvironment.\n  Neighbor to neighbor interactions among the nodes are exploited to build\npoint clouds representing spatial features of the manifold characterizing the\nenvironment based on the sampled data.\n  To extract dominant features from sampled data, topological data analysis is\nused to produce persistence intervals for features, to be used for topological\nmapping. In order to improve robustness characteristics of the sampled data\nwith respect to outliers, density based subsampling algorithms are employed.\nMoreover, a robust scale-invariant classification algorithm for persistence\ndiagrams is proposed to provide a quantitative representation of desired\nfeatures in the data. Furthermore, various strategies for defining encounter\nmetrics with different degrees of information regarding agents' motion are\nsuggested to enhance the precision of the estimation and classification\nperformance of the topological method.",
    "This work addresses the task of risk evaluation in traffic scenarios with\nlimited observability due to restricted sensorial coverage. Here, we\nconcentrate on intersection scenarios that are difficult to access visually. To\nidentify the area of sight, we employ ray casting on a local dynamic map\nproviding geometrical information and road infrastructure. Based on the area\nwith reduced visibility, we first model scene entities that pose a potential\nrisk without being visually perceivable yet. Then, we predict a worst-case\ntrajectory in the survival analysis for collision risk estimation. Resulting\nrisk indicators are utilized to evaluate the driver's current behavior, to warn\nthe driver in critical situations, to give suggestions on how to act safely or\nto plan safe trajectories. We validate our approach by applying the resulting\nintersection warning system on real world scenarios. The proposed system's\nbehavior reveals to mimic the general behavior of a correctly acting human\ndriver.",
    "Designing optimal soft modular robots is difficult, due to non-trivial\ninteractions between morphology and controller. Evolutionary algorithms (EAs),\ncombined with physical simulators, represent a valid tool to overcome this\nissue. In this work, we investigate algorithmic solutions to improve the\nQuality Diversity of co-evolved designs of Tensegrity Soft Modular Robots\n(TSMRs) for two robotic tasks, namely goal reaching and squeezing trough a\nnarrow passage. To this aim, we use three different EAs, i.e., MAP-Elites and\ntwo custom algorithms: one based on Viability Evolution (ViE) and NEAT\n(ViE-NEAT), the other named Double Map MAP-Elites (DM-ME) and devised to seek\ndiversity while co-evolving robot morphologies and neural network (NN)-based\ncontrollers. In detail, DM-ME extends MAP-Elites in that it uses two distinct\nfeature maps, referring to morphologies and controllers respectively, and\nintegrates a mechanism to automatically define the NN-related feature\ndescriptor. Considering the fitness, in the goal-reaching task ViE-NEAT\noutperforms MAP-Elites and results equivalent to DM-ME. Instead, when\nconsidering diversity in terms of \"illumination\" of the feature space, DM-ME\noutperforms the other two algorithms on both tasks, providing a richer pool of\npossible robotic designs, whereas ViE-NEAT shows comparable performance to\nMAP-Elites on goal reaching, although it does not exploit any map.",
    "Search algorithms are applied where data retrieval with specified\nspecifications is required. The motivation behind developing search algorithms\nin Functional Object-Oriented Networks is that most of the time, a certain\nrecipe needs to be retrieved or ingredients for a certain recipe needs to be\ndetermined. According to the introduction, there is a time when execution of an\nentire recipe is not available for a robot thus prompting the need to retrieve\na certain recipe or ingredients. With a quality FOON, robots can decipher a\ntask goal, find the correct objects at the required states on which to operate\nand output a sequence of proper manipulation motions. This paper shows several\nproposed weighted FOON and task planning algorithms that allow a robot and a\nhuman to successfully complete complicated tasks together with higher success\nrates than a human doing them alone.",
    "Target-driven visual navigation is a challenging problem that requires a\nrobot to find the goal using only visual inputs. Many researchers have\ndemonstrated promising results using deep reinforcement learning (deep RL) on\nvarious robotic platforms, but typical end-to-end learning is known for its\npoor extrapolation capability to new scenarios. Therefore, learning a\nnavigation policy for a new robot with a new sensor configuration or a new\ntarget still remains a challenging problem. In this paper, we introduce a\nlearning algorithm that enables rapid adaptation to new sensor configurations\nor target objects with a few shots. We design a policy architecture with latent\nfeatures between perception and inference networks and quickly adapt the\nperception network via meta-learning while freezing the inference network. Our\nexperiments show that our algorithm adapts the learned navigation policy with\nonly three shots for unseen situations with different sensor configurations or\ndifferent target colors. We also analyze the proposed algorithm by\ninvestigating various hyperparameters.",
    "We present a data-driven shared control algorithm that can be used to improve\na human operator's control of complex dynamic machines and achieve tasks that\nwould otherwise be challenging, or impossible, for the user on their own. Our\nmethod assumes no a priori knowledge of the system dynamics. Instead, both the\ndynamics and information about the user's interaction are learned from\nobservation through the use of a Koopman operator. Using the learned model, we\ndefine an optimization problem to compute the autonomous partner's control\npolicy. Finally, we dynamically allocate control authority to each partner\nbased on a comparison of the user input and the autonomously generated control.\nWe refer to this idea as model-based shared control (MbSC). We evaluate the\nefficacy of our approach with two human subjects studies consisting of 32 total\nparticipants (16 subjects in each study). The first study imposes a linear\nconstraint on the modeling and autonomous policy generation algorithms. The\nsecond study explores the more general, nonlinear variant. Overall, we find\nthat model-based shared control significantly improves task and control metrics\nwhen compared to a natural learning, or user only, control paradigm. Our\nexperiments suggest that models learned via the Koopman operator generalize\nacross users, indicating that it is not necessary to collect data from each\nindividual user before providing assistance with MbSC. We also demonstrate the\ndata-efficiency of MbSC and consequently, it's usefulness in online learning\nparadigms. Finally, we find that the nonlinear variant has a greater impact on\na user's ability to successfully achieve a defined task than the linear\nvariant.",
    "Hierarchical Task Network (HTN) planning is a popular approach that cuts down\non the classical planning search space by relying on a given hierarchical\nlibrary of domain control knowledge. This provides an intuitive methodology for\nspecifying high-level instructions on how robots and agents should perform\ntasks, while also giving the planner enough flexibility to choose the\nlower-level steps and their ordering. In this paper we present the HATP\n(Hierarchical Agent-based Task Planner) planning framework which extends the\ntraditional HTN planning domain representation and semantics by making them\nmore suitable for roboticists, and treating agents as \"first class\" entities in\nthe language. The former is achieved by allowing \"social rules\" to be defined\nwhich specify what behaviour is acceptable/unacceptable by the agents/robots in\nthe domain, and interleaving planning with geometric reasoning in order to\nvalidate online -with respect to a detailed geometric 3D world- the human/robot\nactions currently being pursued by HATP.",
    "This paper presents a novel approach to AUV localization, based on a\nsemantic-aided particle filter. Particle filters have been used successfully\nfor robotics localization since many years. Most of the approaches are however\nbased on geometric measurements and geometric information and simulations. In\nthe past years more and more efforts from research goes towards cognitive\nrobotics and the marine domain is not exception. Moving from signal to symbol\nbecomes therefore paramount for more complex applications. This paper presents\na contribution in the well-known area of underwater localization, incorporating\nsemantic information. An extension to the standard particle filter approach is\npresented, based on semantic information of the environment. A comparison with\nthe geometric approach shows the advantages of a semantic layer to successfully\nperform self-localization.",
    "This work presents an efficient framework to generate a motion plan of a\nrobot with high degrees of freedom (e.g., a humanoid robot).\nHigh-dimensionality of the robot configuration space often leads to\ndifficulties in utilizing the widely-used motion planning algorithms, since the\nvolume of the decision space increases exponentially with the number of\ndimensions. To handle complications arising from the large decision space, and\nto solve a corresponding motion planning problem efficiently, two key concepts\nare adopted in this work: First, the Gaussian process latent variable model\n(GP-LVM) is utilized for low-dimensional representation of the original\nconfiguration space. Second, an approximate inference algorithm is used,\nexploiting through the duality between control and estimation, to explore the\ndecision space and to compute a high-quality motion trajectory of the robot.\nUtilizing the GP-LVM and the duality between control and estimation, we\nconstruct a fully probabilistic generative model with which a high-dimensional\nmotion planning problem is transformed into a tractable inference problem.\nFinally, we compute the motion trajectory via an approximate inference\nalgorithm based on a variant of the particle filter. The resulting motions can\nbe viewed in the supplemental video. ( https://youtu.be/kngEaOR4Esc )",
    "With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have\nstudied how MAPF algorithms can be deployed to coordinate hundreds of robots in\nlarge automated warehouses. While most works try to improve the throughput of\nsuch warehouses by developing better MAPF algorithms, we focus on improving the\nthroughput by optimizing the warehouse layout. We show that, even with\nstate-of-the-art MAPF algorithms, commonly used human-designed layouts can lead\nto congestion for warehouses with large numbers of robots and thus have limited\nscalability. We extend existing automatic scenario generation methods to\noptimize warehouse layouts. Results show that our optimized warehouse layouts\n(1) reduce traffic congestion and thus improve throughput, (2) improve the\nscalability of the automated warehouses by doubling the number of robots in\nsome cases, and (3) are capable of generating layouts with user-specified\ndiversity measures. We include the source code at:\nhttps://github.com/lunjohnzhang/warehouse_env_gen_public",
    "We present a navigation system that combines ideas from hierarchical planning\nand machine learning. The system uses a traditional global planner to compute\noptimal paths towards a goal, and a deep local trajectory planner and velocity\ncontroller to compute motion commands. The latter components of the system\nadjust the behavior of the robot through attention mechanisms such that it\nmoves towards the goal, avoids obstacles, and respects the space of nearby\npedestrians. Both the structure of the proposed deep models and the use of\nattention mechanisms make the system's execution interpretable. Our simulation\nexperiments suggest that the proposed architecture outperforms baselines that\ntry to map global plan information and sensor data directly to velocity\ncommands. In comparison to a hand-designed traditional navigation system, the\nproposed approach showed more consistent performance.",
    "This work addresses the problem of long-horizon task planning with the Large\nLanguage Model (LLM) in an open-world household environment. Existing works\nfail to explicitly track key objects and attributes, leading to erroneous\ndecisions in long-horizon tasks, or rely on highly engineered state features\nand feedback, which is not generalizable. We propose an open state\nrepresentation that provides continuous expansion and updating of object\nattributes from the LLM's inherent capabilities for context understanding and\nhistorical action reasoning. Our proposed representation maintains a\ncomprehensive record of an object's attributes and changes, enabling robust\nretrospective summary of the sequence of actions leading to the current state.\nThis allows continuously updating world model to enhance context understanding\nfor decision-making in task planning. We validate our model through experiments\nacross simulated and real-world task planning scenarios, demonstrating\nsignificant improvements over baseline methods in a variety of tasks requiring\nlong-horizon state tracking and reasoning. (Video\\footnote{Video demonstration:\n\\url{https://youtu.be/QkN-8pxV3Mo}.})",
    "In this paper, we propose a novel optimization criterion that leverages\nfeatures of the skew normal distribution to better model the problem of\npersonalized recommendation. Specifically, the developed criterion borrows the\nconcept and the flexibility of the skew normal distribution, based on which\nthree hyperparameters are attached to the optimization criterion. Furthermore,\nfrom a theoretical point of view, we not only establish the relation between\nthe maximization of the proposed criterion and the shape parameter in the skew\nnormal distribution, but also provide the analogies and asymptotic analysis of\nthe proposed criterion to maximization of the area under the ROC curve.\nExperimental results conducted on a range of large-scale real-world datasets\nshow that our model significantly outperforms the state of the art and yields\nconsistently best performance on all tested datasets.",
    "Contract element extraction (CEE) is the novel task of automatically\nidentifying and extracting legally relevant elements such as contract dates,\npayments, and legislation references from contracts. Automatic methods for this\ntask view it as a sequence labeling problem and dramatically reduce human\nlabor. However, as contract genres and element types may vary widely, a\nsignificant challenge for this sequence labeling task is how to transfer\nknowledge from one domain to another, i.e., cross-domain CEE. Cross-domain CEE\ndiffers from cross-domain named entity recognition (NER) in two important ways.\nFirst, contract elements are far more fine-grained than named entities, which\nhinders the transfer of extractors. Second, the extraction zones for\ncross-domain CEE are much larger than for cross-domain NER. As a result, the\ncontexts of elements from different domains can be more diverse. We propose a\nframework, the Bi-directional Feedback cLause-Element relaTion network\n(Bi-FLEET), for the cross-domain CEE task that addresses the above challenges.\nBi-FLEET has three main components: (1) a context encoder, (2) a clause-element\nrelation encoder, and (3) an inference layer. To incorporate invariant\nknowledge about element and clause types, a clause-element graph is constructed\nacross domains and a hierarchical graph neural network is adopted in the\nclause-element relation encoder. To reduce the influence of context variations,\na multi-task framework with a bi-directional feedback scheme is designed in the\ninference layer, conducting both clause classification and element extraction.\nThe experimental results over both cross-domain NER and CEE tasks show that\nBi-FLEET significantly outperforms state-of-the-art baselines.",
    "Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n  Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.",
    "The recent adoption of recurrent neural networks (RNNs) for session modeling\nhas yielded substantial performance gains compared to previous approaches. In\nterms of context-aware session modeling, however, the existing RNN-based models\nare limited in that they are not designed to explicitly model rich static\nuser-side contexts (e.g., age, gender, location). Therefore, in this paper, we\nexplore the utility of explicit user-side context modeling for RNN session\nmodels. Specifically, we propose an augmented RNN (ARNN) model that extracts\nhigh-order user-contextual preference using the product-based neural network\n(PNN) in order to augment any existing RNN session model. Evaluation results\nshow that our proposed model outperforms the baseline RNN session model by a\nlarge margin when rich user-side contexts are available.",
    "Text categorization (TC) is the task of automatically organizing a set of\ndocuments into a set of pre-defined categories. Over the last few years,\nincreased attention has been paid to the use of documents in digital form and\nthis makes text categorization becomes a challenging issue. The most\nsignificant problem of text categorization is its huge number of features. Most\nof these features are redundant, noisy and irrelevant that cause over fitting\nwith most of the classifiers. Hence, feature extraction is an important step to\nimprove the overall accuracy and the performance of the text classifiers. In\nthis paper, we will provide an overview of using principle component analysis\n(PCA) as a feature extraction with various classifiers. It was observed that\nthe performance rate of the classifiers after using PCA to reduce the dimension\nof data improved. Experiments are conducted on three UCI data sets, Classic03,\nCNAE-9 and DBWorld e-mails. We compare the classification performance results\nof using PCA with popular and well-known text classifiers. Results show that\nusing PCA encouragingly enhances classification performance on most of the\nclassifiers.",
    "Recommender system has been deployed in a large amount of real-world\napplications, profoundly influencing people's daily life and\nproduction.Traditional recommender models mostly collect as comprehensive as\npossible user behaviors for accurate preference estimation. However,\nconsidering the privacy, preference shaping and other issues, the users may not\nwant to disclose all their behaviors for training the model. In this paper, we\nstudy a novel recommendation paradigm, where the users are allowed to indicate\ntheir \"willingness\" on disclosing different behaviors, and the models are\noptimized by trading-off the recommendation quality as well as the violation of\nthe user \"willingness\". More specifically, we formulate the recommendation\nproblem as a multiplayer game, where the action is a selection vector\nrepresenting whether the items are involved into the model training. For\nefficiently solving this game, we design a tailored algorithm based on\ninfluence function to lower the time cost for recommendation quality\nexploration, and also extend it with multiple anchor selection vectors.We\nconduct extensive experiments to demonstrate the effectiveness of our model on\nbalancing the recommendation quality and user disclosing willingness.",
    "CNNs, RNNs, GCNs, and CapsNets have shown significant insights in\nrepresentation learning and are widely used in various text mining tasks such\nas large-scale multi-label text classification. However, most existing deep\nmodels for multi-label text classification consider either the non-consecutive\nand long-distance semantics or the sequential semantics, but how to consider\nthem both coherently is less studied. In addition, most existing methods treat\noutput labels as independent methods, but ignore the hierarchical relations\namong them, leading to useful semantic information loss. In this paper, we\npropose a novel hierarchical taxonomy-aware and attentional graph capsule\nrecurrent CNNs framework for large-scale multi-label text classification.\nSpecifically, we first propose to model each document as a word order preserved\ngraph-of-words and normalize it as a corresponding words-matrix representation\nwhich preserves both the non-consecutive, long-distance and local sequential\nsemantics. Then the words-matrix is input to the proposed attentional graph\ncapsule recurrent CNNs for more effectively learning the semantic features. To\nleverage the hierarchical relations among the class labels, we propose a\nhierarchical taxonomy embedding method to learn their representations, and\ndefine a novel weighted margin loss by incorporating the label representation\nsimilarity. Extensive evaluations on three datasets show that our model\nsignificantly improves the performance of large-scale multi-label text\nclassification by comparing with state-of-the-art approaches.",
    "Assigning qualified, unbiased and interested reviewers to paper submissions\nis vital for maintaining the integrity and quality of the academic publishing\nsystem and providing valuable reviews to authors. However, matching thousands\nof submissions with thousands of potential reviewers within a limited time is a\ndaunting challenge for a conference program committee. Prior efforts based on\ntopic modeling have suffered from losing the specific context that help define\nthe topics in a publication or submission abstract. Moreover, in some cases,\ntopics identified are difficult to interpret. We propose an approach that\nlearns from each abstract published by a potential reviewer the topics studied\nand the explicit context in which the reviewer studied the topics. Furthermore,\nwe contribute a new dataset for evaluating reviewer matching systems. Our\nexperiments show a significant, consistent improvement in precision when\ncompared with the existing methods. We also use examples to demonstrate why our\nrecommendations are more explainable. The new approach has been deployed\nsuccessfully at top-tier conferences in the last two years.",
    "Currently, the text document retrieval systems have many challenges in\nexploring the semantics of queries and documents. Each query implies\ninformation which does not appear in the query but the documents related with\nthe information are also expected by user. The disadvantage of the previous\nspreading activation algorithms could be many irrelevant concepts added to the\nquery. In this paper, a proposed novel algorithm is only activate and add to\nthe query named entities which are related with original entities in the query\nand explicit relations in the query.",
    "Text similarity detection aims at measuring the degree of similarity between\na pair of texts. Corpora available for text similarity detection are designed\nto evaluate the algorithms to assess the paraphrase level among documents. In\nthis paper we present a textual German corpus for similarity detection. The\npurpose of this corpus is to automatically assess the similarity between a pair\nof texts and to evaluate different similarity measures, both for whole\ndocuments or for individual sentences. Therefore we have calculated several\nsimple measures on our corpus based on a library of similarity functions.",
    "Personalized review-based rating prediction aims at leveraging existing\nreviews to model user interests and item characteristics for rating prediction.\nMost of the existing studies mainly encounter two issues. First, the rich\nknowledge contained in the fine-grained aspects of each review and the\nknowledge graph is rarely considered to complement the pure text for better\nmodeling user-item interactions. Second, the power of pre-trained language\nmodels is not carefully studied for personalized review-based rating\nprediction. To address these issues, we propose an approach named\nKnowledge-aware Collaborative Filtering with Pre-trained Language Model\n(KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a\ntransformer network to model the interactions of the extracted aspects w.r.t. a\nuser-item pair. For the second issue, to better represent users and items,\nKCF-PLM takes all the historical reviews of a user or an item as input to\npre-trained language models. Moreover, KCF-PLM integrates the transformer\nnetwork and the pre-trained language models through representation propagation\non the knowledge graph and user-item guided attention of the aspect\nrepresentations. Thus KCF-PLM combines review text, aspect, knowledge graph,\nand pre-trained language models together for review-based rating prediction. We\nconduct comprehensive experiments on several public datasets, demonstrating the\neffectiveness of KCF-PLM.",
    "Click-Through Rate (CTR) prediction has been an indispensable component for\nmany industrial applications, such as recommendation systems and online\nadvertising. CTR prediction systems are usually based on multi-field\ncategorical features, i.e., every feature is categorical and belongs to one and\nonly one field. Modeling feature conjunctions is crucial for CTR prediction\naccuracy. However, it requires a massive number of parameters to explicitly\nmodel all feature conjunctions, which is not scalable for real-world production\nsystems. In this paper, we describe a novel Field-Leveraged Embedding Network\n(FLEN) which has been deployed in the commercial recommender system in Meitu\nand serves the main traffic. FLEN devises a field-wise bi-interaction pooling\ntechnique. By suitably exploiting field information, the field-wise\nbi-interaction pooling captures both inter-field and intra-field feature\nconjunctions with a small number of model parameters and an acceptable time\ncomplexity for industrial applications. We show that a variety of\nstate-of-the-art CTR models can be expressed under this technique. Furthermore,\nwe develop Dicefactor: a dropout technique to prevent independent latent\nfeatures from co-adapting. Extensive experiments, including offline evaluations\nand online A/B testing on real production systems, demonstrate the\neffectiveness and efficiency of FLEN against the state-of-the-arts. Notably,\nFLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and\ncomputation time, compared to last version (i.e. NFM).",
    "Information retrieval systems retrieves relevant documents based on a query\nsubmitted by the user. The documents are initially indexed and the words in the\ndocuments are assigned weights using a weighting technique called TFIDF which\nis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF\nrepresents the number of occurrences of a term in a document. IDF measures\nwhether the term is common or rare across all documents. It is computed by\ndividing the total number of documents in the system by the number of documents\ncontaining the term and then computing the logarithm of the quotient. By\ndefault, we use base 10 to calculate the logarithm. In this paper, we are going\nto test this weighting technique by using a range of log bases from 0.1 to\n100.0 to calculate the IDF. Testing different log bases for vector model\nweighting technique is to highlight the importance of understanding the\nperformance of the system at different weighting values. We use the documents\nof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled\nexplicitly for experiments in data information retrieval systems.",
    "News recommendation is different from movie or e-commercial recommendation as\npeople usually do not grade the news. Therefore, user feedback for news is\nalways implicit (click behavior, reading time, etc). Inevitably, there are\nnoises in implicit feedback. On one hand, the user may exit immediately after\nclicking the news as he dislikes the news content, leaving the noise in his\npositive implicit feedback; on the other hand, the user may be recommended\nmultiple interesting news at the same time and only click one of them,\nproducing the noise in his negative implicit feedback. Opposite implicit\nfeedback could construct more integrated user preferences and help each other\nto minimize the noise influence. Previous works on news recommendation only\nused positive implicit feedback and suffered from the noise impact. In this\npaper, we propose a denoising neural network for news recommendation with\npositive and negative implicit feedback, named DRPN. DRPN utilizes both\nfeedback for recommendation with a module to denoise both positive and negative\nimplicit feedback to further enhance the performance. Experiments on the\nreal-world large-scale dataset demonstrate the state-of-the-art performance of\nDRPN.",
    "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.",
    "A framework named Copula Component Analysis (CCA) for blind source separation\nis proposed as a generalization of Independent Component Analysis (ICA). It\ndiffers from ICA which assumes independence of sources that the underlying\ncomponents may be dependent with certain structure which is represented by\nCopula. By incorporating dependency structure, much accurate estimation can be\nmade in principle in the case that the assumption of independence is\ninvalidated. A two phrase inference method is introduced for CCA which is based\non the notion of multidimensional ICA.",
    "This paper learns multi-modal embeddings from text, audio, and video\nviews/modes of data in order to improve upon down-stream sentiment\nclassification. The experimental framework also allows investigation of the\nrelative contributions of the individual views in the final multi-modal\nembedding. Individual features derived from the three views are combined into a\nmulti-modal embedding using Deep Canonical Correlation Analysis (DCCA) in two\nways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns text embeddings\nusing BERT, the current state-of-the-art in text encoders. We posit that this\nhighly optimized algorithm dominates over the contribution of other views,\nthough each view does contribute to the final result. Classification tasks are\ncarried out on two benchmark datasets and on a new Debate Emotion data set, and\ntogether these demonstrate that the one-Step DCCA outperforms the current\nstate-of-the-art in learning multi-modal embeddings.",
    "Point-of-Interest (POI) recommendation plays a vital role in various\nlocation-aware services. It has been observed that POI recommendation is driven\nby both sequential and geographical influences. However, since there is no\nannotated label of the dominant influence during recommendation, existing\nmethods tend to entangle these two influences, which may lead to sub-optimal\nrecommendation performance and poor interpretability. In this paper, we address\nthe above challenge by proposing DisenPOI, a novel Disentangled dual-graph\nframework for POI recommendation, which jointly utilizes sequential and\ngeographical relationships on two separate graphs and disentangles the two\ninfluences with self-supervision. The key novelty of our model compared with\nexisting approaches is to extract disentangled representations of both\nsequential and geographical influences with contrastive learning. To be\nspecific, we construct a geographical graph and a sequential graph based on the\ncheck-in sequence of a user. We tailor their propagation schemes to become\nsequence-/geo-aware to better capture the corresponding influences. Preference\nproxies are extracted from check-in sequence as pseudo labels for the two\ninfluences, which supervise the disentanglement via a contrastive loss.\nExtensive experiments on three datasets demonstrate the superiority of the\nproposed model.",
    "We enhance the autonomy of the continuous active learning method shown by\nCormack and Grossman (SIGIR 2014) to be effective for technology-assisted\nreview, in which documents from a collection are retrieved and reviewed, using\nrelevance feedback, until substantially all of the relevant documents have been\nreviewed. Autonomy is enhanced through the elimination of topic-specific and\ndataset-specific tuning parameters, so that the sole input required by the user\nis, at the outset, a short query, topic description, or single relevant\ndocument; and, throughout the review, ongoing relevance assessments of the\nretrieved documents. We show that our enhancements consistently yield superior\nresults to Cormack and Grossman's version of continuous active learning, and\nother methods, not only on average, but on the vast majority of topics from\nfour separate sets of tasks: the legal datasets examined by Cormack and\nGrossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and\nthe construction of the TREC 2002 filtering test collection.",
    "Recently, deep neural networks such as RNN, CNN and Transformer have been\napplied in the task of sequential recommendation, which aims to capture the\ndynamic preference characteristics from logged user behavior data for accurate\nrecommendation. However, in online platforms, logged user behavior data is\ninevitable to contain noise, and deep recommendation models are easy to overfit\non these logged data. To tackle this problem, we borrow the idea of filtering\nalgorithms from signal processing that attenuates the noise in the frequency\ndomain. In our empirical experiments, we find that filtering algorithms can\nsubstantially improve representative sequential recommendation models, and\nintegrating simple filtering algorithms (eg Band-Stop Filter) with an all-MLP\narchitecture can even outperform competitive Transformer-based models.\nMotivated by it, we propose \\textbf{FMLP-Rec}, an all-MLP model with learnable\nfilters for sequential recommendation task. The all-MLP architecture endows our\nmodel with lower time complexity, and the learnable filters can adaptively\nattenuate the noise information in the frequency domain. Extensive experiments\nconducted on eight real-world datasets demonstrate the superiority of our\nproposed method over competitive RNN, CNN, GNN and Transformer-based methods.\nOur code and data are publicly available at the link:\n\\textcolor{blue}{\\url{https://github.com/RUCAIBox/FMLP-Rec}}.",
    "In the rapidly evolving field of artificial intelligence, transformer-based\nmodels have gained significant attention in the context of Sequential\nRecommender Systems (SRSs), demonstrating remarkable proficiency in capturing\nuser-item interactions. However, such attention-based frameworks result in\nsubstantial computational overhead and extended inference time. To address this\nproblem, this paper proposes a novel efficient sequential recommendation\nframework GLINT-RU that leverages dense selective Gated Recurrent Units (GRU)\nmodule to accelerate the inference speed, which is a pioneering work to further\nexploit the potential of efficient GRU modules in SRSs. The GRU module lies at\nthe heart of GLINT-RU, playing a crucial role in substantially reducing both\ninference time and GPU memory usage. Through the integration of a dense\nselective gate, our framework adeptly captures both long-term and short-term\nitem dependencies, enabling the adaptive generation of item scores. GLINT-RU\nfurther integrates a mixing block, enriching it with global user-item\ninteraction information to bolster recommendation quality. Moreover, we design\na gated Multi-layer Perceptron (MLP) for our framework where the information is\ndeeply filtered. Extensive experiments on three datasets are conducted to\nhighlight the effectiveness and efficiency of GLINT-RU. Our GLINT-RU achieves\nexceptional inference speed and prediction accuracy, outperforming existing\nbaselines based on Recurrent Neural Network (RNN), Transformer, MLP and State\nSpace Model (SSM). These results establish a new standard in sequential\nrecommendation, highlighting the potential of GLINT-RU as a renewing approach\nin the realm of recommender systems.",
    "Clinical coding is an administrative process that involves the translation of\ndiagnostic data from episodes of care into a standard code format such as\nICD10. It has many critical applications such as billing and aetiology\nresearch. The automation of clinical coding is very challenging due to data\nsparsity, low interoperability of digital health systems, complexity of\nreal-life diagnosis coupled with the huge size of ICD10 code space. Related\nwork suffer from low applicability due to reliance on many data sources,\ninefficient modelling and less generalizable solutions. We propose a novel\ncollaborative residual learning based model to automatically predict ICD10\ncodes employing only prescriptions data. Extensive experiments were performed\non two real-world clinical datasets (outpatient & inpatient) from Maharaj\nNakorn Chiang Mai Hospital with real case-mix distributions. We obtain\nmulti-label classification accuracy of 0.71 and 0.57 of average precision, 0.57\nand 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal\ndiagnosis for inpatient and outpatient datasets respectively.",
    "The need for diversification of recommendation lists manifests in a number of\nrecommender systems use cases. However, an increase in diversity may undermine\nthe utility of the recommendations, as relevant items in the list may be\nreplaced by more diverse ones. In this work we propose a novel method for\nmaximizing the utility of the recommended items subject to the diversity of\nuser's tastes, and show that an optimal solution to this problem can be found\ngreedily. We evaluate the proposed method in two online user studies as well as\nin an offline analysis incorporating a number of evaluation metrics. The\nresults of evaluations show the superiority of our method over a number of\nbaselines.",
    "Prior work on English monolingual retrieval has shown that a cross-encoder\ntrained using a large number of relevance judgments for query-document pairs\ncan be used as a teacher to train more efficient, but similarly effective,\ndual-encoder student models. Applying a similar knowledge distillation approach\nto training an efficient dual-encoder model for Cross-Language Information\nRetrieval (CLIR), where queries and documents are in different languages, is\nchallenging due to the lack of a sufficiently large training collection when\nthe query and document languages differ. The state of the art for CLIR thus\nrelies on translating queries, documents, or both from the large English MS\nMARCO training set, an approach called Translate-Train. This paper proposes an\nalternative, Translate-Distill, in which knowledge distillation from either a\nmonolingual cross-encoder or a CLIR cross-encoder is used to train a\ndual-encoder CLIR student model. This richer design space enables the teacher\nmodel to perform inference in an optimized setting, while training the student\nmodel directly for CLIR. Trained models and artifacts are publicly available on\nHuggingface.",
    "Assessing the trustworthiness of artificial intelligence systems requires\nknowledge from many different disciplines. These disciplines do not necessarily\nshare concepts between them and might use words with different meanings, or\neven use the same words differently. Additionally, experts from different\ndisciplines might not be aware of specialized terms readily used in other\ndisciplines. Therefore, a core challenge of the assessment process is to\nidentify when experts from different disciplines talk about the same problem\nbut use different terminologies. In other words, the problem is to group\nproblem descriptions (a.k.a. issues) with the same semantic meaning but\ndescribed using slightly different terminologies.\n  In this work, we show how we employed recent advances in natural language\nprocessing, namely sentence embeddings and semantic textual similarity, to\nsupport this identification process and to bridge communication gaps in\ninterdisciplinary teams of experts assessing the trustworthiness of an\nartificial intelligence system used in healthcare.",
    "Recommender systems help users deal with information overload by providing\ntailored item suggestions to them. The recommendation of news is often\nconsidered to be challenging, since the relevance of an article for a user can\ndepend on a variety of factors, including the user's short-term reading\ninterests, the reader's context, or the recency or popularity of an article.\nPrevious work has shown that the use of Recurrent Neural Networks is promising\nfor the next-in-session prediction task, but has certain limitations when only\nrecorded item click sequences are used as input. In this work, we present a\ncontextual hybrid, deep learning based approach for session-based news\nrecommendation that is able to leverage a variety of information types. We\nevaluated our approach on two public datasets, using a temporal evaluation\nprotocol that simulates the dynamics of a news portal in a realistic way. Our\nresults confirm the benefits of considering additional types of information,\nincluding article popularity and recency, in the proposed way, resulting in\nsignificantly higher recommendation accuracy and catalog coverage than other\nsession-based algorithms. Additional experiments show that the proposed\nparameterizable loss function used in our method also allows us to balance two\nusually conflicting quality factors, accuracy and novelty.\n  Keywords: Artificial Neural Networks, Context-Aware Recommender Systems,\nHybrid Recommender Systems, News Recommender Systems, Session-based\nRecommendation",
    "This paper describes PinView, a content-based image retrieval system that\nexploits implicit relevance feedback collected during a search session. PinView\ncontains several novel methods to infer the intent of the user. From relevance\nfeedback, such as eye movements or pointer clicks, and visual features of\nimages, PinView learns a similarity metric between images which depends on the\ncurrent interests of the user. It then retrieves images with a specialized\nonline learning algorithm that balances the tradeoff between exploring new\nimages and exploiting the already inferred interests of the user. We have\nintegrated PinView to the content-based image retrieval system PicSOM, which\nenables applying PinView to real-world image databases. With the new algorithms\nPinView outperforms the original PicSOM, and in online experiments with real\nusers the combination of implicit and explicit feedback gives the best results.",
    "A long user history inevitably reflects the transitions of personal interests\nover time. The analyses on the user history require the robust sequential model\nto anticipate the transitions and the decays of user interests. The user\nhistory is often modeled by various RNN structures, but the RNN structures in\nthe recommendation system still suffer from the long-term dependency and the\ninterest drifts. To resolve these challenges, we suggest HCRNN with three\nhierarchical contexts of the global, the local, and the temporary interests.\nThis structure is designed to withhold the global long-term interest of users,\nto reflect the local sub-sequence interests, and to attend the temporary\ninterests of each transition. Besides, we propose a hierarchical context-based\ngate structure to incorporate our \\textit{interest drift assumption}. As we\nsuggest a new RNN structure, we support HCRNN with a complementary\n\\textit{bi-channel attention} structure to utilize hierarchical context. We\nexperimented the suggested structure on the sequential recommendation tasks\nwith CiteULike, MovieLens, and LastFM, and our model showed the best\nperformances in the sequential recommendations.",
    "We present a novel summarization framework for reviews of products and\nservices by selecting informative and concise text segments from the reviews.\nOur method consists of two major steps. First, we identify five frequently\noccurring variable-length syntactic patterns and use them to extract candidate\nsegments. Then we use the output of a joint generative sentiment topic model to\nfilter out the non-informative segments. We verify the proposed method with\nquantitative and qualitative experiments. In a quantitative study, our approach\noutperforms previous methods in producing informative segments and summaries\nthat capture aspects of products and services as expressed in the\nuser-generated pros and cons lists. Our user study with ninety users resonates\nwith this result: individual segments extracted and filtered by our method are\nrated as more useful by users compared to previous approaches by users.",
    "While entity-oriented neural IR models have advanced significantly, they\noften overlook a key nuance: the varying degrees of influence individual\nentities within a document have on its overall relevance. Addressing this gap,\nwe present DREQ, an entity-oriented dense document re-ranking model. Uniquely,\nwe emphasize the query-relevant entities within a document's representation\nwhile simultaneously attenuating the less relevant ones, thus obtaining a\nquery-specific entity-centric document representation. We then combine this\nentity-centric document representation with the text-centric representation of\nthe document to obtain a \"hybrid\" representation of the document. We learn a\nrelevance score for the document using this hybrid representation. Using four\nlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neural\nand non-neural re-ranking methods, highlighting the effectiveness of our\nentity-oriented representation approach.",
    "We consider the problem of ranking $N$ objects starting from a set of noisy\npairwise comparisons provided by a crowd of equal workers. We assume that\nobjects are endowed with intrinsic qualities and that the probability with\nwhich an object is preferred to another depends only on the difference between\nthe qualities of the two competitors. We propose a class of non-adaptive\nranking algorithms that rely on a least-squares optimization criterion for the\nestimation of qualities. Such algorithms are shown to be asymptotically optimal\n(i.e., they require $O(\\frac{N}{\\epsilon^2}\\log \\frac{N}{\\delta})$ comparisons\nto be $(\\epsilon, \\delta)$-PAC). Numerical results show that our schemes are\nvery efficient also in many non-asymptotic scenarios exhibiting a performance\nsimilar to the maximum-likelihood algorithm. Moreover, we show how they can be\nextended to adaptive schemes and test them on real-world datasets.",
    "A significant amount of search queries originate from some real world\ninformation need or tasks. In order to improve the search experience of the end\nusers, it is important to have accurate representations of tasks. As a result,\nsignificant amount of research has been devoted to extracting proper\nrepresentations of tasks in order to enable search systems to help users\ncomplete their tasks, as well as providing the end user with better query\nsuggestions, for better recommendations, for satisfaction prediction, and for\nimproved personalization in terms of tasks. Most existing task extraction\nmethodologies focus on representing tasks as flat structures. However, tasks\noften tend to have multiple subtasks associated with them and a more\nnaturalistic representation of tasks would be in terms of a hierarchy, where\neach task can be composed of multiple (sub)tasks. To this end, we propose an\nefficient Bayesian nonparametric model for extracting hierarchies of such tasks\n\\& subtasks. We evaluate our method based on real world query log data both\nthrough quantitative and crowdsourced experiments and highlight the importance\nof considering task/subtask hierarchies.",
    "Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.",
    "Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to\nguarantee multiple (often conflicting) goals. Besides accuracy, a MORS can\noperate at the global level, where additional beyond-accuracy goals are met for\nthe system as a whole, or at the individual level, meaning that the\nrecommendations are tailored to the needs of each user. The state-of-the-art\nMORSs either operate at the global or individual level, without assuming the\nco-existence of the two perspectives. In this study, we show that when global\nand individual objectives co-exist, MORSs are not able to meet both types of\ngoals. To overcome this issue, we present an approach that regulates the\nrecommendation lists so as to guarantee both global and individual\nperspectives, while preserving its effectiveness. Specifically, as individual\nperspective, we tackle genre calibration and, as global perspective, provider\nfairness. We validate our approach on two real-world datasets, publicly\nreleased with this paper.",
    "Understanding and analyzing big data is firmly recognized as a powerful and\nstrategic priority. For deeper interpretation of and better intelligence with\nbig data, it is important to transform raw data (unstructured, semi-structured\nand structured data sources, e.g., text, video, image data sets) into curated\ndata: contextualized data and knowledge that is maintained and made available\nfor use by end-users and applications. In particular, data curation acts as the\nglue between raw data and analytics, providing an abstraction layer that\nrelieves users from time consuming, tedious and error prone curation tasks. In\nthis context, the data curation process becomes a vital analytics asset for\nincreasing added value and insights.\n  In this paper, we identify and implement a set of curation APIs and make them\navailable (on GitHub) to researchers and developers to assist them transforming\ntheir raw data into curated data. The curation APIs enable developers to easily\nadd features - such as extracting keyword, part of speech, and named entities\nsuch as Persons, Locations, Organizations, Companies, Products, Diseases,\nDrugs, etc.; providing synonyms and stems for extracted information items\nleveraging lexical knowledge bases for the English language such as WordNet;\nlinking extracted entities to external knowledge bases such as Google Knowledge\nGraph and Wikidata; discovering similarity among the extracted information\nitems, such as calculating similarity between string, number, date and time\ndata; classifying, sorting and categorizing data into various types, forms or\nany other distinct class; and indexing structured and unstructured data - into\ntheir applications.",
    "We present a study on predicting the factuality of reporting and bias of news\nmedia. While previous work has focused on studying the veracity of claims or\ndocuments, here we are interested in characterizing entire news media. These\nare under-studied but arguably important research problems, both in their own\nright and as a prior for fact-checking systems. We experiment with a large list\nof news websites and with a rich set of features derived from (i) a sample of\narticles from the target news medium, (ii) its Wikipedia page, (iii) its\nTwitter account, (iv) the structure of its URL, and (v) information about the\nWeb traffic it attracts. The experimental results show sizable performance\ngains over the baselines, and confirm the importance of each feature type.",
    "To make Sequential Recommendation (SR) successful, recent works focus on\ndesigning effective sequential encoders, fusing side information, and mining\nextra positive self-supervision signals. The strategy of sampling negative\nitems at each time step is less explored. Due to the dynamics of users'\ninterests and model updates during training, considering randomly sampled items\nfrom a user's non-interacted item set as negatives can be uninformative. As a\nresult, the model will inaccurately learn user preferences toward items.\nIdentifying informative negatives is challenging because informative negative\nitems are tied with both dynamically changed interests and model parameters\n(and sampling process should also be efficient). To this end, we propose to\nGenerate Negative Samples (items) for SR (GenNi). A negative item is sampled at\neach time step based on the current SR model's learned user preferences toward\nitems. An efficient implementation is proposed to further accelerate the\ngeneration process, making it scalable to large-scale recommendation tasks.\nExtensive experiments on four public datasets verify the importance of\nproviding high-quality negative samples for SR and demonstrate the\neffectiveness and efficiency of GenNi.",
    "Recommender systems recommend items more accurately by analyzing users'\npotential interest on different brands' items. In conjunction with users'\nrating similarity, the presence of users' implicit feedbacks like clicking\nitems, viewing items specifications, watching videos etc. have been proved to\nbe helpful for learning users' embedding, that helps better rating prediction\nof users. Most existing recommender systems focus on modeling of ratings and\nimplicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can\nbe used to validate the reliability of the particular users and can be used to\nlearn about the users' characteristic. Users' characteristic mean what type of\nreviewers they are. In this paper, we explore three different models for\nrecommendation with more accuracy focusing on users' explicit feedbacks and\nimplicit feedbacks. First one is RHC-PMF that predicts users' rating more\naccurately based on user's three explicit feedbacks (rating, helpfulness score\nand centrality) and second one is RV-PMF, where user's implicit feedback (view\nrelationship) is considered. Last one is RHCV-PMF, where both type of feedbacks\nare considered. In this model users' explicit feedbacks' similarity indicate\nthe similarity of their reliability and characteristic and implicit feedback's\nsimilarity indicates their preference similarity. Extensive experiments on real\nworld dataset, i.e. Amazon.com online review dataset shows that our models\nperform better compare to base-line models in term of users' rating prediction.\nRHCV-PMF model also performs better rating prediction compare to baseline\nmodels for cold start users and cold start items.",
    "We present a method for a wine recommendation system that employs\nmultidimensional clustering and unsupervised learning methods. Our algorithm\nfirst performs clustering on a large corpus of wine reviews. It then uses the\nresulting wine clusters as an approximation of the most common flavor palates,\nrecommending a user a wine by optimizing over a price-quality ratio within\nclusters that they demonstrated a preference for.",
    "More than ever, technical inventions are the symbol of our society's advance.\nPatents guarantee their creators protection against infringement. For an\ninvention being patentable, its novelty and inventiveness have to be assessed.\nTherefore, a search for published work that describes similar inventions to a\ngiven patent application needs to be performed. Currently, this so-called\nsearch for prior art is executed with semi-automatically composed keyword\nqueries, which is not only time consuming, but also prone to errors. In\nparticular, errors may systematically arise by the fact that different keywords\nfor the same technical concepts may exist across disciplines. In this paper, a\nnovel approach is proposed, where the full text of a given patent application\nis compared to existing patents using machine learning and natural language\nprocessing techniques to automatically detect inventions that are similar to\nthe one described in the submitted document. Various state-of-the-art\napproaches for feature extraction and document comparison are evaluated. In\naddition to that, the quality of the current search process is assessed based\non ratings of a domain expert. The evaluation results show that our automated\napproach, besides accelerating the search process, also improves the search\nresults for prior art with respect to their quality.",
    "The problem of disambiguation of company names poses a significant challenge\nin extracting useful information from patents. This issue biases research\noutcomes as it mostly underestimates the number of patents attributed to\ncompanies, particularly multinational corporations which file patents under a\nplethora of names, including alternate spellings of the same entity and,\neventually, companies' subsidiaries. To date, addressing these challenges has\nrelied on labor-intensive dictionary based or string matching approaches,\nleaving the problem of patents' assignee harmonization on large datasets mostly\nunresolved. To bridge this gap, this paper describes the Terrorizer algorithm,\na text-based algorithm that leverages natural language processing (NLP),\nnetwork theory, and rule-based techniques to harmonize the variants of company\nnames recorded as patent assignees. In particular, the algorithm follows the\ntripartite structure of its antecedents, namely parsing, matching and filtering\nstage, adding an original \"knowledge augmentation\" phase which is used to\nenrich the information available on each assignee name. We use Terrorizer on a\nset of 325'917 companies' names who are assignees of patents granted by the\nUSPTO from 2005 to 2022. The performance of Terrorizer is evaluated on four\ngold standard datasets. This validation step shows us two main things: the\nfirst is that the performance of Terrorizer is similar over different kind of\ndatasets, proving that our algorithm generalizes well. Second, when comparing\nits performance with the one of the algorithm currently used in PatentsView for\nthe same task (Monath et al., 2021), it achieves a higher F1 score. Finally, we\nuse the Tree-structured Parzen Estimator (TPE) optimization algorithm for the\nhyperparameters' tuning. Our final result is a reduction in the initial set of\nnames of over 42%.",
    "Searching, browsing, and recommendations are common ways in which the \"choice\noverload\" faced by users in the online marketplace can be mitigated. In this\npaper we propose the use of hierarchical item categories, obtained from\nimplicit feedback data, to enable efficient browsing and recommendations. We\npresent a method of creating hierarchical item categories from implicit\nfeedback data only i.e., without any other information on the items like name,\ngenre etc. Categories created in this fashion are based on users'\nco-consumption of items. Thus, they can be more useful for users in finding\ninteresting and relevant items while they are browsing through the hierarchy.\nWe also show that this item hierarchy can be useful in making category based\nrecommendations, which makes the recommendations more explainable and increases\nthe diversity of the recommendations without compromising much on the accuracy.\nItem hierarchy can also be useful in the creation of an automatic item taxonomy\nskeleton by bypassing manual labeling and annotation. This can especially be\nuseful for small vendors. Our data-driven hierarchical categories are based on\nhierarchical latent tree analysis (HLTA) which has been previously used for\ntext analysis. We present a scaled up learning algorithm \\emph{HLTA-Forest} so\nthat HLTA can be applied to implicit feedback data.",
    "The integration of Linked Open Data (LOD) content in Web pages is a\nchallenging and sometimes tedious task for Web developers. At the same moment,\nmost software packages for blogs, content management systems (CMS), and shop\napplications support the consumption of feed formats, namely RSS and Atom. In\nthis technical report, we demonstrate an on-line tool that fetches e-commerce\ndata from a SPARQL endpoint and syndicates obtained results as RSS or Atom\nfeeds. Our approach combines (1) the popularity and broad tooling support of\nexisting feed formats, (2) the precision of queries against structured data\nbuilt upon common Web vocabularies like schema.org, GoodRelations, FOAF, VCard,\nand WGS 84, and (3) the ease of integrating content from a large number of Web\nsites and other data sources in RDF in general.",
    "Recommendation techniques are important approaches for alleviating\ninformation overload. Being often trained on implicit user feedback, many\nrecommenders suffer from the sparsity challenge due to the lack of explicitly\nnegative samples. The GAN-style recommenders (i.e., IRGAN) addresses the\nchallenge by learning a generator and a discriminator adversarially, such that\nthe generator produces increasingly difficult samples for the discriminator to\naccelerate optimizing the discrimination objective. However, producing samples\nfrom the generator is very time-consuming, and our empirical study shows that\nthe discriminator performs poor in top-k item recommendation. To this end, a\ntheoretical analysis is made for the GAN-style algorithms, showing that the\ngenerator of limit capacity is diverged from the optimal generator. This may\ninterpret the limitation of discriminator's performance. Based on these\nfindings, we propose a Sampling-Decomposable Generative Adversarial Recommender\n(SD-GAR). In the framework, the divergence between some generator and the\noptimum is compensated by self-normalized importance sampling; the efficiency\nof sample generation is improved with a sampling-decomposable generator, such\nthat each sample can be generated in O(1) with the Vose-Alias method.\nInterestingly, due to decomposability of sampling, the generator can be\noptimized with the closed-form solutions in an alternating manner, being\ndifferent from policy gradient in the GAN-style algorithms. We extensively\nevaluate the proposed algorithm with five real-world recommendation datasets.\nThe results show that SD-GAR outperforms IRGAN by 12.4% and the SOTA\nrecommender by 10% on average. Moreover, discriminator training can be 20x\nfaster on the dataset with more than 120K items.",
    "Providing visual summaries of scientific publications can increase\ninformation access for readers and thereby help deal with the exponential\ngrowth in the number of scientific publications. Nonetheless, efforts in\nproviding visual publication summaries have been few and far apart, primarily\nfocusing on the biomedical domain. This is primarily because of the limited\navailability of annotated gold standards, which hampers the application of\nrobust and high-performing supervised learning techniques. To address these\nproblems we create a new benchmark dataset for selecting figures to serve as\nvisual summaries of publications based on their abstracts, covering several\ndomains in computer science. Moreover, we develop a self-supervised learning\napproach, based on heuristic matching of inline references to figures with\nfigure captions. Experiments in both biomedical and computer science domains\nshow that our model is able to outperform the state of the art despite being\nself-supervised and therefore not relying on any annotated training data.",
    "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a\nlarge scale of in-domain relevance training signals, which are not always\navailable in real-world ranking scenarios. To democratize the benefits of\nNeu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method\nthat generalizes Neu-IR models from label-rich source domains to few-shot\ntarget domains. Drawing on source-domain massive relevance supervision,\nMetaAdaptRank contrastively synthesizes a large number of weak supervision\nsignals for target domains and meta-learns to reweight these synthetic \"weak\"\ndata based on their benefits to the target-domain ranking accuracy of Neu-IR\nmodels. Experiments on three TREC benchmarks in the web, news, and biomedical\ndomains show that MetaAdaptRank significantly improves the few-shot ranking\naccuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives\nfrom both its contrastive weak data synthesis and meta-reweighted data\nselection. The code and data of this paper can be obtained from\nhttps://github.com/thunlp/MetaAdaptRank.",
    "Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.",
    "Collaborative filtering (CF), as a fundamental approach for recommender\nsystems, is usually built on the latent factor model with learnable parameters\nto predict users' preferences towards items. However, designing a proper CF\nmodel for a given data is not easy, since the properties of datasets are highly\ndiverse. In this paper, motivated by the recent advances in automated machine\nlearning (AutoML), we propose to design a data-specific CF model by AutoML\ntechniques. The key here is a new framework that unifies state-of-the-art\n(SOTA) CF methods and splits them into disjoint stages of input encoding,\nembedding function, interaction function, and prediction function. We further\ndevelop an easy-to-use, robust, and efficient search strategy, which utilizes\nrandom search and a performance predictor for efficient searching within the\nabove framework. In this way, we can combinatorially generalize data-specific\nCF models, which have not been visited in the literature, from SOTA ones.\nExtensive experiments on five real-world datasets demonstrate that our method\ncan consistently outperform SOTA ones for various CF tasks. Further experiments\nverify the rationality of the proposed framework and the efficiency of the\nsearch strategy. The searched CF models can also provide insights for exploring\nmore effective methods in the future",
    "Session length is a very important aspect in determining a user's\nsatisfaction with a media streaming service. Being able to predict how long a\nsession will last can be of great use for various downstream tasks, such as\nrecommendations and ad scheduling. Most of the related literature on user\ninteraction duration has focused on dwell time for websites, usually in the\ncontext of approximating post-click satisfaction either in search results, or\ndisplay ads. In this work we present the first analysis of session length in a\nmobile-focused online service, using a real world data-set from a major music\nstreaming service. We use survival analysis techniques to show that the\ncharacteristics of the length distributions can differ significantly between\nusers, and use gradient boosted trees with appropriate objectives to predict\nthe length of a session using only information available at its beginning. Our\nevaluation on real world data illustrates that our proposed technique\noutperforms the considered baseline.",
    "In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.",
    "The classification of television content helps users organise and navigate\nthrough the large list of channels and programs now available. In this paper,\nwe address the problem of television content classification by exploiting text\ninformation extracted from program transcriptions. We present an analysis which\nadapts a model for sentiment that has been widely and successfully applied in\nother fields such as music or blog posts. We use a real-world dataset obtained\nfrom the Boxfish API to compare the performance of classifiers trained on a\nnumber of different feature sets. Our experiments show that, over a large\ncollection of television content, program genres can be represented in a\nthree-dimensional space of valence, arousal and dominance, and that promising\nclassification results can be achieved using features based on this\nrepresentation. This finding supports the use of the proposed representation of\ntelevision content as a feature space for similarity computation and\nrecommendation generation.",
    "Recommender systems aim to find an accurate and efficient mapping from\nhistoric data of user-preferred items to a new item that is to be liked by a\nuser. Towards this goal, energy-based sequence generative adversarial nets\n(EB-SeqGANs) are adopted for recommendation by learning a generative model for\nthe time series of user-preferred items. By recasting the energy function as\nthe feature function, the proposed EB-SeqGANs is interpreted as an instance of\nmaximum-entropy imitation learning.",
    "Reinforcement learning based recommender systems (RL-based RS) aim at\nlearning a good policy from a batch of collected data, by casting\nrecommendations to multi-step decision-making tasks. However, current RL-based\nRS research commonly has a large reality gap. In this paper, we introduce the\nfirst open-source real-world dataset, RL4RS, hoping to replace the artificial\ndatasets and semi-simulated RS datasets previous studies used due to the\nresource limitation of the RL-based RS domain. Unlike academic RL research,\nRL-based RS suffers from the difficulties of being well-validated before\ndeployment. We attempt to propose a new systematic evaluation framework,\nincluding evaluation of environment simulation, evaluation on environments,\ncounterfactual policy evaluation, and evaluation on environments built from\ntest set. In summary, the RL4RS (Reinforcement Learning for Recommender\nSystems), a new resource with special concerns on the reality gaps, contains\ntwo real-world datasets, data understanding tools, tuned simulation\nenvironments, related advanced RL baselines, batch RL baselines, and\ncounterfactual policy evaluation algorithms. The RL4RS suite can be found at\nhttps://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender\nsystems, we expect the resource to contribute to research in applied\nreinforcement learning.",
    "With the prevalence of Internet of Things (IoT)-based social media\napplications, the distance among people has been greatly shortened. As a\nresult, recommender systems in IoT-based social media need to be developed\noriented to groups of users rather than individual users. However, existing\nmethods were highly dependent on explicit preference feedbacks, ignoring\nscenarios of implicit feedback. To remedy such gap, this paper proposes an\nimplicit feedback-based group recommender system using probabilistic inference\nand non-cooperative game(GREPING) for IoT-based social media. Particularly,\nunknown process variables can be estimated from observable implicit feedbacks\nvia Bayesian posterior probability inference. In addition, the globally optimal\nrecommendation results can be calculated with the aid of non-cooperative game.\nTwo groups of experiments are conducted to assess the GREPING from two aspects:\nefficiency and robustness. Experimental results show obvious promotion and\nconsiderable stability of the GREPING compared to baseline methods.",
    "This paper explores meta-learning in sequential recommendation to alleviate\nthe item cold-start problem. Sequential recommendation aims to capture user's\ndynamic preferences based on historical behavior sequences and acts as a key\ncomponent of most online recommendation scenarios. However, most previous\nmethods have trouble recommending cold-start items, which are prevalent in\nthose scenarios. As there is generally no side information in the setting of\nsequential recommendation task, previous cold-start methods could not be\napplied when only user-item interactions are available. Thus, we propose a\nMeta-learning-based Cold-Start Sequential Recommendation Framework, namely\nMecos, to mitigate the item cold-start problem in sequential recommendation.\nThis task is non-trivial as it targets at an important problem in a novel and\nchallenging context. Mecos effectively extracts user preference from limited\ninteractions and learns to match the target cold-start item with the potential\nuser. Besides, our framework can be painlessly integrated with neural\nnetwork-based models. Extensive experiments conducted on three real-world\ndatasets verify the superiority of Mecos, with the average improvement up to\n99%, 91%, and 70% in HR@10 over state-of-the-art baseline methods.",
    "Keyphrase provides accurate information of document content that is highly\ncompact, concise, full of meanings, and widely used for discourse\ncomprehension, organization, and text retrieval. Though previous studies have\nmade substantial efforts for automated keyphrase extraction and generation,\nsurprisingly, few studies have been made for \\textit{keyphrase completion}\n(KPC). KPC aims to generate more keyphrases for document (e.g. scientific\npublication) taking advantage of document content along with a very limited\nnumber of known keyphrases, which can be applied to improve text indexing\nsystem, etc. In this paper, we propose a novel KPC method with an\nencoder-decoder framework. We name it \\textit{deep keyphrase completion} (DKPC)\nsince it attempts to capture the deep semantic meaning of the document content\ntogether with known keyphrases via a deep learning framework. Specifically, the\nencoder and the decoder in DKPC play different roles to make full use of the\nknown keyphrases. The former considers the keyphrase-guiding factors, which\naggregates information of known keyphrases into context. On the contrary, the\nlatter considers the keyphrase-inhibited factor to inhibit semantically\nrepeated keyphrase generation. Extensive experiments on benchmark datasets\ndemonstrate the efficacy of our proposed model.",
    "Password authentication is a very important system security procedure to gain\naccess to user resources. In the Traditional password authentication methods a\nserver has check the authenticity of the users. In our proposed method users\ncan freely select their passwords from a predefined character set. They can\nalso use a graphical image as password. The password may be a character or an\nimage it will be converted into binary form and the binary values will be\nnormalized. Associative memories have been used recently for password\nauthentication in order to overcome drawbacks of the traditional password\nauthentication methods. In this paper we proposed a method using Bidirectional\nAssociative Memory algorithm for both alphanumeric (Text) and graphical\npassword. By doing so the amount of security what we provide for the user can\nbe enhanced. This paper along with test results show that converting user\npassword in to Probabilistic values and giving them as input for BAM improves\nthe security of the system",
    "Trojan attack on deep neural networks, also known as backdoor attack, is a\ntypical threat to artificial intelligence. A trojaned neural network behaves\nnormally with clean inputs. However, if the input contains a particular\ntrigger, the trojaned model will have attacker-chosen abnormal behavior.\nAlthough many backdoor detection methods exist, most of them assume that the\ndefender has access to a set of clean validation samples or samples with the\ntrigger, which may not hold in some crucial real-world cases, e.g., the case\nwhere the defender is the maintainer of model-sharing platforms. Thus, in this\npaper, we propose FreeEagle, the first data-free backdoor detection method that\ncan effectively detect complex backdoor attacks on deep neural networks,\nwithout relying on the access to any clean samples or samples with the trigger.\nThe evaluation results on diverse datasets and model architectures show that\nFreeEagle is effective against various complex backdoor attacks, even\noutperforming some state-of-the-art non-data-free backdoor detection methods.",
    "We revisit the well-studied problem of differentially private empirical risk\nminimization (ERM). We show that for unconstrained convex generalized linear\nmodels (GLMs), one can obtain an excess empirical risk of $\\tilde\nO\\left(\\sqrt{{\\texttt{rank}}}/\\epsilon n\\right)$, where ${\\texttt{rank}}$ is\nthe rank of the feature matrix in the GLM problem, $n$ is the number of data\nsamples, and $\\epsilon$ is the privacy parameter. This bound is attained via\ndifferentially private gradient descent (DP-GD). Furthermore, via the first\nlower bound for unconstrained private ERM, we show that our upper bound is\ntight. In sharp contrast to the constrained ERM setting, there is no dependence\non the dimensionality of the ambient model space ($p$). (Notice that\n${\\texttt{rank}}\\leq \\min\\{n, p\\}$.) Besides, we obtain an analogous excess\npopulation risk bound which depends on ${\\texttt{rank}}$ instead of $p$.\n  For the smooth non-convex GLM setting (i.e., where the objective function is\nnon-convex but preserves the GLM structure), we further show that DP-GD attains\na dimension-independent convergence of $\\tilde\nO\\left(\\sqrt{{\\texttt{rank}}}/\\epsilon n\\right)$ to a\nfirst-order-stationary-point of the underlying objective.\n  Finally, we show that for convex GLMs, a variant of DP-GD commonly used in\npractice (which involves clipping the individual gradients) also exhibits the\nsame dimension-independent convergence to the minimum of a well-defined\nobjective. To that end, we provide a structural lemma that characterizes the\neffect of clipping on the optimization profile of DP-GD.",
    "In the realm of Artificial Intelligence (AI), the need for privacy and\nsecurity in data processing has become paramount. As AI applications continue\nto expand, the collection and handling of sensitive data raise concerns about\nindividual privacy protection. Federated Learning (FL) emerges as a promising\nsolution to address these challenges by enabling decentralized model training\non local devices, thus preserving data privacy. This paper introduces FLEX: a\nFLEXible Federated Learning Framework designed to provide maximum flexibility\nin FL research experiments. By offering customizable features for data\ndistribution, privacy parameters, and communication strategies, FLEX empowers\nresearchers to innovate and develop novel FL techniques. The framework also\nincludes libraries for specific FL implementations including: (1) anomalies,\n(2) blockchain, (3) adversarial attacks and defences, (4) natural language\nprocessing and (5) decision trees, enhancing its versatility and applicability\nin various domains. Overall, FLEX represents a significant advancement in FL\nresearch, facilitating the development of robust and efficient FL applications.",
    "Machine Learning (ML), addresses a multitude of complex issues in multiple\ndisciplines, including social sciences, finance, and medical research. ML\nmodels require substantial computing power and are only as powerful as the data\nutilized. Due to high computational cost of ML methods, data scientists\nfrequently use Machine Learning-as-a-Service (MLaaS) to outsource computation\nto external servers. However, when working with private information, like\nfinancial data or health records, outsourcing the computation might result in\nprivacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have\nenabled ML training and inference over protected data through the use of\nPrivacy-Preserving Machine Learning (PPML). However, these techniques are still\nat a preliminary stage and their application in real-world situations is\ndemanding. In order to comprehend discrepancy between theoretical research\nsuggestions and actual applications, this work examines the past and present of\nPPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party\nComputation (SMPC) applied to ML. This work primarily focuses on the ML model's\ntraining phase, where maintaining user data privacy is of utmost importance. We\nprovide a solid theoretical background that eases the understanding of current\napproaches and their limitations. In addition, we present a SoK of the most\nrecent PPML frameworks for model training and provide a comprehensive\ncomparison in terms of the unique properties and performances on standard\nbenchmarks. Also, we reproduce the results for some of the papers and examine\nat what level existing works in the field provide support for open science. We\nbelieve our work serves as a valuable contribution by raising awareness about\nthe current gap between theoretical advancements and real-world applications in\nPPML, specifically regarding open-source availability, reproducibility, and\nusability.",
    "Transforming large deep neural network (DNN) models into the multi-exit\narchitectures can overcome the overthinking issue and distribute a large DNN\nmodel on resource-constrained scenarios (e.g. IoT frontend devices and backend\nservers) for inference and transmission efficiency. Nevertheless, intellectual\nproperty (IP) protection for the multi-exit models in the wild is still an\nunsolved challenge. Previous efforts to verify DNN model ownership mainly rely\non querying the model with specific samples and checking the responses, e.g.,\nDNN watermarking and fingerprinting. However, they are vulnerable to\nadversarial settings such as adversarial training and are not suitable for the\nIP verification for multi-exit DNN models. In this paper, we propose a novel\napproach to fingerprint multi-exit models via inference time rather than\ninference predictions. Specifically, we design an effective method to generate\na set of fingerprint samples to craft the inference process with a unique and\nrobust inference time cost as the evidence for model ownership. We conduct\nextensive experiments to prove the uniqueness and robustness of our method on\nthree structures (ResNet-56, VGG-16, and MobileNet) and three datasets\n(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial\nsettings.",
    "The novel Internet of Things (IoT) paradigm is composed of a growing number\nof heterogeneous smart objects and services that are transforming architectures\nand applications, increasing systems' complexity, and the need for reliability\nand autonomy. In this context, both smart objects and services are often\nprovided by third parties which do not give full transparency regarding the\nsecurity and privacy of the features offered. Although machine-based Service\nLevel Agreements (SLA) have been recently leveraged to establish and share\npolicies in Cloud-based scenarios, and also in the IoT context, the issue of\nmaking end users aware of the overall system security levels and the\nfulfillment of their privacy requirements through the provision of the\nrequested service remains a challenging task. To tackle this problem, we\npropose a complete framework that defines suitable levels of privacy and\nsecurity requirements in the acquisition of services in IoT, according to the\nuser needs. Through the use of a Reinforcement Learning based solution, a user\nagent, inside the environment, is trained to choose the best smart objects\ngranting access to the target services. Moreover, the solution is designed to\nguarantee deadline requirements and user security and privacy needs. Finally,\nto evaluate the correctness and the performance of the proposed approach we\nillustrate an extensive experimental analysis.",
    "Healthcare systems are increasingly incorporating Artificial Intelligence\ninto their systems, but it is not a solution for all difficulties. AI's\nextraordinary potential is being held back by challenges such as a lack of\nmedical datasets for training AI models, adversarial attacks, and a lack of\ntrust due to its black box working style. We explored how blockchain technology\ncan improve the reliability and trustworthiness of AI-based healthcare. This\npaper has conducted a Systematic Literature Review to explore the\nstate-of-the-art research studies conducted in healthcare applications\ndeveloped with different AI techniques and Blockchain Technology. This\nsystematic literature review proceeds with three different paths as natural\nlanguage processing-based healthcare systems, computer vision-based healthcare\nsystems and acoustic AI-based healthcare systems. We found that 1) Defence\ntechniques for adversarial attacks on AI are available for specific kind of\nattacks and even adversarial training is AI based technique which in further\nprone to different attacks. 2) Blockchain can address security and privacy\nissues in healthcare fraternity. 3) Medical data verification and user\nprovenance can be enabled with Blockchain. 4) Blockchain can protect\ndistributed learning on heterogeneous medical data. 5) The issues like single\npoint of failure, non-transparency in healthcare systems can be resolved with\nBlockchain. Nevertheless, it has been identified that research is at the\ninitial stage. As a result, we have synthesized a conceptual framework using\nBlockchain Technology for AI-based healthcare applications that considers the\nneeds of each NLP, Computer Vision, and Acoustic AI application. A global\nsolution for all sort of adversarial attacks on AI based healthcare. However,\nthis technique has significant limits and challenges that need to be addressed\nin future studies.",
    "The paper studies how to release data about a critical infrastructure network\n(e.g., the power network or a transportation network) without disclosing\nsensitive information that can be exploited by malevolent agents, while\npreserving the realism of the network. It proposes a novel obfuscation\nmechanism that combines several privacy-preserving building blocks with a\nbi-level optimization model to significantly improve accuracy. The obfuscation\nis evaluated for both realism and privacy properties on real energy and\ntransportation networks. Experimental results show the obfuscation mechanism\nsubstantially reduces the potential damage of an attack exploiting the released\ndata to harm the real network.",
    "In recent years Deep Neural Networks (DNNs) have achieved remarkable results\nand even showed super-human capabilities in a broad range of domains. This led\npeople to trust in DNNs' classifications and resulting actions even in\nsecurity-sensitive environments like autonomous driving.\n  Despite their impressive achievements, DNNs are known to be vulnerable to\nadversarial examples. Such inputs contain small perturbations to intentionally\nfool the attacked model.\n  In this paper, we present a novel end-to-end framework to detect such attacks\nduring classification without influencing the target model's performance.\nInspired by recent research in neuron-coverage guided testing we show that\ndense layers of DNNs carry security-sensitive information. With a secondary DNN\nwe analyze the activation patterns of the dense layers during classification\nruntime, which enables effective and real-time detection of adversarial\nexamples.\n  Our prototype implementation successfully detects adversarial examples in\nimage, natural language, and audio processing. Thereby, we cover a variety of\ntarget DNNs, including Long Short Term Memory (LSTM) architectures. In\naddition, to effectively defend against state-of-the-art attacks, our approach\ngeneralizes between different sets of adversarial examples. Thus, our method\nmost likely enables us to detect even future, yet unknown attacks. Finally,\nduring white-box adaptive attacks, we show our method cannot be easily\nbypassed.",
    "The Dendritic Cell Algorithm (DCA) as one of the emerging evolutionary\nalgorithms is based on the behavior of the specific immune agents; known as\nDendritic Cells (DCs). DCA has several potentially beneficial features for\nbinary classification problems. In this paper, we aim at providing a new\nversion of this immune-inspired mechanism acts as a semi-supervised classifier\nwhich can be a defensive shield in network intrusion detection problem. Till\nnow, no strategy or idea has already been adopted on the GetAntigen() function\non detection phase, but randomly sampling entails the DCA to provide\nundesirable results in several cycles in each time. This leads to uncertainty.\nWhereas it must be accomplished by biological behaviors of DCs in tissues, we\nhave proposed a novel strategy which exactly acts based on its immunological\nfunctionalities of dendritic cells. The proposed mechanism focuses on two\nitems: First, to obviate the challenge of needing to have a preordered antigen\nset for computing danger signal, and the second, to provide a novel\nimmune-inspired idea in order to non-random data sampling. A variable\nfunctional migration threshold is also computed cycle by cycle that shows\nnecessity of the Migration threshold (MT) flexibility. A significant criterion\nso called capability of intrusion detection (CID) used for tests. All of the\ntests have been performed in a new benchmark dataset named UNSW-NB15.\nExperimental consequences demonstrate that the present schema dominates the\nstandard DCA and has higher CID in comparison with other approaches found in\nliterature.",
    "Homomorphic encryption (HE) is a promising cryptographic technique for\nenabling secure collaborative machine learning in the cloud. However, support\nfor homomorphic computation on ciphertexts under multiple keys is inefficient.\nCurrent solutions often require key setup before any computation or incur large\nciphertext size (at best, grow linearly to the number of involved keys). In\nthis paper, we proposed a new approach that leverages threshold and multi-key\nHE to support computations on ciphertexts under different keys. Our new\napproach removes the need for key setup between each client and the set of\nmodel owners. At the same time, this approach reduces the number of encrypted\nmodels to be offloaded to the cloud evaluator, and the ciphertext size with a\ndimension reduction from (N+1)x2 to 2x2. We present the details of each step\nand discuss the complexity and security of our approach.",
    "In this paper we propose a novel methodology to assist in identifying\nvulnerabilities in a real-world complex heterogeneous industrial control\nsystems (ICS) using two evolutionary multiobjective optimisation (EMO)\nalgorithms, NSGA-II and SPEA2. Our approach is evaluated on a well known\nbenchmark chemical plant simulator, the Tennessee Eastman (TE) process model.\nWe identified vulnerabilities in individual components of the TE model and then\nmade use of these to generate combinatorial attacks to damage the safety of the\nsystem, and to cause economic loss. Results were compared against random\nattacks, and the performance of the EMO algorithms were evaluated using\nhypervolume, spread and inverted generational distance (IGD) metrics. A defence\nagainst these attacks in the form of a novel intrusion detection system was\ndeveloped, using a number of machine learning algorithms. Designed approach was\nfurther tested against the developed detection methods. Results demonstrate\nthat EMO algorithms are a promising tool in the identification of the most\nvulnerable components of ICS, and weaknesses of any existing detection systems\nin place to protect the system. The proposed approach can be used by control\nand security engineers to design security aware control, and test the\neffectiveness of security mechanisms, both during design, and later during\nsystem operation.",
    "Since the number of elderly and patients who are in hospitals and healthcare\ncenters are growing, providing efficient remote healthcare services seems very\nimportant. Currently, most such systems benefit from the distribution and\nautonomy features of multiagent systems and the structure of wireless sensor\nnetworks. On the one hand, securing the data of remote healthcare systems is\none of the most significant concerns; particularly recent types of research\nabout the security of remote healthcare systems keep them secure from\neavesdropping and data modification. On the other hand, existing remote\nhealthcare systems are still vulnerable against other common attacks of\nhealthcare networks such as Denial of Service (DoS) and User to Root (U2R)\nattacks, because they are managed remotely and based on the Internet.\nTherefore, in this paper, we propose a secure framework for remote healthcare\nsystems that consists of two phases. First, we design a healthcare system base\non multiagent technology to collect data from a sensor network. Then, in the\nsecond phase, a layered architecture of intrusion detection systems that uses\nSupport Vector Machine to learn the behavior of network traffic is applied.\nBased on our framework, we implement a secure remote healthcare system and\nevaluate this system against the frequent attacks of healthcare networks such\nas Smurf, Buffer overflow, Neptune, and Pod attacks. In the end, evaluation\nparameters of the layered architecture of intrusion detection systems prove the\nefficiency and correctness of our proposed framework.",
    "In the recent years, Portable Document Format, commonly known as PDF, has\nbecome a democratized standard for document exchange and dissemination. This\ntrend has been due to its characteristics such as its flexibility and\nportability across platforms. The widespread use of PDF has installed a false\nimpression of inherent safety among benign users. However, the characteristics\nof PDF motivated hackers to exploit various types of vulnerabilities, overcome\nsecurity safeguards, thereby making the PDF format one of the most efficient\nmalicious code attack vectors. Therefore, efficiently detecting malicious PDF\nfiles is crucial for information security. Several analysis techniques has been\nproposed in the literature, be it static or dynamic, to extract the main\nfeatures that allow the discrimination of malware files from benign ones. Since\nclassical analysis techniques may be limited in case of zero-days,\nmachine-learning based techniques have emerged recently as an automatic\nPDF-malware detection method that is able to generalize from a set of training\nsamples. These techniques are themselves facing the challenge of evasion\nattacks where a malicious PDF is transformed to look benign. In this work, we\ngive an overview on the PDF-malware detection problem. We give a perspective on\nthe new challenges and emerging solutions.",
    "Security vulnerabilities play a vital role in network security system.\nFuzzing technology is widely used as a vulnerability discovery technology to\nreduce damage in advance. However, traditional fuzzing techniques have many\nchallenges, such as how to mutate input seed files, how to increase code\ncoverage, and how to effectively bypass verification. Machine learning\ntechnology has been introduced as a new method into fuzzing test to alleviate\nthese challenges. This paper reviews the research progress of using machine\nlearning technology for fuzzing test in recent years, analyzes how machine\nlearning improve the fuzz process and results, and sheds light on future work\nin fuzzing. Firstly, this paper discusses the reasons why machine learning\ntechniques can be used for fuzzing scenarios and identifies six different\nstages in which machine learning have been used. Then this paper systematically\nstudy the machine learning based fuzzing models from selection of machine\nlearning algorithm, pre-processing methods, datasets, evaluation metrics, and\nhyperparameters setting. Next, this paper assesses the performance of the\nmachine learning models based on the frequently used evaluation metrics. The\nresults of the evaluation prove that machine learning technology has an\nacceptable capability of categorize predictive for fuzzing. Finally, the\ncomparison on capability of discovering vulnerabilities between traditional\nfuzzing tools and machine learning based fuzzing tools is analyzed. The results\ndepict that the introduction of machine learning technology can improve the\nperformance of fuzzing. However, there are still some limitations, such as\nunbalanced training samples and difficult to extract the characteristics\nrelated to vulnerabilities.",
    "Recently, with the continuous development of deep learning, the performance\nof named entity recognition tasks has been dramatically improved. However, the\nprivacy and the confidentiality of data in some specific fields, such as\nbiomedical and military, cause insufficient data to support the training of\ndeep neural networks. In this paper, we propose an encryption learning\nframework to address the problems of data leakage and inconvenient disclosure\nof sensitive data in certain domains. We introduce multiple encryption\nalgorithms to encrypt training data in the named entity recognition task for\nthe first time. In other words, we train the deep neural network using the\nencrypted data. We conduct experiments on six Chinese datasets, three of which\nare constructed by ourselves. The experimental results show that the encryption\nmethod achieves satisfactory results. The performance of some models trained\nwith encrypted data even exceeds the performance of the unencrypted method,\nwhich verifies the effectiveness of the introduced encryption method and solves\nthe problem of data leakage to a certain extent.",
    "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users.",
    "This work assesses the impact of blockchain and smart contract on the\nvisibility of construction supply chain and in the context of payments\n(intersection of cash and product flows). It uses comparative empirical\nexperiments (Charrette Test Method) to draw comparisons between the visibility\nof state-of-practice and blockchain-enabled payment systems in a commercial\nconstruction project. Comparisons were drawn across four levels of granularity.\nThe findings are twofold: 1) blockchain improved information completeness and\ninformation accuracy respectively by an average 216% and 261% compared with the\ndigital state-of-practice solution. The improvements were significantly more\npronounced for inquiries that had higher product, trade, and temporal\ngranularity; 2) blockchain-enabled solution was robust in the face of increased\ngranularity, while the conventional solution experienced 50% and 66.7% decline\nrespectively in completeness and accuracy of information. The paper concludes\nwith a discussion of mechanisms contributing to visibility and technology\nadoption based on business objectives.",
    "In this paper, we present a general multiparty modeling paradigm with Privacy\nPreserving Principal Component Analysis (PPPCA) for horizontally partitioned\ndata. PPPCA can accomplish multiparty cooperative execution of PCA under the\npremise of keeping plaintext data locally. We also propose implementations\nusing two techniques, i.e., homomorphic encryption and secret sharing. The\noutput of PPPCA can be sent directly to data consumer to build any machine\nlearning models. We conduct experiments on three UCI benchmark datasets and a\nreal-world fraud detection dataset. Results show that the accuracy of the model\nbuilt upon PPPCA is the same as the model with PCA that is built based on\ncentralized plaintext data.",
    "Top-k predictions are used in many real-world applications such as machine\nlearning as a service, recommender systems, and web searches. $\\ell_0$-norm\nadversarial perturbation characterizes an attack that arbitrarily modifies some\nfeatures of an input such that a classifier makes an incorrect prediction for\nthe perturbed input. $\\ell_0$-norm adversarial perturbation is easy to\ninterpret and can be implemented in the physical world. Therefore, certifying\nrobustness of top-$k$ predictions against $\\ell_0$-norm adversarial\nperturbation is important. However, existing studies either focused on\ncertifying $\\ell_0$-norm robustness of top-$1$ predictions or $\\ell_2$-norm\nrobustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our\napproach is based on randomized smoothing, which builds a provably robust\nclassifier from an arbitrary classifier via randomizing an input. Our major\ntheoretical contribution is an almost tight $\\ell_0$-norm certified robustness\nguarantee for top-$k$ predictions. We empirically evaluate our method on\nCIFAR10 and ImageNet. For instance, our method can build a classifier that\nachieves a certified top-3 accuracy of 69.2\\% on ImageNet when an attacker can\narbitrarily perturb 5 pixels of a testing image.",
    "The consumer Internet of Things (IoT) have developed in recent years. Mass\nIoT devices are constructed to build a huge communications network. But these\ndevices are insecure in reality, it means that the communications network are\nexposed by the attacker. Moreover, the IoT communication network also faces\nwith variety of sudden errors. Therefore, it easily leads to that is vulnerable\nwith the threat of attacker and system failure. The severe situation of IoT\ncommunication network motivates the development of new techniques to\nautomatically detect multi-anomaly. In this paper, we propose SS-VTCN, a\nsemi-supervised network for IoT multiple anomaly detection that works well\neffectively for IoT communication network. SS-VTCN is designed to capture the\nnormal patterns of the IoT traffic data based on the distribution whether it is\nlabeled or not by learning their representations with key techniques such as\nVariational Autoencoders and Temporal Convolutional Network. This network can\nuse the encode data to predict preliminary result, and reconstruct input data\nto determine anomalies by the representations. Extensive evaluation experiments\nbased on a benchmark dataset and a real consumer smart home dataset demonstrate\nthat SS-VTCN is more suitable than supervised and unsupervised method with\nbetter performance when compared other state-of-art semi-supervised method.",
    "The Artificial Intelligence (AI) institute for Intelligent\nCyberinfrastructure with Computational Learning in the Environment (ICICLE) is\nfunded by the NSF to build the next generation of Cyberinfrastructure to render\nAI more accessible to everyone and drive its further democratization in the\nlarger society. We describe our efforts to develop Jupyter Notebooks and Python\ncommand line clients that would access these ICICLE resources and services\nusing ICICLE authentication mechanisms. To connect our clients, we used Tapis,\nwhich is a framework that supports computational research to enable scientists\nto access, utilize, and manage multi-institution resources and services. We\nused Neo4j to organize data into a knowledge graph (KG). We then hosted the KG\non a Tapis Pod, which offers persistent data storage with a template made\nspecifically for Neo4j KGs. In order to demonstrate the capabilities of our\nsoftware, we developed several clients: Jupyter notebooks authentication,\nNeural Networks (NN) notebook, and command line applications that provide a\nconvenient frontend to the Tapis API. In addition, we developed a data\nprocessing notebook that can manipulate KGs on the Tapis servers, including\ncreations of a KG, data upload and modification. In this report we present the\nsoftware architecture, design and approach, the successfulness of our client\nsoftware, and future work.",
    "Intrusion detection into computer networks has become one of the most\nimportant issues in cybersecurity. Attackers keep on researching and coding to\ndiscover new vulnerabilities to penetrate information security system. In\nconsequence computer systems must be daily upgraded using up-to-date techniques\nto keep hackers at bay. This paper focuses on the design and implementation of\nan intrusion detection system based on Deep Learning architectures. As a first\nstep, a shallow network is trained with labelled log-in [into a computer\nnetwork] data taken from the Dataset CICIDS2017. The internal behaviour of this\nnetwork is carefully tracked and tuned by using plotting and exploring codes\nuntil it reaches a functional peak in intrusion prediction accuracy. As a\nsecond step, an autoencoder, trained with big unlabelled data, is used as a\nmiddle processor which feeds compressed information and abstract representation\nto the original shallow network. It is proven that the resultant deep\narchitecture has a better performance than any version of the shallow network\nalone. The resultant functional code scripts, written in MATLAB, represent a\nre-trainable system which has been proved using real data, producing good\nprecision and fast response.",
    "Autonomous vehicle navigation and healthcare diagnostics are among the many\nfields where the reliability and security of machine learning models for image\ndata are critical. We conduct a comprehensive investigation into the\nsusceptibility of Convolutional Neural Networks (CNNs), which are widely used\nfor image data, to white-box adversarial attacks. We investigate the effects of\nvarious sophisticated attacks -- Fast Gradient Sign Method, Basic Iterative\nMethod, Jacobian-based Saliency Map Attack, Carlini & Wagner, Projected\nGradient Descent, and DeepFool -- on CNN performance metrics, (e.g., loss,\naccuracy), the differential efficacy of adversarial techniques in increasing\nerror rates, the relationship between perceived image quality metrics (e.g.,\nERGAS, PSNR, SSIM, and SAM) and classification performance, and the comparative\neffectiveness of iterative versus single-step attacks. Using the MNIST,\nCIFAR-10, CIFAR-100, and Fashio_MNIST datasets, we explore the effect of\ndifferent attacks on the CNNs performance metrics by varying the\nhyperparameters of CNNs. Our study provides insights into the robustness of\nCNNs against adversarial threats, pinpoints vulnerabilities, and underscores\nthe urgent need for developing robust defense mechanisms to protect CNNs and\nensuring their trustworthy deployment in real-world scenarios.",
    "In Autonomous Vehicles (AVs), one fundamental pillar is perception, which\nleverages sensors like cameras and LiDARs (Light Detection and Ranging) to\nunderstand the driving environment. Due to its direct impact on road safety,\nmultiple prior efforts have been made to study its the security of perception\nsystems. In contrast to prior work that concentrates on camera-based\nperception, in this work we perform the first security study of LiDAR-based\nperception in AV settings, which is highly important but unexplored. We\nconsider LiDAR spoofing attacks as the threat model and set the attack goal as\nspoofing obstacles close to the front of a victim AV. We find that blindly\napplying LiDAR spoofing is insufficient to achieve this goal due to the machine\nlearning-based object detection process. Thus, we then explore the possibility\nof strategically controlling the spoofed attack to fool the machine learning\nmodel. We formulate this task as an optimization problem and design modeling\nmethods for the input perturbation function and the objective function. We also\nidentify the inherent limitations of directly solving the problem using\noptimization and design an algorithm that combines optimization and global\nsampling, which improves the attack success rates to around 75%. As a case\nstudy to understand the attack impact at the AV driving decision level, we\nconstruct and evaluate two attack scenarios that may damage road safety and\nmobility. We also discuss defense directions at the AV system, sensor, and\nmachine learning model levels.",
    "Artificial Intelligence Generated Content (AIGC) is one of the latest\nachievements in AI development. The content generated by related applications,\nsuch as text, images and audio, has sparked a heated discussion. Various\nderived AIGC applications are also gradually entering all walks of life,\nbringing unimaginable impact to people's daily lives. However, the rapid\ndevelopment of such generative tools has also raised concerns about privacy and\nsecurity issues, and even copyright issues in AIGC. We note that advanced\ntechnologies such as blockchain and privacy computing can be combined with AIGC\ntools, but no work has yet been done to investigate their relevance and\nprospect in a systematic and detailed way. Therefore it is necessary to\ninvestigate how they can be used to protect the privacy and security of data in\nAIGC by fully exploring the aforementioned technologies. In this paper, we\nfirst systematically review the concept, classification and underlying\ntechnologies of AIGC. Then, we discuss the privacy and security challenges\nfaced by AIGC from multiple perspectives and purposefully list the\ncountermeasures that currently exist. We hope our survey will help researchers\nand industry to build a more secure and robust AIGC system.",
    "Differential privacy has gained popularity in machine learning as a strong\nprivacy guarantee, in contrast to privacy mitigation techniques such as\nk-anonymity. However, applying differential privacy to n-gram counts\nsignificantly degrades the utility of derived language models due to their\nlarge vocabularies. We propose a differential privacy mechanism that uses\npublic data as a prior in a Bayesian setup to provide tighter bounds on the\nprivacy loss metric epsilon, and thus better privacy-utility trade-offs. It\nfirst transforms the counts to log space, approximating the distribution of the\npublic and private data as Gaussian. The posterior distribution is then\nevaluated and softmax is applied to produce a probability distribution. This\ntechnique achieves up to 85% reduction in KL divergence compared to previously\nknown mechanisms at epsilon equals 0.1. We compare our mechanism to k-anonymity\nin a n-gram language modelling task and show that it offers competitive\nperformance at large vocabulary sizes, while also providing superior privacy\nprotection.",
    "Botnet detectors based on machine learning are potential targets for\nadversarial evasion attacks. Several research works employ adversarial training\nwith samples generated from generative adversarial nets (GANs) to make the\nbotnet detectors adept at recognising adversarial evasions. However, the\nsynthetic evasions may not follow the original semantics of the input samples.\nThis paper proposes a novel GAN model leveraged with deep reinforcement\nlearning (DRL) to explore semantic aware samples and simultaneously harden its\ndetection. A DRL agent is used to attack the discriminator of the GAN that acts\nas a botnet detector. The discriminator is trained on the crafted perturbations\nby the agent during the GAN training, which helps the GAN generator converge\nearlier than the case without DRL. We name this model RELEVAGAN, i.e. [\"relive\na GAN\" or deep REinforcement Learning-based Evasion Generative Adversarial\nNetwork] because, with the help of DRL, it minimises the GAN's job by letting\nits generator explore the evasion samples within the semantic limits. During\nthe GAN training, the attacks are conducted to adjust the discriminator weights\nfor learning crafted perturbations by the agent. RELEVAGAN does not require\nadversarial training for the ML classifiers since it can act as an adversarial\nsemantic-aware botnet detection model. Code will be available at\nhttps://github.com/rhr407/RELEVAGAN.",
    "As Android has become increasingly popular, so has malware targeting it, thus\npushing the research community to propose different detection techniques.\nHowever, the constant evolution of the Android ecosystem, and of malware\nitself, makes it hard to design robust tools that can operate for long periods\nof time without the need for modifications or costly re-training. Aiming to\naddress this issue, we set to detect malware from a behavioral point of view,\nmodeled as the sequence of abstracted API calls. We introduce MaMaDroid, a\nstatic-analysis based system that abstracts the API calls performed by an app\nto their class, package, or family, and builds a model from their sequences\nobtained from the call graph of an app as Markov chains. This ensures that the\nmodel is more resilient to API changes and the features set is of manageable\nsize. We evaluate MaMaDroid using a dataset of 8.5K benign and 35.5K malicious\napps collected over a period of six years, showing that it effectively detects\nmalware (with up to 0.99 F-measure) and keeps its detection capabilities for\nlong periods of time (up to 0.87 F-measure two years after training). We also\nshow that MaMaDroid remarkably outperforms DroidAPIMiner, a state-of-the-art\ndetection system that relies on the frequency of (raw) API calls. Aiming to\nassess whether MaMaDroid's effectiveness mainly stems from the API abstraction\nor from the sequencing modeling, we also evaluate a variant of it that uses\nfrequency (instead of sequences), of abstracted API calls. We find that it is\nnot as accurate, failing to capture maliciousness when trained on malware\nsamples that include API calls that are equally or more frequently used by\nbenign apps.",
    "Maximum order complexity is an important tool for measuring the nonlinearity\nof a pseudorandom sequence. There is a lack of tools for predicting the\nstrength of a pseudorandom binary sequence in an effective and efficient\nmanner. To this end, this paper proposes a neural-network-based model for\nmeasuring the strength of a pseudorandom binary sequence. Using the Shrinking\nGenerator (SG) keystream as pseudorandom binary sequences, then calculating the\nUnique Window Size (UWS) as a representation of Maximum order complexity, we\ndemonstrate that the proposed model provides more accurate and efficient\npredictions (measurements) than a classical method for predicting the maximum\norder complexity.",
    "Increased dependence on networked, software based control has escalated the\nvulnerabilities of Cyber Physical Systems (CPSs). Detection and monitoring\ncomponents developed leveraging dynamical systems theory are often employed as\nlightweight security measures for protecting such safety critical CPSs against\nfalse data injection attacks. However, existing approaches do not correlate\nattack scenarios with parameters of detection systems. In the present work, we\npropose a Reinforcement Learning (RL) based framework which adaptively sets the\nparameters of such detectors based on experience learned from attack scenarios,\nmaximizing detection rate and minimizing false alarms in the process while\nattempting performance preserving control actions.",
    "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
    "Not too long ago, AI security used to mean the research and practice of how\nAI can empower cybersecurity, that is, AI for security. Ever since Ian\nGoodfellow and his team popularized adversarial attacks on machine learning,\nsecurity for AI became an important concern and also part of AI security. It is\nimperative to understand the threats to machine learning products and avoid\ncommon pitfalls in AI product development. This article is addressed to\ndevelopers, designers, managers and researchers of AI software products.",
    "The binary similarity problem consists in determining if two functions are\nsimilar by only considering their compiled form. Advanced techniques for binary\nsimilarity recently gained momentum as they can be applied in several fields,\nsuch as copyright disputes, malware analysis, vulnerability detection, etc.,\nand thus have an immediate practical impact. Current solutions compare\nfunctions by first transforming their binary code in multi-dimensional vector\nrepresentations (embeddings), and then comparing vectors through simple and\nefficient geometric operations. However, embeddings are usually derived from\nbinary code using manual feature extraction, that may fail in considering\nimportant function characteristics, or may consider features that are not\nimportant for the binary similarity problem. In this paper we propose SAFE, a\nnovel architecture for the embedding of functions based on a self-attentive\nneural network. SAFE works directly on disassembled binary functions, does not\nrequire manual feature extraction, is computationally more efficient than\nexisting solutions (i.e., it does not incur in the computational overhead of\nbuilding or manipulating control flow graphs), and is more general as it works\non stripped binaries and on multiple architectures. We report the results from\na quantitative and qualitative analysis that show how SAFE provides a\nnoticeable performance improvement with respect to previous solutions.\nFurthermore, we show how clusters of our embedding vectors are closely related\nto the semantic of the implemented algorithms, paving the way for further\ninteresting applications (e.g. semantic-based binary function search).",
    "We investigate the problem of nodes clustering under privacy constraints when\nrepresenting a dataset as a graph. Our contribution is threefold. First we\nformally define the concept of differential privacy for structured databases\nsuch as graphs, and give an alternative definition based on a new neighborhood\nnotion between graphs. This definition is adapted to particular frameworks that\ncan be met in various application fields such as genomics, world wide web,\npopulation survey, etc. Second, we introduce a new algorithm to tackle the\nissue of privately releasing an approximated minimum spanning tree topology for\na simple-undirected-weighted graph. It provides a simple way of producing the\ntopology of a private almost minimum spanning tree which outperforms, in most\ncases, the state of the art \"Laplace mechanism\" in terms of\nweight-approximation error.\n  Finally, we propose a theoretically motivated method combining a sanitizing\nmechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree\n(MST)-based clustering algorithm. It provides an accurate method for nodes\nclustering in a graph while keeping the sensitive information contained in the\nedges weights of the private graph. We provide some theoretical results on the\nrobustness of an almost minimum spanning tree construction for Laplace\nsanitizing mechanisms. These results exhibit which conditions the graph weights\nshould respect in order to consider that the nodes form well separated clusters\nboth for Laplace and our algorithm as sanitizing mechanism. The method has been\nexperimentally evaluated on simulated data, and preliminary results show the\ngood behavior of the algorithm while identifying well separated clusters.",
    "Homomorphic Encryption (HE), allowing computations on encrypted data\n(ciphertext) without decrypting it first, enables secure but prohibitively slow\nConvolutional Neural Network (CNN) inference for privacy-preserving\napplications in clouds. To reduce the inference latency, one approach is to\npack multiple messages into a single ciphertext in order to reduce the number\nof ciphertexts and support massive parallelism of Homomorphic\nMultiply-Accumulate (HMA) operations between ciphertexts. Despite the faster\nHECNN inference, the mainstream packing schemes Dense Packing (DensePack) and\nConvolution Packing (ConvPack) introduce expensive rotation overhead, which\nprolongs the inference latency of HECNN for deeper and wider CNN architectures.\nIn this paper, we propose a low-rank factorization method named FFConv\ndedicated to efficient ciphertext packing for reducing both the rotation\noverhead and HMA operations. FFConv approximates a d x d convolution layer with\nlow-rank factorized convolutions, in which a d x d low-rank convolution with\nfewer channels is followed by a 1 x 1 convolution to restore the channels. The\nd x d low-rank convolution with DensePack leads to significantly reduced\nrotation operations, while the rotation overhead of 1 x 1 convolution with\nConvPack is close to zero. To our knowledge, FFConv is the first work that is\ncapable of reducing the rotation overhead incurred by DensePack and ConvPack\nsimultaneously, without introducing additional special blocks into the HECNN\ninference pipeline. Compared to prior art LoLa and Falcon, our method reduces\nthe inference latency by up to 88% and 21%, respectively, with comparable\naccuracy on MNIST and CIFAR-10.",
    "Bitcoin has created a new exchange paradigm within which financial\ntransactions can be trusted without an intermediary. This premise of a free\ndecentralized transactional network however requires, in its current\nimplementation, unrestricted access to the ledger for peer-based transaction\nverification. A number of studies have shown that, in this pseudonymous\ncontext, identities can be leaked based on transaction features or off-network\ninformation. In this work, we analyze the information revealed by the pattern\nof transactions in the neighborhood of a given entity transaction. By\ndefinition, these features which pertain to an extended network are not\ndirectly controllable by the entity, but might enable leakage of information\nabout transacting entities. We define a number of new features relevant to\nentity characterization on the Bitcoin Blockchain and study their efficacy in\npractice. We show that even a weak attacker with shallow data mining knowledge\nis able to leverage these features to characterize the entity properties.",
    "We present remote Operating System detection as an inference problem: given a\nset of observations (the target host responses to a set of tests), we want to\ninfer the OS type which most probably generated these observations. Classical\ntechniques used to perform this analysis present several limitations. To\nimprove the analysis, we have developed tools using neural networks and\nStatistics tools. We present two working modules: one which uses DCE-RPC\nendpoints to distinguish Windows versions, and another which uses Nmap\nsignatures to distinguish different version of Windows, Linux, Solaris,\nOpenBSD, FreeBSD and NetBSD systems. We explain the details of the topology and\ninner workings of the neural networks used, and the fine tuning of their\nparameters. Finally we show positive experimental results.",
    "Neural text detectors aim to decide the characteristics that distinguish\nneural (machine-generated) from human texts. To challenge such detectors,\nadversarial attacks can alter the statistical characteristics of the generated\ntext, making the detection task more and more difficult. Inspired by the\nadvances of mutation analysis in software development and testing, in this\npaper, we propose character- and word-based mutation operators for generating\nadversarial samples to attack state-of-the-art natural text detectors. This\nfalls under white-box adversarial attacks. In such attacks, attackers have\naccess to the original text and create mutation instances based on this\noriginal text. The ultimate goal is to confuse machine learning models and\nclassifiers and decrease their prediction accuracy.",
    "Backdoor attack is a severe security threat to deep neural networks (DNNs).\nWe envision that, like adversarial examples, there will be a cat-and-mouse game\nfor backdoor attacks, i.e., new empirical defenses are developed to defend\nagainst backdoor attacks but they are soon broken by strong adaptive backdoor\nattacks. To prevent such cat-and-mouse game, we take the first step towards\ncertified defenses against backdoor attacks. Specifically, in this work, we\nstudy the feasibility and effectiveness of certifying robustness against\nbackdoor attacks using a recent technique called randomized smoothing.\nRandomized smoothing was originally developed to certify robustness against\nadversarial examples. We generalize randomized smoothing to defend against\nbackdoor attacks. Our results show the theoretical feasibility of using\nrandomized smoothing to certify robustness against backdoor attacks. However,\nwe also find that existing randomized smoothing methods have limited\neffectiveness at defending against backdoor attacks, which highlight the needs\nof new theory and methods to certify robustness against backdoor attacks.",
    "In the fields of statistics and unsupervised machine learning a fundamental\nand well-studied problem is anomaly detection. Anomalies are difficult to\ndefine, yet many algorithms have been proposed. Underlying the approaches is\nthe nebulous understanding that anomalies are rare, unusual or inconsistent\nwith the majority of data. The present work provides a philosophical treatise\nto clearly define anomalies and develops an algorithm for their efficient\ndetection with minimal user intervention. Inspired by the Gestalt School of\nPsychology and the Helmholtz principle of human perception, anomalies are\nassumed to be observations that are unexpected to occur with respect to certain\ngroupings made by the majority of the data. Under appropriate random variable\nmodelling anomalies are directly found in a set of data by a uniform and\nindependent random assumption of the distribution of constituent elements of\nthe observations, with anomalies corresponding to those observations where the\nexpectation of the number of occurrences of the elements in a given view is\n$<1$. Starting from fundamental principles of human perception an unsupervised\nanomaly detection algorithm is developed that is simple, real-time and\nparameter-free. Experiments suggest it as a competing choice for univariate\ndata with promising results on the detection of global anomalies in\nmultivariate data.",
    "In the burgeoning field of Large Language Models (LLMs), developing a robust\nsafety mechanism, colloquially known as \"safeguards\" or \"guardrails\", has\nbecome imperative to ensure the ethical use of LLMs within prescribed\nboundaries. This article provides a systematic literature review on the current\nstatus of this critical mechanism. It discusses its major challenges and how it\ncan be enhanced into a comprehensive mechanism dealing with ethical issues in\nvarious contexts. First, the paper elucidates the current landscape of\nsafeguarding mechanisms that major LLM service providers and the open-source\ncommunity employ. This is followed by the techniques to evaluate, analyze, and\nenhance some (un)desirable properties that a guardrail might want to enforce,\nsuch as hallucinations, fairness, privacy, and so on. Based on them, we review\ntechniques to circumvent these controls (i.e., attacks), to defend the attacks,\nand to reinforce the guardrails. While the techniques mentioned above represent\nthe current status and the active research trends, we also discuss several\nchallenges that cannot be easily dealt with by the methods and present our\nvision on how to implement a comprehensive guardrail through the full\nconsideration of multi-disciplinary approach, neural-symbolic method, and\nsystems development lifecycle.",
    "Membership inference attacks seek to infer the membership of individual\ntraining instances of a privately trained model. This paper presents a\nmembership privacy analysis and evaluation system, called MPLens, with three\nunique contributions. First, through MPLens, we demonstrate how membership\ninference attack methods can be leveraged in adversarial machine learning.\nSecond, through MPLens, we highlight how the vulnerability of pre-trained\nmodels under membership inference attack is not uniform across all classes,\nparticularly when the training data itself is skewed. We show that risk from\nmembership inference attacks is routinely increased when models use skewed\ntraining data. Finally, we investigate the effectiveness of differential\nprivacy as a mitigation technique against membership inference attacks. We\ndiscuss the trade-offs of implementing such a mitigation strategy with respect\nto the model complexity, the learning task complexity, the dataset complexity\nand the privacy parameter settings. Our empirical results reveal that (1)\nminority groups within skewed datasets display increased risk for membership\ninference and (2) differential privacy presents many challenging trade-offs as\na mitigation technique to membership inference risk.",
    "In recent work, Cheu et al. (Eurocrypt 2019) proposed a protocol for\n$n$-party real summation in the shuffle model of differential privacy with\n$O_{\\epsilon, \\delta}(1)$ error and $\\Theta(\\epsilon\\sqrt{n})$ one-bit messages\nper party. In contrast, every local model protocol for real summation must\nincur error $\\Omega(1/\\sqrt{n})$, and there exist protocols matching this lower\nbound which require just one bit of communication per party. Whether this gap\nin number of messages is necessary was left open by Cheu et al.\n  In this note we show a protocol with $O(1/\\epsilon)$ error and\n$O(\\log(n/\\delta))$ messages of size $O(\\log(n))$ per party. This protocol is\nbased on the work of Ishai et al.\\ (FOCS 2006) showing how to implement\ndistributed summation from secure shuffling, and the observation that this\nallows simulating the Laplace mechanism in the shuffle model.",
    "Despite the broad application of Machine Learning models as a Service\n(MLaaS), they are vulnerable to model stealing attacks. These attacks can\nreplicate the model functionality by using the black-box query process without\nany prior knowledge of the target victim model. Existing stealing defenses add\ndeceptive perturbations to the victim's posterior probabilities to mislead the\nattackers. However, these defenses are now suffering problems of high inference\ncomputational overheads and unfavorable trade-offs between benign accuracy and\nstealing robustness, which challenges the feasibility of deployed models in\npractice. To address the problems, this paper proposes Isolation and Induction\n(InI), a novel and effective training framework for model stealing defenses.\nInstead of deploying auxiliary defense modules that introduce redundant\ninference time, InI directly trains a defensive model by isolating the\nadversary's training gradient from the expected gradient, which can effectively\nreduce the inference computational cost. In contrast to adding perturbations\nover model predictions that harm the benign accuracy, we train models to\nproduce uninformative outputs against stealing queries, which can induce the\nadversary to extract little useful knowledge from victim models with minimal\nimpact on the benign performance. Extensive experiments on several visual\nclassification datasets (e.g., MNIST and CIFAR10) demonstrate the superior\nrobustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4x\nfaster) of our InI over other state-of-the-art methods. Our codes can be found\nin https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
    "Modern malware evolves various detection avoidance techniques to bypass the\nstate-of-the-art detection methods. An emerging trend to deal with this issue\nis the combination of image transformation and machine learning techniques to\nclassify and detect malware. However, existing works in this field only perform\nsimple image transformation methods that limit the accuracy of the detection.\nIn this paper, we introduce a novel approach to classify malware by using a\ndeep network on images transformed from binary samples. In particular, we first\ndevelop a novel hybrid image transformation method to convert binaries into\ncolor images that convey the binary semantics. The images are trained by a deep\nconvolutional neural network that later classifies the test inputs into benign\nor malicious categories. Through the extensive experiments, our proposed method\nsurpasses all baselines and achieves 99.14% in terms of accuracy on the testing\nset.",
    "In this paper, we propose a new feature extraction technique for program\nexecution logs. First, we automatically extract complex patterns from a\nprogram's behavior graph. Then, we embed these patterns into a continuous space\nby training an autoencoder. We evaluate the proposed features on a real-world\nmalicious software detection task. We also find that the embedding space\ncaptures interpretable structures in the space of pattern parts.",
    "Advanced persistent threats (APTs) have novel features such as multi-stage\npenetration, highly-tailored intention, and evasive tactics. APTs defense\nrequires fusing multi-dimensional Cyber threat intelligence data to identify\nattack intentions and conducts efficient knowledge discovery strategies by\ndata-driven machine learning to recognize entity relationships. However,\ndata-driven machine learning lacks generalization ability on fresh or unknown\nsamples, reducing the accuracy and practicality of the defense model. Besides,\nthe private deployment of these APT defense models on heterogeneous\nenvironments and various network devices requires significant investment in\ncontext awareness (such as known attack entities, continuous network states,\nand current security strategies). In this paper, we propose a few-shot\nmulti-domain knowledge rearming (FMKR) scheme for context-aware defense against\nAPTs. By completing multiple small tasks that are generated from different\nnetwork domains with meta-learning, the FMKR firstly trains a model with good\ndiscrimination and generalization ability for fresh and unknown APT attacks. In\neach FMKR task, both threat intelligence and local entities are fused into the\nsupport/query sets in meta-learning to identify possible attack stages.\nSecondly, to rearm current security strategies, an finetuning-based deployment\nmechanism is proposed to transfer learned knowledge into the student model,\nwhile minimizing the defense cost. Compared to multiple model replacement\nstrategies, the FMKR provides a faster response to attack behaviors while\nconsuming less scheduling cost. Based on the feedback from multiple real users\nof the Industrial Internet of Things (IIoT) over 2 months, we demonstrate that\nthe proposed scheme can improve the defense satisfaction rate.",
    "Malware analysis has been extensively investigated as the number and types of\nmalware has increased dramatically. However, most previous studies use\nend-to-end systems to detect whether a sample is malicious, or to identify its\nmalware family. In this paper, we propose a neural network framework composed\nof an embedder, an encoder, and a filter to learn malware representations from\ncharacteristic execution sequences for malware family classification. The\nembedder uses BERT and Sent2Vec, state-of-the-art embedding modules, to capture\nrelations within a single API call and among consecutive API calls in an\nexecution trace. The encoder comprises gated recurrent units (GRU) to preserve\nthe ordinal position of API calls and a self-attention mechanism for comparing\nintra-relations among different positions of API calls. The filter identifies\nrepresentative API calls to build the malware representation. We conduct broad\nexperiments to determine the influence of individual framework components. The\nresults show that the proposed framework outperforms the baselines, and also\ndemonstrates that considering Sent2Vec to learn complete API call embeddings\nand GRU to explicitly preserve ordinal information yields more information and\nthus significant improvements. Also, the proposed approach effectively\nclassifies new malicious execution traces on the basis of similarities with\npreviously collected families.",
    "Blockchain offers a decentralized, immutable, transparent system of records.\nIt offers a peer-to-peer network of nodes with no centralised governing entity\nmaking it unhackable and therefore, more secure than the traditional\npaper-based or centralised system of records like banks etc. While there are\ncertain advantages to the paper-based recording approach, it does not work well\nwith digital relationships where the data is in constant flux. Unlike\ntraditional channels, governed by centralized entities, blockchain offers its\nusers a certain level of anonymity by providing capabilities to interact\nwithout disclosing their personal identities and allows them to build trust\nwithout a third-party governing entity. Due to the aforementioned\ncharacteristics of blockchain, more and more users around the globe are\ninclined towards making a digital transaction via blockchain than via\nrudimentary channels. Therefore, there is a dire need for us to gain insight on\nhow these transactions are processed by the blockchain and how much time it may\ntake for a peer to confirm a transaction and add it to the blockchain network.\nThis paper presents a novel approach that would allow one to estimate the time,\nin block time or otherwise, it would take for a mining node to accept and\nconfirm a transaction to a block using machine learning. The paper also aims to\ncompare the predictive accuracy of two machine learning regression models-\nRandom Forest Regressor and Multilayer Perceptron against previously proposed\nstatistical regression model under a set evaluation criterion. The objective is\nto determine whether machine learning offers a more accurate predictive model\nthan conventional statistical models. The proposed model results in improved\naccuracy in prediction.",
    "To this date, CAPTCHAs have served as the first line of defense preventing\nunauthorized access by (malicious) bots to web-based services, while at the\nsame time maintaining a trouble-free experience for human visitors. However,\nrecent work in the literature has provided evidence of sophisticated bots that\nmake use of advancements in machine learning (ML) to easily bypass existing\nCAPTCHA-based defenses. In this work, we take the first step to address this\nproblem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial\nexamples. While typically adversarial examples are used to lead an ML model\nastray, with CAPTURE, we attempt to make a \"good use\" of such mechanisms. Our\nempirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to\nsolve by humans while at the same time, effectively thwarting ML-based bot\nsolvers.",
    "Deep learning models have achieved high performance on many tasks, and thus\nhave been applied to many security-critical scenarios. For example, deep\nlearning-based face recognition systems have been used to authenticate users to\naccess many security-sensitive applications like payment apps. Such usages of\ndeep learning systems provide the adversaries with sufficient incentives to\nperform attacks against these systems for their adversarial purposes. In this\nwork, we consider a new type of attacks, called backdoor attacks, where the\nattacker's goal is to create a backdoor into a learning-based authentication\nsystem, so that he can easily circumvent the system by leveraging the backdoor.\nSpecifically, the adversary aims at creating backdoor instances, so that the\nvictim learning system will be misled to classify the backdoor instances as a\ntarget label specified by the adversary. In particular, we study backdoor\npoisoning attacks, which achieve backdoor attacks using poisoning strategies.\nDifferent from all existing work, our studied poisoning strategies can apply\nunder a very weak threat model: (1) the adversary has no knowledge of the model\nand the training set used by the victim system; (2) the attacker is allowed to\ninject only a small amount of poisoning samples; (3) the backdoor key is hard\nto notice even by human beings to achieve stealthiness. We conduct evaluation\nto demonstrate that a backdoor adversary can inject only around 50 poisoning\nsamples, while achieving an attack success rate of above 90%. We are also the\nfirst work to show that a data poisoning attack can create physically\nimplementable backdoors without touching the training process. Our work\ndemonstrates that backdoor poisoning attacks pose real threats to a learning\nsystem, and thus highlights the importance of further investigation and\nproposing defense strategies against them.",
    "Our computer systems for decades have been threatened by various types of\nhardware and software attacks of which Malwares have been one of them. This\nmalware has the ability to steal, destroy, contaminate, gain unintended access,\nor even disrupt the entire system. There have been techniques to detect malware\nby performing static and dynamic analysis of malware files, but, stealthy\nmalware has circumvented the static analysis method and for dynamic analysis,\nthere have been previous works that propose different methods to detect malware\nbut, in this work we propose a novel technique to detect malware. We use\nmalware binary images and then extract different features from the same and\nthen employ different ML-classifiers on the dataset thus obtained. We show that\nthis technique is successful in differentiating classes of malware based on the\nfeatures extracted.",
    "The continued growth in the deployment of Internet-of-Things (IoT) devices\nhas been fueled by the increased connectivity demand, particularly in\nindustrial environments. However, this has led to an increase in the number of\nnetwork related attacks due to the increased number of potential attack\nsurfaces. Industrial IoT (IIoT) devices are prone to various network related\nattacks that can have severe consequences on the manufacturing process as well\nas on the safety of the workers in the manufacturing plant. One promising\nsolution that has emerged in recent years for attack detection is Machine\nlearning (ML). More specifically, ensemble learning models have shown great\npromise in improving the performance of the underlying ML models. Accordingly,\nthis paper proposes a framework based on the combined use of Bayesian\nOptimization-Gaussian Process (BO-GP) with an ensemble tree-based learning\nmodel to improve the performance of intrusion and attack detection in IIoT\nenvironments. The proposed framework's performance is evaluated using the\nWindows 10 dataset collected by the Cyber Range and IoT labs at University of\nNew South Wales. Experimental results illustrate the improvement in detection\naccuracy, precision, and F-score when compared to standard tree and ensemble\ntree models.",
    "Ethereum smart contracts are automated decentralized applications on the\nblockchain that describe the terms of the agreement between buyers and sellers,\nreducing the need for trusted intermediaries and arbitration. However, the\ndeployment of smart contracts introduces new attack vectors into the\ncryptocurrency systems. In particular, programming flaws in smart contracts can\nbe and have already been exploited to gain enormous financial profits. It is\nthus an emerging yet crucial issue to detect vulnerabilities of different\nclasses in contracts in an efficient manner. Existing machine learning-based\nvulnerability detection methods are limited and only inspect whether the smart\ncontract is vulnerable, or train individual classifiers for each specific\nvulnerability, or demonstrate multi-class vulnerability detection without\nextensibility consideration. To overcome the scalability and generalization\nlimitations of existing works, we propose ESCORT, the first Deep Neural Network\n(DNN)-based vulnerability detection framework for Ethereum smart contracts that\nsupport lightweight transfer learning on unseen security vulnerabilities, thus\nis extensible and generalizable. ESCORT leverages a multi-output NN\narchitecture that consists of two parts: (i) A common feature extractor that\nlearns the semantics of the input contract; (ii) Multiple branch structures\nwhere each branch learns a specific vulnerability type based on features\nobtained from the feature extractor. Experimental results show that ESCORT\nachieves an average F1-score of 95% on six vulnerability types and the\ndetection time is 0.02 seconds per contract. When extended to new vulnerability\ntypes, ESCORT yields an average F1-score of 93%. To the best of our knowledge,\nESCORT is the first framework that enables transfer learning on new\nvulnerability types with minimal modification of the DNN model architecture and\nre-training overhead.",
    "Clustering is an essential technique for network analysis, with applications\nin a diverse range of fields. Although spectral clustering is a popular and\neffective method, it fails to consider higher-order structure and can perform\npoorly on directed networks. One approach is to capture and cluster\nhigher-order structures using motif adjacency matrices. However, current\nformulations fail to take edge weights into account, and thus are somewhat\nlimited when weight is a key component of the network under study.\n  We address these shortcomings by exploring motif-based weighted spectral\nclustering methods. We present new and computationally useful matrix formulae\nfor motif adjacency matrices on weighted networks, which can be used to\nconstruct efficient algorithms for any anchored or non-anchored motif on three\nnodes. In a very sparse regime, our proposed method can handle graphs with a\nmillion nodes and tens of millions of edges. We further use our framework to\nconstruct a motif-based approach for clustering bipartite networks.\n  We provide comprehensive experimental results, demonstrating (i) the\nscalability of our approach, (ii) advantages of higher-order clustering on\nsynthetic examples, and (iii) the effectiveness of our techniques on a variety\nof real world data sets; and compare against several techniques from the\nliterature. We conclude that motif-based spectral clustering is a valuable tool\nfor analysis of directed and bipartite weighted networks, which is also\nscalable and easy to implement.",
    "One of the longstanding open problems in spectral graph clustering (SGC) is\nthe so-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. We propose\nautomated model order selection (AMOS), a solution to the SGC model selection\nproblem under a random interconnection model (RIM) using a novel selection\ncriterion that is based on an asymptotic phase transition analysis. AMOS can\nmore generally be applied to discovering hidden block diagonal structure in\nsymmetric non-negative matrices. Numerical experiments on simulated graphs\nvalidate the phase transition analysis, and real-world network data is used to\nvalidate the performance of the proposed model selection procedure.",
    "The ubiquitous proliferation of online social networks has led to the\nwidescale emergence of relational graphs expressing unique patterns in link\nformation and descriptive user node features. Matrix Factorization and\nCompletion have become popular methods for Link Prediction due to the low rank\nnature of mutual node friendship information, and the availability of parallel\ncomputer architectures for rapid matrix processing. Current Link Prediction\nliterature has demonstrated vast performance improvement through the\nutilization of sparsity in addition to the low rank matrix assumption. However,\nthe majority of research has introduced sparsity through the limited L1 or\nFrobenius norms, instead of considering the more detailed distributions which\nled to the graph formation and relationship evolution. In particular, social\nnetworks have been found to express either Pareto, or more recently discovered,\nLog Normal distributions. Employing the convexity-inducing Lovasz Extension, we\ndemonstrate how incorporating specific degree distribution information can lead\nto large scale improvements in Matrix Completion based Link prediction. We\nintroduce Log-Normal Matrix Completion (LNMC), and solve the complex\noptimization problem by employing Alternating Direction Method of Multipliers.\nUsing data from three popular social networks, our experiments yield up to 5%\nAUC increase over top-performing non-structured sparsity based methods.",
    "In early 2020, the Corona Virus Disease 2019 (COVID-19) pandemic swept the\nworld.In China, COVID-19 has caused severe consequences. Moreover, online\nrumors during the COVID-19 pandemic increased people's panic about public\nhealth and social stability. At present, understanding and curbing the spread\nof online rumors is an urgent task. Therefore, we analyzed the rumor spreading\nmechanism and propose a method to quantify a rumors' influence by the speed of\nnew insiders. The search frequency of the rumor is used as an observation\nvariable of new insiders. The peak coefficient and the attenuation coefficient\nare calculated for the search frequency, which conforms to the exponential\ndistribution. We designed several rumor features and used the above two\ncoefficients as predictable labels. A 5-fold cross-validation experiment using\nthe mean square error (MSE) as the loss function showed that the decision tree\nwas suitable for predicting the peak coefficient, and the linear regression\nmodel was ideal for predicting the attenuation coefficient. Our feature\nanalysis showed that precursor features were the most important for the\noutbreak coefficient, while location information and rumor entity information\nwere the most important for the attenuation coefficient. Meanwhile, features\nthat were conducive to the outbreak were usually harmful to the continued\nspread of rumors. At the same time, anxiety was a crucial rumor causing factor.\nFinally, we discuss how to use deep learning technology to reduce the forecast\nloss by using the Bidirectional Encoder Representations from Transformers\n(BERT) model.",
    "With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it\nhas becoming inherently important to disseminate accurate and timely\ninformation about the disease. Due to the ubiquity of Internet connectivity and\nsmart devices, social sensing is emerging as a dynamic AI-driven sensing\nparadigm to extract real-time observations from online users. In this paper, we\npropose CovidSens, a vision of social sensing based risk alert systems to\nspontaneously obtain and analyze social data to infer COVID-19 propagation.\nCovidSens can actively help to keep the general public informed about the\nCOVID-19 spread and identify risk-prone areas. The CovidSens concept is\nmotivated by three observations: 1) people actively share their experience of\nCOVID-19 via online social media, 2) official warning channels and news\nagencies are relatively slower than people reporting on social media, and 3)\nonline users are frequently equipped with powerful mobile devices that can\nperform data processing and analytics. We envision unprecedented opportunities\nto leverage posts generated by ordinary people to build real-time sensing and\nanalytic system for gathering and circulating COVID-19 propagation data.\nSpecifically, the vision of CovidSens attempts to answer the questions: How to\ndistill reliable information on COVID-19 with prevailing rumors and\nmisinformation? How to inform the general public about the state of the spread\ntimely and effectively? How to leverage the computational power on edge devices\nto construct fully integrated edge-based social sensing platforms? In this\nvision paper, we discuss the roles of CovidSens and identify potential\nchallenges in developing reliable social sensing based risk alert systems. We\nenvision that approaches originating from multiple disciplines can be effective\nin addressing the challenges. Finally, we outline a few research directions for\nfuture work in CovidSens.",
    "A lack of information exists about the health issues of lesbian, gay,\nbisexual, transgender, and queer (LGBTQ) people who are often excluded from\nnational demographic assessments, health studies, and clinical trials. As a\nresult, medical experts and researchers lack a holistic understanding of the\nhealth disparities facing these populations. Fortunately, publicly available\nsocial media data such as Twitter data can be utilized to support the decisions\nof public health policy makers and managers with respect to LGBTQ people. This\nresearch employs a computational approach to collect tweets from gay users on\nhealth-related topics and model these topics. To determine the nature of\nhealth-related information shared by men who have sex with men on Twitter, we\ncollected thousands of tweets from 177 active users. We sampled these tweets\nusing a framework that can be applied to other LGBTQ sub-populations in future\nresearch. We found 11 diseases in 7 categories based on ICD 10 that are in line\nwith the published studies and official reports.",
    "Recent advances in specialized hardware for solving optimization problems\nsuch quantum computers, quantum annealers, and CMOS annealers give rise to new\nways for solving real-word complex problems. However, given current and\nnear-term hardware limitations, the number of variables required to express a\nlarge real-world problem easily exceeds the hardware capabilities, thus hybrid\nmethods are usually developed in order to utilize the hardware. In this work,\nwe advocate for the development of hybrid methods that are built on top of the\nframeworks of existing state-of-art heuristics, thereby improving these\nmethods. We demonstrate this by building on the so called Louvain method, which\nis one of the most popular algorithms for the Community detection problem and\ndevelop and Ising-based Louvain method. The proposed method outperforms two\nstate-of-the-art community detection algorithms in clustering several small to\nlarge-scale graphs. The results show promise in adapting the same optimization\napproach to other unsupervised learning heuristics to improve their\nperformance.",
    "Trust plays an essential role in an individual's decision-making. Traditional\ntrust prediction models rely on pairwise correlations to infer potential\nrelationships between users. However, in the real world, interactions between\nusers are usually complicated rather than pairwise only. Hypergraphs offer a\nflexible approach to modeling these complex high-order correlations (not just\npairwise connections), since hypergraphs can leverage hyperedeges to link more\nthan two nodes. However, most hypergraph-based methods are generic and cannot\nbe well applied to the trust prediction task. In this paper, we propose an\nAdaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that\nimproves trust prediction accuracy by using higher-order correlations. AHNTP\nutilizes Motif-based PageRank to capture high-order social influence\ninformation. In addition, it constructs hypergroups from both node-level and\nstructure-level attributes to incorporate complex correlation information.\nFurthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network\n(GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user\nembeddings, facilitating trust relationship prediction. To enhance model\ngeneralization and robustness, we introduce a novel supervised contrastive\nlearning loss for optimization. Extensive experiments demonstrate the\nsuperiority of our model over the state-of-the-art approaches in terms of trust\nprediction accuracy. The source code of this work can be accessed via\nhttps://github.com/Sherry-XU1995/AHNTP.",
    "Users increasingly rely on social media feeds for consuming daily\ninformation. The items in a feed, such as news, questions, songs, etc., usually\nresult from the complex interplay of a user's social contacts, her interests\nand her actions on the platform. The relationship of the user's own behavior\nand the received feed is often puzzling, and many users would like to have a\nclear explanation on why certain items were shown to them. Transparency and\nexplainability are key concerns in the modern world of cognitive overload,\nfilter bubbles, user tracking, and privacy risks. This paper presents FAIRY, a\nframework that systematically discovers, ranks, and explains relationships\nbetween users' actions and items in their social media feeds. We model the\nuser's local neighborhood on the platform as an interaction graph, a form of\nheterogeneous information network constructed solely from information that is\neasily accessible to the concerned user. We posit that paths in this\ninteraction graph connecting the user and her feed items can act as pertinent\nexplanations for the user. These paths are scored with a learning-to-rank model\nthat captures relevance and surprisal. User studies on two social platforms\ndemonstrate the practical viability and user benefits of the FAIRY method.",
    "Efforts by foreign actors to influence public opinion have gained\nconsiderable attention because of their potential to impact democratic\nelections. Thus, the ability to identify and counter sources of disinformation\nis increasingly becoming a top priority for government entities in order to\nprotect the integrity of democratic processes. This study presents a method of\nidentifying Russian disinformation bots on Twitter using centering resonance\nanalysis and Clauset-Newman-Moore community detection. The data reflect a\nsignificant degree of discursive dissimilarity between known Russian\ndisinformation bots and a control set of Twitter users during the timeframe of\nthe 2016 U.S. Presidential Election. The data also demonstrate statistically\nsignificant classification capabilities (MCC = 0.9070) based on community\nclustering. The prediction algorithm is very effective at identifying true\npositives (bots), but is not able to resolve true negatives (non-bots) because\nof the lack of discursive similarity between control users. This leads to a\nhighly sensitive means of identifying propagators of disinformation with a high\ndegree of discursive similarity on Twitter, with implications for limiting the\nspread of disinformation that could impact democratic processes.",
    "Real-world networks usually have community structure, that is, nodes are\ngrouped into densely connected communities. Community detection is one of the\nmost popular and best-studied research topics in network science and has\nattracted attention in many different fields, including computer science,\nstatistics, social sciences, among others. Numerous approaches for community\ndetection have been proposed in literature, from ad-hoc algorithms to\nsystematic model-based approaches. The large number of available methods leads\nto a fundamental question: whether a certain method can provide consistent\nestimates of community labels. The stochastic blockmodel (SBM) and its variants\nprovide a convenient framework for the study of such problems. This article is\na survey on the recent theoretical advances of community detection. The authors\nreview a number of community detection methods and their theoretical\nproperties, including graph cut methods, profile likelihoods, the\npseudo-likelihood method, the variational method, belief propagation, spectral\nclustering, and semidefinite relaxations of the SBM. The authors also briefly\ndiscuss other research topics in community detection such as robust community\ndetection, community detection with nodal covariates and model selection, as\nwell as suggest a few possible directions for future research.",
    "With the widespread use of information technologies, information networks are\nbecoming increasingly popular to capture complex relationships across various\ndisciplines, such as social networks, citation networks, telecommunication\nnetworks, and biological networks. Analyzing these networks sheds light on\ndifferent aspects of social life such as the structure of societies,\ninformation diffusion, and communication patterns. In reality, however, the\nlarge scale of information networks often makes network analytic tasks\ncomputationally expensive or intractable. Network representation learning has\nbeen recently proposed as a new learning paradigm to embed network vertices\ninto a low-dimensional vector space, by preserving network topology structure,\nvertex content, and other side information. This facilitates the original\nnetwork to be easily handled in the new vector space for further analysis. In\nthis survey, we perform a comprehensive review of the current literature on\nnetwork representation learning in the data mining and machine learning field.\nWe propose new taxonomies to categorize and summarize the state-of-the-art\nnetwork representation learning techniques according to the underlying learning\nmechanisms, the network information intended to preserve, as well as the\nalgorithmic designs and methodologies. We summarize evaluation protocols used\nfor validating network representation learning including published benchmark\ndatasets, evaluation methods, and open source algorithms. We also perform\nempirical studies to compare the performance of representative algorithms on\ncommon datasets, and analyze their computational complexity. Finally, we\nsuggest promising research directions to facilitate future study.",
    "Cyberbullying is a pervasive problem in online communities. To identify\ncyberbullying cases in large-scale social networks, content moderators depend\non machine learning classifiers for automatic cyberbullying detection. However,\nexisting models remain unfit for real-world applications, largely due to a\nshortage of publicly available training data and a lack of standard criteria\nfor assigning ground truth labels. In this study, we address the need for\nreliable data using an original annotation framework. Inspired by social\nsciences research into bullying behavior, we characterize the nuanced problem\nof cyberbullying using five explicit factors to represent its social and\nlinguistic aspects. We model this behavior using social network and\nlanguage-based features, which improve classifier performance. These results\ndemonstrate the importance of representing and modeling cyberbullying as a\nsocial phenomenon.",
    "With the significant increase in users on social media platforms, a new means\nof political campaigning has appeared. Twitter and Facebook are now notable\ncampaigning tools during elections. Indeed, the candidates and their parties\nnow take to the internet to interact and spread their ideas. In this paper, we\naim to identify political communities formed on Twitter during the 2022 French\npresidential election and analyze each respective community. We create a\nlarge-scale Twitter dataset containing 1.2 million users and 62.6 million\ntweets that mention keywords relevant to the election. We perform community\ndetection on a retweet graph of users and propose an in-depth analysis of the\nstance of each community. Finally, we attempt to detect offensive tweets and\nautomatic bots, comparing across communities in order to gain insight into each\ncandidate's supporter demographics and online campaign strategy.",
    "Deep graph embedding is an important approach for community discovery. Deep\ngraph neural network with self-supervised mechanism can obtain the\nlow-dimensional embedding vectors of nodes from unlabeled and unstructured\ngraph data. The high-order information of graph can provide more abundant\nstructure information for the representation learning of nodes. However, most\nself-supervised graph neural networks only use adjacency matrix as the input\ntopology information of graph and cannot obtain too high-order information\nsince the number of layers of graph neural network is fairly limited. If there\nare too many layers, the phenomenon of over smoothing will appear. Therefore\nhow to obtain and fuse high-order information of graph by a shallow graph\nneural network is an important problem. In this paper, a deep graph embedding\nalgorithm with self-supervised mechanism for community discovery is proposed.\nThe proposed algorithm uses self-supervised mechanism and different high-order\ninformation of graph to train multiple deep graph convolution neural networks.\nThe outputs of multiple graph convolution neural networks are fused to extract\nthe representations of nodes which include the attribute and structure\ninformation of a graph. In addition, data augmentation and negative sampling\nare introduced into the training process to facilitate the improvement of\nembedding result. The proposed algorithm and the comparison algorithms are\nconducted on the five experimental data sets. The experimental results show\nthat the proposed algorithm outperforms the comparison algorithms on the most\nexperimental data sets. The experimental results demonstrate that the proposed\nalgorithm is an effective algorithm for community discovery.",
    "Recent work in graph models has found that probabilistic hyperedge\nreplacement grammars (HRGs) can be extracted from graphs and used to generate\nnew random graphs with graph properties and substructures close to the\noriginal. In this paper, we show how to add latent variables to the model,\ntrained using Expectation-Maximization, to generate still better graphs, that\nis, ones that generalize better to the test data. We evaluate the new method by\nseparating training and test graphs, building the model on the former and\nmeasuring the likelihood of the latter, as a more stringent test of how well\nthe model can generalize to new graphs. On this metric, we find that our\nlatent-variable HRGs consistently outperform several existing graph models and\nprovide interesting insights into the building blocks of real world networks.",
    "Edge streams are commonly used to capture interactions in dynamic networks,\nsuch as email, social, or computer networks. The problem of detecting anomalies\nor rare events in edge streams has a wide range of applications. However, it\npresents many challenges due to lack of labels, a highly dynamic nature of\ninteractions, and the entanglement of temporal and structural changes in the\nnetwork. Current methods are limited in their ability to address the above\nchallenges and to efficiently process a large number of interactions. Here, we\npropose F-FADE, a new approach for detection of anomalies in edge streams,\nwhich uses a novel frequency-factorization technique to efficiently model the\ntime-evolving distributions of frequencies of interactions between node-pairs.\nThe anomalies are then determined based on the likelihood of the observed\nfrequency of each incoming interaction. F-FADE is able to handle in an online\nstreaming setting a broad variety of anomalies with temporal and structural\nchanges, while requiring only constant memory. Our experiments on one synthetic\nand six real-world dynamic networks show that F-FADE achieves state of the art\nperformance and may detect anomalies that previous methods are unable to find.",
    "With the great success of graph embedding model on both academic and industry\narea, the robustness of graph embedding against adversarial attack inevitably\nbecomes a central problem in graph learning domain. Regardless of the fruitful\nprogress, most of the current works perform the attack in a white-box fashion:\nthey need to access the model predictions and labels to construct their\nadversarial loss. However, the inaccessibility of model predictions in real\nsystems makes the white-box attack impractical to real graph learning system.\nThis paper promotes current frameworks in a more general and flexible sense --\nwe demand to attack various kinds of graph embedding model with black-box\ndriven. To this end, we begin by investigating the theoretical connections\nbetween graph signal processing and graph embedding models in a principled way\nand formulate the graph embedding model as a general graph signal process with\ncorresponding graph filter. As such, a generalized adversarial attacker:\nGF-Attack is constructed by the graph filter and feature matrix. Instead of\naccessing any knowledge of the target classifiers used in graph embedding,\nGF-Attack performs the attack only on the graph filter in a black-box attack\nfashion. To validate the generalization of GF-Attack, we construct the attacker\non four popular graph embedding models. Extensive experimental results validate\nthe effectiveness of our attacker on several benchmark datasets. Particularly\nby using our attack, even small graph perturbations like one-edge flip is able\nto consistently make a strong attack in performance to different graph\nembedding models.",
    "This work investigates the problem of learning temporal interaction networks.\nA temporal interaction network consists of a series of chronological\ninteractions between users and items. Previous methods tackle this problem by\nusing different variants of recurrent neural networks to model sequential\ninteractions, which fail to consider the structural information of temporal\ninteraction networks and inevitably lead to sub-optimal results. To this end,\nwe propose a novel Deep Structural Point Process termed as DSPP for learning\ntemporal interaction networks. DSPP simultaneously incorporates the topological\nstructure and long-range dependency structure into our intensity function to\nenhance model expressiveness. To be specific, by using the topological\nstructure as a strong prior, we first design a topological fusion encoder to\nobtain node embeddings. An attentive shift encoder is then developed to learn\nthe long-range dependency structure between users and items in continuous time.\nThe proposed two modules enable our model to capture the user-item correlation\nand dynamic influence in temporal interaction networks. DSPP is evaluated on\nthree real-world datasets for both tasks of item prediction and time\nprediction. Extensive experiments demonstrate that our model achieves\nconsistent and significant improvements over state-of-the-art baselines.",
    "Detecting anomalies from a series of temporal networks has many applications,\nincluding road accidents in transport networks and suspicious events in social\nnetworks. While there are many methods for network anomaly detection,\nstatistical methods are under utilised in this space even though they have a\nlong history and proven capability in handling temporal dependencies. In this\npaper, we introduce \\textit{oddnet}, a feature-based network anomaly detection\nmethod that uses time series methods to model temporal dependencies. We\ndemonstrate the effectiveness of oddnet on synthetic and real-world datasets.\nThe R package oddnet implements this algorithm.",
    "Public opinion is a crucial factor in shaping political decision-making.\nNowadays, social media has become an essential platform for individuals to\nengage in political discussions and express their political views, presenting\nresearchers with an invaluable resource for analyzing public opinion. In this\npaper, we focus on the 2020 US presidential election and create a large-scale\ndataset from Twitter. To detect political opinions in tweets, we build a\nuser-tweet bipartite graph based on users' posting and retweeting behaviors and\nconvert the task into a Graph Neural Network (GNN)-based node classification\nproblem. Then, we introduce a novel skip aggregation mechanism that makes tweet\nnodes aggregate information from second-order neighbors, which are also tweet\nnodes due to the graph's bipartite nature, effectively leveraging user\nbehavioral information. The experimental results show that our proposed model\nsignificantly outperforms several competitive baselines. Further analyses\ndemonstrate the significance of user behavioral information and the\neffectiveness of skip aggregation.",
    "Do higher-order network structures aid graph semi-supervised learning? Given\na graph and a few labeled vertices, labeling the remaining vertices is a\nhigh-impact problem with applications in several tasks, such as recommender\nsystems, fraud detection and protein identification. However, traditional\nmethods rely on edges for spreading labels, which is limited as all edges are\nnot equal. Vertices with stronger connections participate in higher-order\nstructures in graphs, which calls for methods that can leverage these\nstructures in the semi-supervised learning tasks.\n  To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels\nusing higher-order structures. HOLS has strong theoretical guarantees and\nreduces to standard label spreading in the base case. Via extensive\nexperiments, we show that higher-order label spreading using triangles in\naddition to edges is up to 4.7% better than label spreading using edges alone.\nCompared to prior traditional and state-of-the-art methods, the proposed method\nleads to statistically significant accuracy gains in all-but-one cases, while\nremaining fast and scalable to large graphs.",
    "Social media for news consumption is a double-edged sword. On the one hand,\nits low cost, easy access, and rapid dissemination of information lead people\nto seek out and consume news from social media. On the other hand, it enables\nthe wide spread of \"fake news\", i.e., low quality news with intentionally false\ninformation. The extensive spread of fake news has the potential for extremely\nnegative impacts on individuals and society. Therefore, fake news detection on\nsocial media has recently become an emerging research that is attracting\ntremendous attention. Fake news detection on social media presents unique\ncharacteristics and challenges that make existing detection algorithms from\ntraditional news media ineffective or not applicable. First, fake news is\nintentionally written to mislead readers to believe false information, which\nmakes it difficult and nontrivial to detect based on news content; therefore,\nwe need to include auxiliary information, such as user social engagements on\nsocial media, to help make a determination. Second, exploiting this auxiliary\ninformation is challenging in and of itself as users' social engagements with\nfake news produce data that is big, incomplete, unstructured, and noisy.\nBecause the issue of fake news detection on social media is both challenging\nand relevant, we conducted this survey to further facilitate research on the\nproblem. In this survey, we present a comprehensive review of detecting fake\nnews on social media, including fake news characterizations on psychology and\nsocial theories, existing algorithms from a data mining perspective, evaluation\nmetrics and representative datasets. We also discuss related research areas,\nopen problems, and future research directions for fake news detection on social\nmedia.",
    "Multi-view networks are broadly present in real-world applications. In the\nmeantime, network embedding has emerged as an effective representation learning\napproach for networked data. Therefore, we are motivated to study the problem\nof multi-view network embedding with a focus on the optimization objectives\nthat are specific and important in embedding this type of network. In our\npractice of embedding real-world multi-view networks, we explicitly identify\ntwo such objectives, which we refer to as preservation and collaboration. The\nin-depth analysis of these two objectives is discussed throughout this paper.\nIn addition, the mvn2vec algorithms are proposed to (i) study how varied extent\nof preservation and collaboration can impact embedding learning and (ii)\nexplore the feasibility of achieving better embedding quality by modeling them\nsimultaneously. With experiments on a series of synthetic datasets, a\nlarge-scale internal Snapchat dataset, and two public datasets, we confirm the\nvalidity and importance of preservation and collaboration as two objectives for\nmulti-view network embedding. These experiments further demonstrate that better\nembedding can be obtained by simultaneously modeling the two objectives, while\nnot over-complicating the model or requiring additional supervision. The code\nand the processed datasets are available at\nhttp://yushi2.web.engr.illinois.edu/.",
    "Finding meaningful communities in social network has attracted the attentions\nof many researchers. The community structure of complex networks reveals both\ntheir organization and hidden relations among their constituents. Most of the\nresearches in the field of community detection mainly focus on the topological\nstructure of the network without performing any content analysis. Nowadays,\nreal world social networks are containing a vast range of information including\nshared objects, comments, following information, etc. In recent years, a number\nof researches have proposed approaches which consider both the contents that\nare interchanged in the networks and the topological structures of the networks\nin order to find more meaningful communities. In this research, the effect of\ntopic analysis in finding more meaningful communities in social networking\nsites in which the users express their feelings toward different objects (like\nmovies) by the means of rating is demonstrated by performing extensive\nexperiments.",
    "How can we recognise social roles of people, given a completely unlabelled\nsocial network? We present a transfer learning approach to network role\nclassification based on feature transformations from each network's local\nfeature distribution to a global feature space. Experiments are carried out on\nreal-world datasets. (See manuscript for the full abstract.)",
    "We discuss a variant of `blind' community detection, in which we aim to\npartition an unobserved network from the observation of a (dynamical) graph\nsignal defined on the network. We consider a scenario where our observed graph\nsignals are obtained by filtering white noise input, and the underlying network\nis different for every observation. In this fashion, the filtered graph signals\ncan be interpreted as defined on a time-varying network. We model each of the\nunderlying network realizations as generated by an independent draw from a\nlatent stochastic blockmodel (SBM). To infer the partition of the latent SBM,\nwe propose a simple spectral algorithm for which we provide a theoretical\nanalysis and establish consistency guarantees for the recovery. We illustrate\nour results using numerical experiments on synthetic and real data,\nhighlighting the efficacy of our approach.",
    "The discourse around conspiracy theories is currently thriving amidst the\nrampant misinformation in online environments. Research in this field has been\nfocused on detecting conspiracy theories on social media, often relying on\nlimited datasets. In this study, we present a novel methodology for\nconstructing a Twitter dataset that encompasses accounts engaged in\nconspiracy-related activities throughout the year 2022. Our approach centers on\ndata collection that is independent of specific conspiracy theories and\ninformation operations. Additionally, our dataset includes a control group\ncomprising randomly selected users who can be fairly compared to the\nindividuals involved in conspiracy activities. This comprehensive collection\neffort yielded a total of 15K accounts and 37M tweets extracted from their\ntimelines. We conduct a comparative analysis of the two groups across three\ndimensions: topics, profiles, and behavioral characteristics. The results\nindicate that conspiracy and control users exhibit similarity in terms of their\nprofile metadata characteristics. However, they diverge significantly in terms\nof behavior and activity, particularly regarding the discussed topics, the\nterminology used, and their stance on trending subjects. In addition, we find\nno significant disparity in the presence of bot users between the two groups.\nFinally, we develop a classifier to identify conspiracy users using features\nborrowed from bot, troll and linguistic literature. The results demonstrate a\nhigh accuracy level (with an F1 score of 0.94), enabling us to uncover the most\ndiscriminating features associated with conspiracy-related accounts.",
    "In this work, we study the utility of graph embeddings to generate latent\nuser representations for trust-based collaborative filtering. In a cold-start\nsetting, on three publicly available datasets, we evaluate approaches from four\nmethod families: (i) factorization-based, (ii) random walk-based, (iii) deep\nlearning-based, and (iv) the Large-scale Information Network Embedding (LINE)\napproach. We find that across the four families, random-walk-based approaches\nconsistently achieve the best accuracy. Besides, they result in highly novel\nand diverse recommendations. Furthermore, our results show that the use of\ngraph embeddings in trust-based collaborative filtering significantly improves\nuser coverage.",
    "Networks found in the real-world are numerous and varied. A common type of\nnetwork is the heterogeneous network, where the nodes (and edges) can be of\ndifferent types. Accordingly, there have been efforts at learning\nrepresentations of these heterogeneous networks in low-dimensional space.\nHowever, most of the existing heterogeneous network embedding methods suffer\nfrom the following two drawbacks: (1) The target space is usually Euclidean.\nConversely, many recent works have shown that complex networks may have\nhyperbolic latent anatomy, which is non-Euclidean. (2) These methods usually\nrely on meta-paths, which require domain-specific prior knowledge for meta-path\nselection. Additionally, different down-streaming tasks on the same network\nmight require different meta-paths in order to generate task-specific\nembeddings. In this paper, we propose a novel self-guided random walk method\nthat does not require meta-path for embedding heterogeneous networks into\nhyperbolic space. We conduct thorough experiments for the tasks of network\nreconstruction and link prediction on two public datasets, showing that our\nmodel outperforms a variety of well-known baselines across all tasks.",
    "Bipartite graph embedding has recently attracted much attention due to the\nfact that bipartite graphs are widely used in various application domains. Most\nprevious methods, which adopt random walk-based or reconstruction-based\nobjectives, are typically effective to learn local graph structures. However,\nthe global properties of bipartite graph, including community structures of\nhomogeneous nodes and long-range dependencies of heterogeneous nodes, are not\nwell preserved. In this paper, we propose a bipartite graph embedding called\nBiGI to capture such global properties by introducing a novel local-global\ninfomax objective. Specifically, BiGI first generates a global representation\nwhich is composed of two prototype representations. BiGI then encodes sampled\nedges as local representations via the proposed subgraph-level attention\nmechanism. Through maximizing the mutual information between local and global\nrepresentations, BiGI enables nodes in bipartite graph to be globally relevant.\nOur model is evaluated on various benchmark datasets for the tasks of top-K\nrecommendation and link prediction. Extensive experiments demonstrate that BiGI\nachieves consistent and significant improvements over state-of-the-art\nbaselines. Detailed analyses verify the high effectiveness of modeling the\nglobal properties of bipartite graph.",
    "We propose a method for simultaneously detecting shared and unshared\ncommunities in heterogeneous multilayer weighted and undirected networks. The\nmultilayer network is assumed to follow a generative probabilistic model that\ntakes into account the similarities and dissimilarities between the\ncommunities. We make use of a variational Bayes approach for jointly inferring\nthe shared and unshared hidden communities from multilayer network\nobservations. We show that our approach outperforms state-of-the-art algorithms\nin detecting disparate (shared and private) communities on synthetic data as\nwell as on real genome-wide fibroblast proliferation dataset.",
    "Many works have been proposed in the literature to capture the dynamics of\ndiffusion in networks. While some of them define graphical markovian models to\nextract temporal relationships between node infections in networks, others\nconsider diffusion episodes as sequences of infections via recurrent neural\nmodels. In this paper we propose a model at the crossroads of these two\nextremes, which embeds the history of diffusion in infected nodes as hidden\ncontinuous states. Depending on the trajectory followed by the content before\nreaching a given node, the distribution of influence probabilities may vary.\nHowever, content trajectories are usually hidden in the data, which induces\nchallenging learning problems. We propose a topological recurrent neural model\nwhich exhibits good experimental performances for diffusion modelling and\nprediction.",
    "Inferring latent attributes of people online is an important social computing\ntask, but requires integrating the many heterogeneous sources of information\navailable on the web. We propose learning individual representations of people\nusing neural nets to integrate rich linguistic and network evidence gathered\nfrom social media. The algorithm is able to combine diverse cues, such as the\ntext a person writes, their attributes (e.g. gender, employer, education,\nlocation) and social relations to other people. We show that by integrating\nboth textual and network evidence, these representations offer improved\nperformance at four important tasks in social media inference on Twitter:\npredicting (1) gender, (2) occupation, (3) location, and (4) friendships for\nusers. Our approach scales to large datasets and the learned representations\ncan be used as general features in and have the potential to benefit a large\nnumber of downstream tasks including link prediction, community detection, or\nprobabilistic reasoning over social networks.",
    "In this short paper, we evaluate the performance of three well-known Machine\nLearning techniques for predicting the impact of a post in Facebook. Social\nmedias have a huge influence in the social behaviour. Therefore to develop an\nautomatic model for predicting the impact of posts in social medias can be\nuseful to the society. In this article, we analyze the efficiency for\npredicting the post impact of three popular techniques: Support Vector\nRegression (SVR), Echo State Network (ESN) and Adaptive Network Fuzzy Inject\nSystem (ANFIS). The evaluation was done over a public and well-known benchmark\ndataset.",
    "An identity denotes the role an individual or a group plays in highly\ndifferentiated contemporary societies. In this paper, our goal is to classify\nTwitter users based on their role identities. We first collect a coarse-grained\npublic figure dataset automatically, then manually label a more fine-grained\nidentity dataset. We propose a hierarchical self-attention neural network for\nTwitter user role identity classification. Our experiments demonstrate that the\nproposed model significantly outperforms multiple baselines. We further propose\na transfer learning scheme that improves our model's performance by a large\nmargin. Such transfer learning also greatly reduces the need for a large amount\nof human labeled data.",
    "In this work we propose Lasagne, a methodology to learn locality and\nstructure aware graph node embeddings in an unsupervised way. In particular, we\nshow that the performance of existing random-walk based approaches depends\nstrongly on the structural properties of the graph, e.g., the size of the\ngraph, whether the graph has a flat or upward-sloping Network Community Profile\n(NCP), whether the graph is expander-like, whether the classes of interest are\nmore k-core-like or more peripheral, etc. For larger graphs with flat NCPs that\nare strongly expander-like, existing methods lead to random walks that expand\nrapidly, touching many dissimilar nodes, thereby leading to lower-quality\nvector representations that are less useful for downstream tasks. Rather than\nrelying on global random walks or neighbors within fixed hop distances, Lasagne\nexploits strongly local Approximate Personalized PageRank stationary\ndistributions to more precisely engineer local information into node\nembeddings. This leads, in particular, to more meaningful and more useful\nvector representations of nodes in poorly-structured graphs. We show that\nLasagne leads to significant improvement in downstream multi-label\nclassification for larger graphs with flat NCPs, that it is comparable for\nsmaller graphs with upward-sloping NCPs, and that is comparable to existing\nmethods for link prediction tasks.",
    "On 26 January 2021, India witnessed a national embarrassment from the\ndemographic least expected from - farmers. People across the nation watched in\nhorror as a pseudo-patriotic mob of farmers stormed capital Delhi and\nvandalized the national pride- Red Fort. Investigations that followed the event\nrevealed the existence of a social media trail that led to the likes of such an\nevent. Consequently, it became essential and necessary to archive this trail\nfor social media analysis - not only to understand the bread-crumbs that are\ndispersed across the trail but also to visualize the role played by\nmisinformation and fake news in this event. In this paper, we propose the\ntractor2twitter dataset which contains around 0.05 million tweets that were\nposted before, during, and after this event. Also, we benchmark our dataset\nwith an Explainable AI ML model for classification of each tweet into either of\nthe three categories - disinformation, misinformation, and opinion.",
    "Social networks are often associated with rich side information, such as\ntexts and images. While numerous methods have been developed to identify\ncommunities from pairwise interactions, they usually ignore such side\ninformation. In this work, we study an extension of the Stochastic Block Model\n(SBM), a widely used statistical framework for community detection, that\nintegrates vectorial edges covariates: the Vectorial Edges Covariates\nStochastic Block Model (VEC-SBM). We propose a novel algorithm based on\niterative refinement techniques and show that it optimally recovers the latent\ncommunities under the VEC-SBM. Furthermore, we rigorously assess the added\nvalue of leveraging edge's side information in the community detection process.\nWe complement our theoretical results with numerical experiments on synthetic\nand semi-synthetic data.",
    "In standard graph clustering/community detection, one is interested in\npartitioning the graph into more densely connected subsets of nodes. In\ncontrast, the \"search\" problem of this paper aims to only find the nodes in a\n\"single\" such community, the target, out of the many communities that may\nexist. To do so , we are given suitable side information about the target; for\nexample, a very small number of nodes from the target are labeled as such.\n  We consider a general yet simple notion of side information: all nodes are\nassumed to have random weights, with nodes in the target having higher weights\non average. Given these weights and the graph, we develop a variant of the\nmethod of moments that identifies nodes in the target more reliably, and with\nlower computation, than generic community detection methods that do not use\nside information and partition the entire graph. Our empirical results show\nsignificant gains in runtime, and also gains in accuracy over other graph\nclustering algorithms.",
    "We consider the problem of learning the weighted edges of a graph by\nobserving the noisy times of infection for multiple epidemic cascades on this\ngraph. Past work has considered this problem when the cascade information,\ni.e., infection times, are known exactly. Though the noisy setting is well\nmotivated by many epidemic processes (e.g., most human epidemics), to the best\nof our knowledge, very little is known about when it is solvable. Previous work\non the no-noise setting critically uses the ordering information. If noise can\nreverse this -- a node's reported (noisy) infection time comes after the\nreported infection time of some node it infected -- then we are unable to see\nhow previous results can be extended.\n  We therefore tackle two versions of the noisy setting: the limited-noise\nsetting, where we know noisy times of infections, and the extreme-noise\nsetting, in which we only know whether or not a node was infected. We provide a\npolynomial time algorithm for recovering the structure of bidirectional trees\nin the extreme-noise setting, and show our algorithm almost matches lower\nbounds established in the no-noise setting, and hence is optimal up to\nlog-factors. We extend our results for general degree-bounded graphs, where\nagain we show that our (poly-time) algorithm can recover the structure of the\ngraph with optimal sample complexity. We also provide the first efficient\nalgorithm to learn the weights of the bidirectional tree in the limited-noise\nsetting. Finally, we give a polynomial time algorithm for learning the weights\nof general bounded-degree graphs in the limited-noise setting. This algorithm\nextends to general graphs (at the price of exponential running time), proving\nthe problem is solvable in the general case. All our algorithms work for any\nnoise distribution, without any restriction on the variance.",
    "Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics\nin network analysis, providing essential reference for discerning the\nsignificance of nodes within complex networks. These measures find wide\napplications in critical tasks, such as community detection and network\ndismantling. However, their practical implementation on extensive networks\nremains computationally demanding due to their high time complexity. To\nmitigate these computational challenges, numerous approximation algorithms have\nbeen developed to expedite the computation of CC and BC. Nevertheless, even\nthese approximations still necessitate substantial processing time when applied\nto large-scale networks. Furthermore, their output proves sensitive to even\nminor perturbations within the network structure.\n  In this work, We redefine the CC and BC node ranking problem as a machine\nlearning problem and propose the CNCA-IGE model, which is an encoder-decoder\nmodel based on inductive graph neural networks designed to rank nodes based on\nspecified CC or BC metrics. We incorporate the MLP-Mixer model as the decoder\nin the BC ranking prediction task to enhance the model's robustness and\ncapacity. Our approach is evaluated on diverse synthetic and real-world\nnetworks of varying scales, and the experimental results demonstrate that the\nCNCA-IGE model outperforms state-of-the-art baseline models, significantly\nreducing execution time while improving performance.",
    "We propose a detailed analysis of the online-learning problem for Independent\nCascade (IC) models under node-level feedback. These models have widespread\napplications in modern social networks. Existing works for IC models have only\nshed light on edge-level feedback models, where the agent knows the explicit\noutcome of every observed edge. Little is known about node-level feedback\nmodels, where only combined outcomes for sets of edges are observed; in other\nwords, the realization of each edge is censored. This censored information,\ntogether with the nonlinear form of the aggregated influence probability, make\nboth parameter estimation and algorithm design challenging. We establish the\nfirst confidence-region result under this setting. We also develop an online\nalgorithm achieving a cumulative regret of $\\mathcal{O}( \\sqrt{T})$, matching\nthe theoretical regret bound for IC models with edge-level feedback.",
    "Much of the complexity of social, biological, and engineered systems arises\nfrom a network of complex interactions connecting many basic components.\nNetwork analysis tools have been successful at uncovering latent structure\ntermed communities in such networks. However, some of the most interesting\nstructure can be difficult to uncover because it is obscured by the more\ndominant structure. Our previous work proposes a general structure\namplification technique called HICODE that uncovers many layers of functional\nhidden structure in complex networks. HICODE incrementally weakens dominant\nstructure through randomization allowing the hidden functionality to emerge,\nand uncovers these hidden structure in real-world networks that previous\nmethods rarely uncover. In this work, we conduct a comprehensive and systematic\ntheoretical analysis on the hidden community structure. In what follows, we\ndefine multi-layer stochastic block model, and provide theoretical support\nusing the model on why the existence of hidden structure will make the\ndetection of dominant structure harder compared with equivalent random noise.\nWe then provide theoretical proofs that the iterative reducing methods could\nhelp promote the uncovering of hidden structure as well as boosting the\ndetection quality of dominant structure.",
    "Existing network embedding approaches tackle the problem of learning\nlow-dimensional node representations. However, networks can also be seen in the\nlight of edges interlinking pairs of nodes. The broad goal of this paper is to\nintroduce edge-centric network embeddings. We present an approach called ECNE,\nwhich instead of computing node embeddings directly, computes edge embeddings\nby relying on the notion of line graph coupled with an edge weighting mechanism\nto preserve the dynamic of the original graph in the line graph. We also\npresent a link prediction framework called ECNE-LP, which given a target link\n(u,v) first collects paths between nodes u and v, then directly embeds the\nedges in these paths, and finally aggregates them toward predicting the\nexistence of a link. We show that both ECNE and ECNE-LP bring benefit wrt the\nstate-of-the-art.",
    "A collection of $U \\: (\\in \\mathbb{N})$ data vectors is called a $U$-tuple,\nand the association strength among the vectors of a tuple is termed as the\n\\emph{hyperlink weight}, that is assumed to be symmetric with respect to\npermutation of the entries in the index. We herein propose Bregman hyperlink\nregression (BHLR), which learns a user-specified symmetric similarity function\nsuch that it predicts the tuple's hyperlink weight from data vectors stored in\nthe $U$-tuple. BHLR is a simple and general framework for hyper-relational\nlearning, that minimizes Bregman-divergence (BD) between the hyperlink weights\nand estimated similarities defined for the corresponding tuples; BHLR\nencompasses various existing methods, such as logistic regression ($U=1$),\nPoisson regression ($U=1$), link prediction ($U=2$), and those for\nrepresentation learning, such as graph embedding ($U=2$), matrix factorization\n($U=2$), tensor factorization ($U \\geq 2$), and their variants equipped with\narbitrary BD. Nonlinear functions (e.g., neural networks), can be employed for\nthe similarity functions. However, there are theoretical challenges such that\nsome of different tuples of BHLR may share data vectors therein, unlike the\ni.i.d. setting of classical regression. We address these theoretical issues,\nand proved that BHLR equipped with arbitrary BD and $U \\in \\mathbb{N}$ is (P-1)\nstatistically consistent, that is, it asymptotically recovers the underlying\ntrue conditional expectation of hyperlink weights given data vectors, and (P-2)\ncomputationally tractable, that is, it is efficiently computed by stochastic\noptimization algorithms using a novel generalized minibatch sampling procedure\nfor hyper-relational data. Consequently, theoretical guarantees for BHLR\nincluding several existing methods, that have been examined experimentally, are\nprovided in a unified manner.",
    "Though current researches often study the properties of online social\nrelationship from an objective view, we also need to understand individuals'\nsubjective opinions on their interrelationships in social computing studies.\nInspired by the theories from sociolinguistics, the latest work indicates that\ninteractive language can reveal individuals' asymmetric opinions on their\ninterrelationship. In this work, in order to explain the opinions' asymmetry on\ninterrelationship with more latent factors, we extend the investigation from\nsingle relationship to the structural context in online social network. We\nanalyze the correlation between interactive language features and the\nstructural context of interrelationships. The structural context of vertex,\nedges and triangles in social network are considered. With statistical analysis\non Enron email dataset, we find that individuals' opinions (measured by\ninteractive language features) on their interrelationship are related to some\nof their important structural context in social network. This result can help\nus to understand and measure the individuals' opinions on their\ninterrelationship with more intrinsic information.",
    "Link prediction is widely used in a variety of industrial applications, such\nas merchant recommendation, fraudulent transaction detection, and so on.\nHowever, it's a great challenge to train and deploy a link prediction model on\nindustrial-scale graphs with billions of nodes and edges. In this work, we\npresent a scalable and distributed framework for semi-supervised link\nprediction problem (named DSSLP), which is able to handle industrial-scale\ngraphs. Instead of training model on the whole graph, DSSLP is proposed to\ntrain on the \\emph{$k$-hops neighborhood} of nodes in a mini-batch setting,\nwhich helps reduce the scale of the input graph and distribute the training\nprocedure. In order to generate negative examples effectively, DSSLP contains a\ndistributed batched runtime sampling module. It implements uniform and dynamic\nsampling approaches, and is able to adaptively construct positive and negative\nexamples to guide the training process. Moreover, DSSLP proposes a model-split\nstrategy to accelerate the speed of inference process of the link prediction\ntask. Experimental results demonstrate that the effectiveness and efficiency of\nDSSLP in serval public datasets as well as real-world datasets of\nindustrial-scale graphs.",
    "We explore a method to influence or even control the diversity of opinions\nwithin a polarised social group. We leverage the voter model in which users\nhold binary opinions and repeatedly update their beliefs based on others they\nconnect with. Stubborn agents who never change their minds (\"zealots\") are also\ndisseminated through the network, which is modelled by a connected graph.\nBuilding on earlier results, we provide a closed-form expression for the\naverage opinion of the group at equilibrium. This leads us to a strategy to\ninject zealots into a polarised network in order to shift the average opinion\ntowards any target value. We account for the possible presence of a backfire\neffect, which may lead the group to react negatively and reinforce its level of\npolarisation in response. Our results are supported by numerical experiments on\nsynthetic data.",
    "Arabic Twitter space is crawling with bots that fuel political feuds, spread\nmisinformation, and proliferate sectarian rhetoric. While efforts have long\nexisted to analyze and detect English bots, Arabic bot detection and\ncharacterization remains largely understudied. In this work, we contribute new\ninsights into the role of bots in spreading religious hatred on Arabic Twitter\nand introduce a novel regression model that can accurately identify Arabic\nlanguage bots. Our assessment shows that existing tools that are highly\naccurate in detecting English bots don't perform as well on Arabic bots. We\nidentify the possible reasons for this poor performance, perform a thorough\nanalysis of linguistic, content, behavioral and network features, and report on\nthe most informative features that distinguish Arabic bots from humans as well\nas the differences between Arabic and English bots. Our results mark an\nimportant step toward understanding the behavior of malicious bots on Arabic\nTwitter and pave the way for a more effective Arabic bot detection tools.",
    "We present a novel active learning algorithm for community detection on\nnetworks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)\ncriterion for querying network nodes label assignments. MEMC detects nodes that\nmaximally change the community assignment likelihood model following a query.\nOur method is inspired by detection in the benchmark Stochastic Block Model\n(SBM), where we provide sample complexity analysis and empirical study with SBM\nand real network data for binary as well as for the multi-class settings. The\nanalysis also covers the most challenging case of sparse degree and\nbelow-detection-threshold SBMs, where we observe a super-linear error\nreduction. MEMC is shown to be superior to the random selection baseline and\nother state-of-the-art active learners.",
    "Using an intuitive concept of what constitutes a meaningful community, a\nnovel metric is formulated for detecting non-overlapping communities in\nundirected, weighted heterogeneous networks. This metric, modularity density,\nis shown to be superior to the versions of modularity density in present\nliterature. Compared to the previous versions of modularity density,\nmaximization of our metric is proven to be free from bias and better detect\nweakly-separated communities particularly in heterogeneous networks. In\naddition to these characteristics, the computational running time of our\nmodularity density is found to be on par or faster than that of the previous\nvariants. Our findings further reveal that community detection by maximization\nof our metric is mathematically related to partitioning a network by\nminimization of the normalized cut criterion.",
    "Generative models for graphs have been typically committed to strong prior\nassumptions concerning the form of the modeled distributions. Moreover, the\nvast majority of currently available models are either only suitable for\ncharacterizing some particular network properties (such as degree distribution\nor clustering coefficient), or they are aimed at estimating joint probability\ndistributions, which is often intractable in large-scale networks. In this\npaper, we first propose a novel network statistic, based on the Laplacian\nspectrum of graphs, which allows to dispense with any parametric assumption\nconcerning the modeled network properties. Second, we use the defined statistic\nto develop the Fiedler random graph model, switching the focus from the\nestimation of joint probability distributions to a more tractable conditional\nestimation setting. After analyzing the dependence structure characterizing\nFiedler random graphs, we evaluate them experimentally in edge prediction over\nseveral real-world networks, showing that they allow to reach a much higher\nprediction accuracy than various alternative statistical models.",
    "We consider the problem of signal recovery on graphs as graphs model data\nwith complex structure as signals on a graph. Graph signal recovery implies\nrecovery of one or multiple smooth graph signals from noisy, corrupted, or\nincomplete measurements. We propose a graph signal model and formulate signal\nrecovery as a corresponding optimization problem. We provide a general solution\nby using the alternating direction methods of multipliers. We next show how\nsignal inpainting, matrix completion, robust principal component analysis, and\nanomaly detection all relate to graph signal recovery, and provide\ncorresponding specific solutions and theoretical analysis. Finally, we validate\nthe proposed methods on real-world recovery problems, including online blog\nclassification, bridge condition identification, temperature estimation,\nrecommender system, and expert opinion combination of online blog\nclassification.",
    "Using different methods for laying out a graph can lead to very different\nvisual appearances, with which the viewer perceives different information.\nSelecting a \"good\" layout method is thus important for visualizing a graph. The\nselection can be highly subjective and dependent on the given task. A common\napproach to selecting a good layout is to use aesthetic criteria and visual\ninspection. However, fully calculating various layouts and their associated\naesthetic metrics is computationally expensive. In this paper, we present a\nmachine learning approach to large graph visualization based on computing the\ntopological similarity of graphs using graph kernels. For a given graph, our\napproach can show what the graph would look like in different layouts and\nestimate their corresponding aesthetic metrics. An important contribution of\nour work is the development of a new framework to design graph kernels. Our\nexperimental study shows that our estimation calculation is considerably faster\nthan computing the actual layouts and their aesthetic metrics. Also, our graph\nkernels outperform the state-of-the-art ones in both time and accuracy. In\naddition, we conducted a user study to demonstrate that the topological\nsimilarity computed with our graph kernel matches perceptual similarity\nassessed by human users.",
    "Community detection is one of the most important and interesting issues in\nsocial network analysis. In recent years, simultaneous considering of nodes'\nattributes and topological structures of social networks in the process of\ncommunity detection has attracted the attentions of many scholars, and this\nconsideration has been recently used in some community detection methods to\nincrease their efficiencies and to enhance their performances in finding\nmeaningful and relevant communities. But the problem is that most of these\nmethods tend to find non-overlapping communities, while many real-world\nnetworks include communities that often overlap to some extent. In order to\nsolve this problem, an evolutionary algorithm called MOBBO-OCD, which is based\non multi-objective biogeography-based optimization (BBO), is proposed in this\npaper to automatically find overlapping communities in a social network with\nnode attributes with synchronously considering the density of connections and\nthe similarity of nodes' attributes in the network. In MOBBO-OCD, an extended\nlocus-based adjacency representation called OLAR is introduced to encode and\ndecode overlapping communities. Based on OLAR, a rank-based migration operator\nalong with a novel two-phase mutation strategy and a new double-point crossover\nare used in the evolution process of MOBBO-OCD to effectively lead the\npopulation into the evolution path. In order to assess the performance of\nMOBBO-OCD, a new metric called alpha_SAEM is proposed in this paper, which is\nable to evaluate the goodness of both overlapping and non-overlapping\npartitions with considering the two aspects of node attributes and linkage\nstructure. Quantitative evaluations reveal that MOBBO-OCD achieves favorable\nresults which are quite superior to the results of 15 relevant community\ndetection algorithms in the literature.",
    "Natural language programming is a promising approach to enable end users to\ninstruct new tasks for intelligent agents. However, our formative study found\nthat end users would often use unclear, ambiguous or vague concepts when\nnaturally instructing tasks in natural language, especially when specifying\nconditionals. Existing systems have limited support for letting the user teach\nagents new concepts or explaining unclear concepts. In this paper, we describe\na new multi-modal domain-independent approach that combines natural language\nprogramming and programming-by-demonstration to allow users to first naturally\ndescribe tasks and associated conditions at a high level, and then collaborate\nwith the agent to recursively resolve any ambiguities or vagueness through\nconversations and demonstrations. Users can also define new procedures and\nconcepts by demonstrating and referring to contents within GUIs of existing\nmobile apps. We demonstrate this approach in PUMICE, an end-user programmable\nagent that implements this approach. A lab study with 10 users showed its\nusability.",
    "This research report presents a proof-of-concept study on the application of\nmachine learning techniques to video and speech data collected during\ndiagnostic cognitive assessments of children with a neurodevelopmental\ndisorder. The study utilised a dataset of 39 video recordings, capturing\nextensive sessions where clinicians administered, among other things, four\ncognitive assessment tests. From the first 40 minutes of each clinical session,\ncovering the administration of the Wechsler Intelligence Scale for Children\n(WISC-V), we extracted head positions and speech turns of both clinician and\nchild. Despite the limited sample size and heterogeneous recording styles, the\nanalysis successfully extracted path signatures as features from the recorded\ndata, focusing on patient-clinician interactions. Importantly, these features\nquantify the interpersonal dynamics of the assessment process (dialogue and\nmovement patterns). Results suggest that these features exhibit promising\npotential for predicting all cognitive tests scores of the entire session\nlength and for prototyping a predictive model as a clinical decision support\ntool. Overall, this proof of concept demonstrates the feasibility of leveraging\nmachine learning techniques for clinical video and speech data analysis in\norder to potentially enhance the efficiency of cognitive assessments for\nneurodevelopmental disorders in children.",
    "We explore the expression of personality and adaptivity through the gestures\nof virtual agents in a storytelling task. We conduct two experiments using four\ndifferent dialogic stories. We manipulate agent personality on the extraversion\nscale, whether the agents adapt to one another in their gestural performance\nand agent gender. Our results show that subjects are able to perceive the\nintended variation in extraversion between different virtual agents,\nindependently of the story they are telling and the gender of the agent. A\nsecond study shows that subjects also prefer adaptive to nonadaptive virtual\nagents.",
    "Human-AI collaboration has the potential to transform various domains by\nleveraging the complementary strengths of human experts and Artificial\nIntelligence (AI) systems. However, unobserved confounding can undermine the\neffectiveness of this collaboration, leading to biased and unreliable outcomes.\nIn this paper, we propose a novel solution to address unobserved confounding in\nhuman-AI collaboration by employing the marginal sensitivity model (MSM). Our\napproach combines domain expertise with AI-driven statistical modeling to\naccount for potential confounders that may otherwise remain hidden. We present\na deferral collaboration framework for incorporating the MSM into policy\nlearning from observational data, enabling the system to control for the\ninfluence of unobserved confounding factors. In addition, we propose a\npersonalized deferral collaboration system to leverage the diverse expertise of\ndifferent human decision-makers. By adjusting for potential biases, our\nproposed solution enhances the robustness and reliability of collaborative\noutcomes. The empirical and theoretical analyses demonstrate the efficacy of\nour approach in mitigating unobserved confounding and improving the overall\nperformance of human-AI collaborations.",
    "Recent work has explored how complementary strengths of humans and artificial\nintelligence (AI) systems might be productively combined. However, successful\nforms of human-AI partnership have rarely been demonstrated in real-world\nsettings. We present the iterative design and evaluation of Lumilo, smart\nglasses that help teachers help their students in AI-supported classrooms by\npresenting real-time analytics about students' learning, metacognition, and\nbehavior. Results from a field study conducted in K-12 classrooms indicate that\nstudents learn more when teachers and AI tutors work together during class. We\ndiscuss implications of this research for the design of human-AI partnerships.\nWe argue for more participatory approaches to research and design in this area,\nin which practitioners and other stakeholders are deeply, meaningfully involved\nthroughout the process. Furthermore, we advocate for theory-building and for\nprincipled approaches to the study of human-AI decision-making in real-world\ncontexts.",
    "The proliferation of text messaging for mobile health is generating a large\namount of patient-doctor conversations that can be extremely valuable to health\ncare professionals. We present ConVIScope, a visual text analytic system that\ntightly integrates interactive visualization with natural language processing\nin analyzing patient-doctor conversations. ConVIScope was developed in\ncollaboration with healthcare professionals following a user-centered iterative\ndesign. Case studies with six domain experts suggest the potential utility of\nConVIScope and reveal lessons for further developments.",
    "The rise of Artificial Intelligence (AI) and Generative Artificial\nIntelligence (GenAI) in higher education necessitates assessment reform. This\nstudy addresses a critical gap by exploring student and academic staff\nexperiences with AI and GenAI tools, focusing on their familiarity and comfort\nwith current and potential future applications in learning and assessment. An\nonline survey collected data from 35 academic staff and 282 students across two\nuniversities in Vietnam and one in Singapore, examining GenAI familiarity,\nperceptions of its use in assessment marking and feedback, knowledge checking\nand participation, and experiences of GenAI text detection.\n  Descriptive statistics and reflexive thematic analysis revealed a generally\nlow familiarity with GenAI among both groups. GenAI feedback was viewed\nnegatively; however, it was viewed more positively when combined with\ninstructor feedback. Academic staff were more accepting of GenAI text detection\ntools and grade adjustments based on detection results compared to students.\nQualitative analysis identified three themes: unclear understanding of text\ndetection tools, variability in experiences with GenAI detectors, and mixed\nfeelings about GenAI's future impact on educational assessment. These findings\nhave major implications regarding the development of policies and practices for\nGenAI-enabled assessment and feedback in higher education.",
    "From deciding on a PhD program to buying a new camera, unfamiliar\ndecisions--decisions without domain knowledge--are frequent and significant.\nThe complexity and uncertainty of such decisions demand unique approaches to\ninformation seeking, understanding, and decision-making. Our formative study\nhighlights that users want to start by discovering broad and relevant domain\ninformation evenly and simultaneously, quickly address emerging inquiries, and\ngain personalized standards to assess information found. We present\nChoiceMates, an interactive multi-agent system designed to address these needs\nby enabling users to engage with a dynamic set of LLM agents each presenting a\nunique experience in the domain. Unlike existing multi-agent systems that\nautomate tasks with agents, the user orchestrates agents to assist their\ndecision-making process. Our user evaluation (n=12) shows that ChoiceMates\nenables a more confident, satisfactory decision-making with better situation\nunderstanding than web search, and higher decision quality and confidence than\na commercial multi-agent framework. This work provides insights into designing\na more controllable and collaborative multi-agent system.",
    "Data visualization and interaction with large data sets is known to be\nessential and critical in many businesses today, and the same applies to\nresearch and teaching, in this case, when exploring large and complex\nmathematical objects. GAP is a computer algebra system for computational\ndiscrete algebra with an emphasis on computational group theory. The existing\nXGAP package for GAP works exclusively on the X Window System. It lacks\nabstraction between its mathematical and graphical cores, making it difficult\nto extend, maintain, or port. In this paper, we present Francy, a graphical\nsemantics package for GAP. Francy is responsible for creating a\nrepresentational structure that can be rendered using many GUI frameworks\nindependent from any particular programming language or operating system.\nBuilding on this, we use state of the art web technologies that take advantage\nof an improved REPL environment, which is currently under development for GAP.\nThe integration of this project with Jupyter provides a rich graphical\nenvironment full of features enhancing the usability and accessibility of GAP.",
    "The think aloud method is an important and commonly used tool for usability\noptimization. However, analyzing think aloud data could be time consuming. In\nthis paper, we put forth an automatic analysis of verbal protocols and test the\nlink between spoken feedback and the stimulus using eye tracking and mouse\ntracking. The gained data - user feedback linked to a specific area of the\nstimulus - could be used to let an expert review the feedback on specific web\npage elements or to visualize on which parts of the web page the feedback was\ngiven. Specifically, we test if participants fixate on or point with the mouse\nto the content of the webpage that they are verbalizing. During the testing,\nparticipants were shown three websites and asked to verbally give their\nopinion. The verbal responses, along with the eye and cursor movements were\nrecorded. We compared the hit rate, defined as the percentage of verbally\nmentioned areas of interest (AOIs) that were fixated with gaze or pointed to\nwith the mouse. The results revealed a significantly higher hit rate for the\ngaze compared to the mouse data. Further investigation revealed that, while the\nmouse was mostly used passively to scroll, the gaze was often directed towards\nrelevant AOIs, thus establishing a strong association between spoken words and\nstimuli. Therefore, eye tracking data possibly provides more detailed\ninformation and more valuable insights about the verbalizations compared to the\nmouse data.",
    "Game balancing is an important part of the (computer) game design process, in\nwhich designers adapt a game prototype so that the resulting gameplay is as\nentertaining as possible. In industry, the evaluation of a game is often based\non costly playtests with human players. It suggests itself to automate this\nprocess using surrogate models for the prediction of gameplay and outcome. In\nthis paper, the feasibility of automatic balancing using simulation- and\ndeck-based objectives is investigated for the card game top trumps.\nAdditionally, the necessity of a multi-objective approach is asserted by a\ncomparison with the only known (single-objective) method. We apply a\nmulti-objective evolutionary algorithm to obtain decks that optimise\nobjectives, e.g. win rate and average number of tricks, developed to express\nthe fairness and the excitement of a game of top trumps. The results are\ncompared with decks from published top trumps decks using simulation-based\nobjectives. The possibility to generate decks better or at least as good as\ndecks from published top trumps decks in terms of these objectives is\ndemonstrated. Our results indicate that automatic balancing with the presented\napproach is feasible even for more complex games such as real-time strategy\ngames.",
    "While Machine learning gives rise to astonishing results in automated\nsystems, it is usually at the cost of large data requirements. This makes many\nsuccessful algorithms from machine learning unsuitable for human-machine\ninteraction, where the machine must learn from a small number of training\nsamples that can be provided by a user within a reasonable time frame.\nFortunately, the user can tailor the training data they create to be as useful\nas possible, severely limiting its necessary size -- as long as they know about\nthe machine's requirements and limitations. Of course, acquiring this knowledge\ncan in turn be cumbersome and costly. This raises the question of how easy\nmachine learning algorithms are to interact with. In this work, we address this\nissue by analyzing the intuitiveness of certain algorithms when they are\nactively taught by users. After developing a theoretical framework of\nintuitiveness as a property of algorithms, we introduce an active teaching\nparadigm involving a prototypical two-dimensional spatial learning task as a\nmethod to judge the efficacy of human-machine interactions. Finally, we present\nand discuss the results of a large-scale user study into the performance and\nteaching strategies of 800 users interacting with two prominent machine\nlearning algorithms in our system, providing first evidence for the role of\nintuition as an important factor impacting human-machine interaction.",
    "When performing a task alone, humans achieve a certain level of performance.\nWhen humans are assisted by a tool or automation to perform the same task,\nperformance is enhanced (augmented). Recently developed cognitive systems are\nable to perform cognitive processing at or above the level of a human in some\ndomains. When humans work collaboratively with such cogs in a human/cog\nensemble, we expect augmentation of cognitive processing to be evident and\nmeasurable. This paper shows the degree of cognitive augmentation depends on\nthe nature of the information the cog contributes to the ensemble. Results of\nan experiment are reported showing conceptual information is the most effective\ntype of information resulting in increases in cognitive accuracy, cognitive\nprecision, and cognitive power.",
    "Advances in artificial intelligence and human-computer interaction will\nlikely lead to extended reality (XR) becoming pervasive. While XR can provide\nusers with interactive, engaging, and immersive experiences, non-player\ncharacters are often utilized in pre-scripted and conventional ways. This paper\nargues for using large language models (LLMs) in XR by embedding them in\navatars or as narratives to facilitate inclusion through prompt engineering and\nfine-tuning the LLMs. We argue that this inclusion will promote diversity for\nXR use. Furthermore, the versatile conversational capabilities of LLMs will\nlikely increase engagement in XR, helping XR become ubiquitous. Lastly, we\nspeculate that combining the information provided to LLM-powered spaces by\nusers and the biometric data obtained might lead to novel privacy invasions.\nWhile exploring potential privacy breaches, examining user privacy concerns and\npreferences is also essential. Therefore, despite challenges, LLM-powered XR is\na promising area with several opportunities.",
    "Short videos on social media are the dominant way young people consume\ncontent. News outlets aim to reach audiences through news reels -- short videos\nconveying news -- but struggle to translate traditional journalistic formats\ninto short, entertaining videos. To translate news into social media reels, we\nsupport journalists in reframing the narrative. In literature, narrative\nframing is a high-level structure that shapes the overall presentation of a\nstory. We identified three narrative framings for reels that adapt social media\nnorms but preserve news value, each with a different balance of information and\nentertainment. We introduce ReelFramer, a human-AI co-creative system that\nhelps journalists translate print articles into scripts and storyboards.\nReelFramer supports exploring multiple narrative framings to find one\nappropriate to the story. AI suggests foundational narrative details, including\ncharacters, plot, setting, and key information. ReelFramer also supports visual\nframing; AI suggests character and visual detail designs before generating a\nfull storyboard. Our studies show that narrative framing introduces the\nnecessary diversity to translate various articles into reels, and establishing\nfoundational details helps generate scripts that are more relevant and\ncoherent. We also discuss the benefits of using narrative framing and\nfoundational details in content retargeting.",
    "Human and AI are increasingly interacting and collaborating to accomplish\nvarious complex tasks in the context of diverse application domains (e.g.,\nhealthcare, transportation, and creative design). Two dynamic, learning\nentities (AI and human) have distinct mental model, expertise, and ability;\nsuch fundamental difference/mismatch offers opportunities for bringing new\nperspectives to achieve better results. However, this mismatch can cause\nunexpected failure and result in serious consequences. While recent research\nhas paid much attention to enhancing interpretability or explainability to\nallow machine to explain how it makes a decision for supporting humans, this\nresearch argues that there is urging the need for both human and AI should\ndevelop specific, corresponding ability to interact and collaborate with each\nother to form a human-AI team to accomplish superior results. This research\nintroduces a conceptual framework called \"Co-Learning,\" in which people can\nlearn with/from and grow with AI partners over time. We characterize three key\nconcepts of co-learning: \"mutual understanding,\" \"mutual benefits,\" and \"mutual\ngrowth\" for facilitating human-AI collaboration on complex problem solving. We\nwill present proof-of-concepts to investigate whether and how our approach can\nhelp human-AI team to understand and benefit each other, and ultimately improve\nproductivity and creativity on creative problem domains. The insights will\ncontribute to the design of Human-AI collaboration.",
    "Recently, research in human-robot interaction began to consider a robot's\ninfluence at the group level. Despite the recent growth in research\ninvestigating the effects of robots within groups of people, our overall\nunderstanding of what happens when robots are placed within groups or teams of\npeople is still limited. This paper investigates several key problems for\nsocial robots that manage conversations in a group setting, where the number of\nparticipants is more than two. In a group setting, the conversation dynamics\nare a lot more complicated than the conventional one-to-one conversation, thus,\nthere are more challenges need to be solved.",
    "Millions of people participate in online peer-to-peer support sessions, yet\nthere has been little prior research on systematic psychology-based evaluations\nof fine-grained peer-counselor behavior in relation to client satisfaction.\nThis paper seeks to bridge this gap by mapping peer-counselor chat-messages to\nmotivational interviewing (MI) techniques. We annotate 14,797 utterances from\n734 chat conversations using 17 MI techniques and introduce four new\ninterviewing codes such as chit-chat and inappropriate to account for the\nunique conversational patterns observed on online platforms. We automate the\nprocess of labeling peer-counselor responses to MI techniques by fine-tuning\nlarge domain-specific language models and then use these automated measures to\ninvestigate the behavior of the peer counselors via correlational studies.\nSpecifically, we study the impact of MI techniques on the conversation ratings\nto investigate the techniques that predict clients' satisfaction with their\ncounseling sessions. When counselors use techniques such as reflection and\naffirmation, clients are more satisfied. Examining volunteer counselors' change\nin usage of techniques suggest that counselors learn to use more introduction\nand open questions as they gain experience. This work provides a deeper\nunderstanding of the use of motivational interviewing techniques on\npeer-to-peer counselor platforms and sheds light on how to build better\ntraining programs for volunteer counselors on online platforms.",
    "Explicitly alerting users is not always an optimal intervention, especially\nwhen they are not motivated to obey. For example, in video-based learning,\nlearners who are distracted from the video would not follow an alert asking\nthem to pay attention. Inspired by the concept of Mindless Computing, we\npropose a novel intervention approach, Mindless Attractor, that leverages the\nnature of human speech communication to help learners refocus their attention\nwithout relying on their motivation. Specifically, it perturbs the voice in the\nvideo to direct their attention without consuming their conscious awareness.\nOur experiments not only confirmed the validity of the proposed approach but\nalso emphasized its advantages in combination with a machine learning-based\nsensing module. Namely, it would not frustrate users even though the\nintervention is activated by false-positive detection of their attentive state.\nOur intervention approach can be a reliable way to induce behavioral change in\nhuman-AI symbiosis.",
    "We present AceWiki, a prototype of a new kind of semantic wiki using the\ncontrolled natural language Attempto Controlled English (ACE) for representing\nits content. ACE is a subset of English with a restricted grammar and a formal\nsemantics. The use of ACE has two important advantages over existing semantic\nwikis. First, we can improve the usability and achieve a shallow learning\ncurve. Second, ACE is more expressive than the formal languages of existing\nsemantic wikis. Our evaluation shows that people who are not familiar with the\nformal foundations of the Semantic Web are able to deal with AceWiki after a\nvery short learning phase and without the help of an expert.",
    "Despite the widespread use of artificial intelligence (AI), designing user\nexperiences (UX) for AI-powered systems remains challenging. UX designers face\nhurdles understanding AI technologies, such as pre-trained language models, as\ndesign materials. This limits their ability to ideate and make decisions about\nwhether, where, and how to use AI. To address this problem, we bridge the\nliterature on AI design and AI transparency to explore whether and how\nframeworks for transparent model reporting can support design ideation with\npre-trained models. By interviewing 23 UX practitioners, we find that\npractitioners frequently work with pre-trained models, but lack support for\nUX-led ideation. Through a scenario-based design task, we identify common goals\nthat designers seek model understanding for and pinpoint their model\ntransparency information needs. Our study highlights the pivotal role that UX\ndesigners can play in Responsible AI and calls for supporting their\nunderstanding of AI limitations through model transparency and interrogation.",
    "In recent years, the CHI community has seen significant growth in research on\nHuman-Centered Responsible Artificial Intelligence. While different research\ncommunities may use different terminology to discuss similar topics, all of\nthis work is ultimately aimed at developing AI that benefits humanity while\nbeing grounded in human rights and ethics, and reducing the potential harms of\nAI. In this special interest group, we aim to bring together researchers from\nacademia and industry interested in these topics to map current and future\nresearch trends to advance this important area of research by fostering\ncollaboration and sharing ideas.",
    "Self-guided mental health interventions, such as \"do-it-yourself\" tools to\nlearn and practice coping strategies, show great promise to improve access to\nmental health care. However, these interventions are often cognitively\ndemanding and emotionally triggering, creating accessibility barriers that\nlimit their wide-scale implementation and adoption. In this paper, we study how\nhuman-language model interaction can support self-guided mental health\ninterventions. We take cognitive restructuring, an evidence-based therapeutic\ntechnique to overcome negative thinking, as a case study. In an IRB-approved\nrandomized field study on a large mental health website with 15,531\nparticipants, we design and evaluate a system that uses language models to\nsupport people through various steps of cognitive restructuring. Our findings\nreveal that our system positively impacts emotional intensity for 67% of\nparticipants and helps 65% overcome negative thoughts. Although adolescents\nreport relatively worse outcomes, we find that tailored interventions that\nsimplify language model generations improve overall effectiveness and equity.",
    "Given the rapid advance in ITS technologies, future mobility is pointing to\nvehicular autonomy. However, there is still a long way before full automation,\nand human intervention is required. This work sheds light on understanding\nhuman drivers' intervention behavior involved in the operation of autonomous\nvehicles (AVs) and utilizes this knowledge to improve the perception of\ncritical driving scenarios. Experiment environments were implemented where the\nvirtual reality (VR) and traffic micro-simulation are integrated, and tests\nwere carried out under typical and diverse traffic scenes. Performance\nindicators such as the probability of intervention, accident rates are defined\nand used to quantify and compare the risk levels. By offering novel insights\ninto drivers' intervention behavior, this work will help improve the\nperformances of the automated control under similar scenarios. Furthermore,\nsuch an integrated and immersive tool for autonomous driving studies will be\nvaluable for research on human-to-automation trust. To the best knowledge of\nthe authors, this work is among the pioneer works making efforts into such\ntypes of tools.",
    "This paper explores the links between Knowledge Management and new\ncommunity-based models of the organization from both a theoretical and an\nempirical perspective. From a theoretical standpoint, we look at Communities of\nPractice (CoPs) and Knowledge Management (KM) and explore the links between the\ntwo as they relate to the use of information systems to manage knowledge. We\nbegin by reviewing technologically supported approaches to KM and introduce the\nidea of \"Systemes d'Aide a la Gestion des Connaissances\" SAGC (Systems to aid\nthe Management of Knowledge). Following this we examine the contribution that\ncommunal structures such as CoPs can make to intraorganizational KM and\nhighlight some of 'success factors' for this approach to KM that are found in\nthe literature. From an empirical standpoint, we present the results of a\nsurvey involving the Chief Knowledge Officers (CKOs) of twelve large French\nbusinesses; the objective of this study was to identify the factors that might\ninfluence the success of such approaches. The survey was analysed using\nthematic content analysis and the results are presented here with some short\nillustrative quotes from the CKOs. Finally, the paper concludes with some brief\nreflections on what can be learnt from looking at this problem from these two\nperspectives.",
    "Graphical User Interface (GUI) agents are designed to automate complex tasks\non digital devices, such as smartphones and desktops. Most existing GUI agents\ninteract with the environment through extracted structured data, which can be\nnotably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).\nTo alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which\nonly relies on screenshots for task automation. In our preliminary study, we\nhave discovered a key challenge in developing visual GUI agents: GUI grounding\n-- the capacity to accurately locate screen elements based on instructions. To\ntackle this challenge, we propose to enhance SeeClick with GUI grounding\npre-training and devise a method to automate the curation of GUI grounding\ndata. Along with the efforts above, we have also created ScreenSpot, the first\nrealistic GUI grounding benchmark that encompasses mobile, desktop, and web\nenvironments. After pre-training, SeeClick demonstrates significant improvement\nin ScreenSpot over various baselines. Moreover, comprehensive evaluations on\nthree widely used benchmarks consistently support our finding that advancements\nin GUI grounding directly correlate with enhanced performance in downstream GUI\nagent tasks. The model, data and code are available at\nhttps://github.com/njucckevin/SeeClick.",
    "This whitepaper reports on Project CLAI (Command Line AI), which aims to\nbring the power of AI to the command line interface (CLI). The CLAI platform\nsets up the CLI as a new environment for AI researchers to conquer by surfacing\nthe command line as a generic environment that researchers can interface to\nusing a simple sense-act API, much like the traditional AI agent architecture.\nIn this paper, we discuss the design and implementation of the platform in\ndetail, through illustrative use cases of new end user interaction patterns\nenabled by this design, and through quantitative evaluation of the system\nfootprint of a CLAI-enabled terminal. We also report on some early user\nfeedback on CLAI's features from an internal survey.",
    "Human-swarm interaction (HSI) is an active research challenge in the realms\nof swarm robotics and human-factors engineering. Here we apply a cognitive\nsystems engineering perspective and introduce a neuro-inspired joint systems\ntheory of HSI. The mindset defines predictions for adaptive, robust and\nscalable HSI dynamics and therefore has the potential to inform human-swarm\nloop design.",
    "Exploration has been one of the greatest challenges in reinforcement learning\n(RL), which is a large obstacle in the application of RL to robotics. Even with\nstate-of-the-art RL algorithms, building a well-learned agent often requires\ntoo many trials, mainly due to the difficulty of matching its actions with\nrewards in the distant future. A remedy for this is to train an agent with\nreal-time feedback from a human observer who immediately gives rewards for some\nactions. This study tackles a series of challenges for introducing such a\nhuman-in-the-loop RL scheme. The first contribution of this work is our\nexperiments with a precisely modeled human observer: binary, delay,\nstochasticity, unsustainability, and natural reaction. We also propose an RL\nmethod called DQN-TAMER, which efficiently uses both human feedback and distant\nrewards. We find that DQN-TAMER agents outperform their baselines in Maze and\nTaxi simulated environments. Furthermore, we demonstrate a real-world\nhuman-in-the-loop RL application where a camera automatically recognizes a\nuser's facial expressions as feedback to the agent while the agent explores a\nmaze.",
    "Brain-Computer interfaces (BCI) are widely used in reading brain signals and\nconverting them into real-world motion. However, the signals produced from the\nBCI are noisy and hard to analyze. This paper looks specifically towards\ncombining the BCI's latest technology with ultrasonic sensors to provide a\nhands-free wheelchair that can efficiently navigate through crowded\nenvironments. This combination provides safety and obstacle avoidance features\nnecessary for the BCI Navigation system to gain more confidence and operate the\nwheelchair at a relatively higher velocity. A population of six human subjects\ntested the BCI-controller and obstacle avoidance features. Subjects were able\nto mentally control the destination of the wheelchair, by moving the target\nfrom the starting position to a predefined position, in an average of 287.12\nseconds and a standard deviation of 48.63 seconds after 10 minutes of training.\nThe wheelchair successfully avoided all obstacles placed by the subjects during\nthe test.",
    "The smart textile and wearables sector is looking towards advancing\ntechnologies to meet both industry, consumer and new emerging innovative\ntextile application demands, within a fast paced textile industry. In parallel\ninspiration based on the biological neural workings of the human brain is\ndriving the next generation of artificial intelligence. Artificial intelligence\ninspired hardware (neuromorphic computing) and software modules mimicking the\nprocessing capabilities and properties of neural networks and the human nervous\nsystem are taking shape. The textile sector needs to actively look at such\nemerging and new technologies taking inspiration from their workings and\nprocessing methods in order to stimulate new and innovative embedded\nintelligence advancements in the etextile world. This emerging next generation\nof Artificial intelligence(AI) is rapidly gaining interest across varying\nindustries (textile, medical, automotive, aerospace, military). How such\nproperties can inspire and drive advancements within the etextiles sector needs\nto be considered. This paper will provide an insight into current\nnanotechnology and artificial intelligence advancements in the etextiles domain\nbefore focusing specifically on the future vision and direction around the\npotential application of neuromorphic computing and spiking neural network\ninspired AI technologies within the textile sector. We investigate the core\narchitectural elements of artificial neural networks, neuromorphic computing\nand how such neuroscience inspired technologies could impact and inspire change\nand new research developments within the e-textile sector.",
    "Large Language Models (LLMs) are notorious for blending fact with fiction and\ngenerating non-factual content, known as hallucinations. To address this\nchallenge, we propose an interactive system that helps users gain insight into\nthe reliability of the generated text. Our approach is based on the idea that\nthe self-consistency of multiple samples generated by the same LLM relates to\nits confidence in individual claims in the generated texts. Using this idea, we\ndesign RELIC, an interactive system that enables users to investigate and\nverify semantic-level variations in multiple long-form responses. This allows\nusers to recognize potentially inaccurate information in the generated text and\nmake necessary corrections. From a user study with ten participants, we\ndemonstrate that our approach helps users better verify the reliability of the\ngenerated text. We further summarize the design implications and lessons\nlearned from this research for future studies of reliable human-LLM\ninteractions.",
    "This text analyses the papers accepted for the workshop \"Reuse of designs: an\ninterdisciplinary cognitive approach\". Several dimensions and questions\nconsidered as important (by the authors and/or by us) are addressed: What about\nthe \"interdisciplinary cognitive\" character of the approaches adopted by the\nauthors? Is design indeed a domain where the use of CBR is particularly\nsuitable? Are there important distinctions between CBR and other approaches?\nWhich types of knowledge -other than cases- is being, or might be, used in CBR\nsystems? With respect to cases: are there different \"types\" of case and\ndifferent types of case use? which formats are adopted for their\nrepresentation? do cases have \"components\"? how are cases organised in the case\nmemory? Concerning their retrieval: which types of index are used? on which\ntypes of relation is retrieval based? how does one retrieve only a selected\nnumber of cases, i.e., how does one retrieve only the \"best\" cases? which\nprocesses and strategies are used, by the system and by its user? Finally, some\nimportant aspects of CBR system development are shortly discussed: should CBR\nsystems be assistance or autonomous systems? how can case knowledge be\n\"acquired\"? what about the empirical evaluation of CBR systems? The conclusion\npoints out some lacking points: not much attention is paid to the user, and few\npapers have indeed adopted an interdisciplinary cognitive approach.",
    "SimDialog is a visual editor for dialog in computer games. This paper\npresents the design of SimDialog, illustrating how script writers and\nnon-programmers can easily create dialog for video games with complex branching\nstructures and dynamic response characteristics. The system creates dialog as a\ndirected graph. This allows for play using the dialog with a state-based cause\nand effect system that controls selection of non-player character responses and\ncan provide a basic scoring mechanism for games.",
    "Large language models (LLMs) have facilitated significant strides in\ngenerating conversational agents, enabling seamless, contextually relevant\ndialogues across diverse topics. However, the existing LLM-driven\nconversational agents have fixed personalities and functionalities, limiting\ntheir adaptability to individual user needs. Creating personalized agent\npersonas with distinct expertise or traits can address this issue. Nonetheless,\nwe lack knowledge of how people customize and interact with agent personas. In\nthis research, we investigated how users customize agent personas and their\nimpact on interaction quality, diversity, and dynamics. To this end, we\ndeveloped CloChat, an interface supporting easy and accurate customization of\nagent personas in LLMs. We conducted a study comparing how participants\ninteract with CloChat and ChatGPT. The results indicate that participants\nformed emotional bonds with the customized agents, engaged in more dynamic\ndialogues, and showed interest in sustaining interactions. These findings\ncontribute to design implications for future systems with conversational agents\nusing LLMs.",
    "We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.",
    "One of the most crucial issues in data mining is to model human behaviour in\norder to provide personalisation, adaptation and recommendation. This usually\ninvolves implicit or explicit knowledge, either by observing user interactions,\nor by asking users directly. But these sources of information are always\nsubject to the volatility of human decisions, making utilised data uncertain to\na particular extent. In this contribution, we elaborate on the impact of this\nhuman uncertainty when it comes to comparative assessments of different data\nmining approaches. In particular, we reveal two problems: (1) biasing effects\non various metrics of model-based prediction and (2) the propagation of\nuncertainty and its thus induced error probabilities for algorithm rankings.\nFor this purpose, we introduce a probabilistic view and prove the existence of\nthose problems mathematically, as well as provide possible solution strategies.\nWe exemplify our theory mainly in the context of recommender systems along with\nthe metric RMSE as a prominent example of precision quality measures.",
    "Model explanations such as saliency maps can improve user trust in AI by\nhighlighting important features for a prediction. However, these become\ndistorted and misleading when explaining predictions of images that are subject\nto systematic error (bias). Furthermore, the distortions persist despite model\nfine-tuning on images biased by different factors (blur, color temperature,\nday/night). We present Debiased-CAM to recover explanation faithfulness across\nvarious bias types and levels by training a multi-input, multi-task model with\nauxiliary tasks for explanation and bias level predictions. In simulation\nstudies, the approach not only enhanced prediction accuracy, but also generated\nhighly faithful explanations about these predictions as if the images were\nunbiased. In user studies, debiased explanations improved user task\nperformance, perceived truthfulness and perceived helpfulness. Debiased\ntraining can provide a versatile platform for robust performance and\nexplanation faithfulness for a wide range of applications with data biases.",
    "There is increasing interest in the adoption of LLMs in HCI research.\nHowever, LLMs may often be regarded as a panacea because of their powerful\ncapabilities with an accompanying oversight on whether they are suitable for\ntheir intended tasks. We contend that LLMs should be adopted in a critical\nmanner following rigorous evaluation. Accordingly, we present the evaluation of\nan LLM in identifying logical fallacies that will form part of a digital\nmisinformation intervention. By comparing to a labeled dataset, we found that\nGPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes\ninvalid or unidentified instances, an accuracy of 0.90. This gives us the\nconfidence to proceed with the application of the LLM while keeping in mind the\nareas where it still falls short. The paper describes our evaluation approach,\nresults and reflections on the use of the LLM for our intended task.",
    "Recent advancements in artificial intelligence (AI) and its sub-branch\nmachine learning (ML) promise machines that go beyond the boundaries of\nautomation and behave autonomously. Applications of these machines in creative\npractices such as art and design entail relationships between users and\nmachines that have been described as a form of collaboration or co-creation\nbetween computational and human agents. This paper uses examples from art and\ndesign to argue that this frame is incomplete as it fails to acknowledge the\nsocio-technical nature of AI systems, and the different human agencies involved\nin their design, implementation, and operation. Situating applications of\nAI-enabled tools in creative practices in a spectrum between automation and\nautonomy, this paper distinguishes different kinds of human engagement elicited\nby systems deemed automated or autonomous. Reviewing models of artistic\ncollaboration during the late 20th century, it suggests that collaboration is\nat the core of these artistic practices. We build upon the growing literature\nof machine learning and art to look for the human agencies inscribed in works\nof computational creativity, and expand the co-creation frame to incorporate\nemerging forms of human-human collaboration mediated through technical\nartifacts such as algorithms and data.",
    "We present results from a set of experiments in this pilot study to\ninvestigate the causal influence of user activity on various environmental\nparameters monitored by occupant carried multi-purpose sensors. Hypotheses with\nrespect to each type of measurements are verified, including temperature,\nhumidity, and light level collected during eight typical activities: sitting in\nlab / cubicle, indoor walking / running, resting after physical activity,\nclimbing stairs, taking elevators, and outdoor walking. Our main contribution\nis the development of features for activity and location recognition based on\nenvironmental measurements, which exploit location- and activity-specific\ncharacteristics and capture the trends resulted from the underlying\nphysiological process. The features are statistically shown to have good\nseparability and are also information-rich. Fusing environmental sensing\ntogether with acceleration is shown to achieve classification accuracy as high\nas 99.13%. For building applications, this study motivates a sensor fusion\nparadigm for learning individualized activity, location, and environmental\npreferences for energy management and user comfort.",
    "Interaction is a fundamental part of using any computer system but it is\nstill an issue for people with special needs. In order to improve this\nsituation, this paper describes a new device-interaction model based on\nadaptation rules for user models. The aim is the adaptation at the interaction\nlevel, taking into account the interaction device features in order to improve\nthe usability through the user experience in the education sector. In the\nevaluation process, several students from a special education center have\nparticipated. These students have either a physical or sensory disability or\nautism. The results are promising enough to consider that this model will be\nable to help students with disabilities to interact with a computer system\nwhich will inevitably provide tremendous benefits to their academic and\npersonal development.",
    "When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.",
    "We propose a text editor to help users plan, structure and reflect on their\nwriting process. It provides continuously updated paragraph-wise summaries as\nmargin annotations, using automatic text summarization. Summary levels range\nfrom full text, to selected (central) sentences, down to a collection of\nkeywords. To understand how users interact with this system during writing, we\nconducted two user studies (N=4 and N=8) in which people wrote analytic essays\nabout a given topic and article. As a key finding, the summaries gave users an\nexternal perspective on their writing and helped them to revise the content and\nscope of their drafted paragraphs. People further used the tool to quickly gain\nan overview of the text and developed strategies to integrate insights from the\nautomated summaries. More broadly, this work explores and highlights the value\nof designing AI tools for writers, with Natural Language Processing (NLP)\ncapabilities that go beyond direct text generation and correction.",
    "Image generation using generative AI is rapidly becoming a major new source\nof visual media, with billions of AI generated images created using diffusion\nmodels such as Stable Diffusion and Midjourney over the last few years. In this\npaper we collect and analyse over 3 million prompts and the images they\ngenerate. Using natural language processing, topic analysis and visualisation\nmethods we aim to understand collectively how people are using text prompts,\nthe impact of these systems on artists, and more broadly on the visual cultures\nthey promote. Our study shows that prompting focuses largely on surface\naesthetics, reinforcing cultural norms, popular conventional representations\nand imagery. We also find that many users focus on popular topics (such as\nmaking colouring books, fantasy art, or Christmas cards), suggesting that the\ndominant use for the systems analysed is recreational rather than artistic.",
    "NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.",
    "As we become increasingly entangled with digital technologies, the boundary\nbetween human and machine is progressively blurring. Adopting a performative,\nposthumanist perspective resolves this ambiguity by proposing that such\nboundaries are not predetermined, rather they are enacted within a certain\nmaterial configuration. Using this approach, dubbed `Entanglement HCI', this\npaper presents \\emph{Message Ritual} -- a novel, integrated AI system that\nencourages the re-framing of memory through machine generated poetics. Embodied\nwithin a domestic table lamp, the system listens in on conversations occurring\nwithin the home, drawing out key topics and phrases of the day and\nreconstituting them through machine generated poetry, delivered to household\nmembers via SMS upon waking each morning. Participants across four households\nwere asked to live with the lamp over a two week period. We present a\ndiffractive analysis exploring how the lamp \\emph{becomes with} participants\nand discuss the implications of this method for future HCI research.",
    "HCI researchers' and practitioners' awareness of intersectionality has been\nexpanding, producing knowledge, recommendations, and prototypes for supporting\nintersectional populations. However, doing intersectional HCI work is uniquely\nexpensive: it leads to a combinatorial explosion of empirical work (expense 1),\nand little of the work on one intersectional population can be leveraged to\nserve another (expense 2). In this paper, we explain how representations\nemployed by certain analytical design methods correspond to type abstractions,\nand use that correspondence to identify a (de)compositional model in which a\npopulation's diverse identity properties can be joined and split. We formally\nprove the model's correctness, and show how it enables HCI designers to harness\nexisting analytical HCI methods for use on new intersectional populations of\ninterest. We illustrate through four design use-cases, how the model can reduce\nthe amount of expense 1 and enable designers to leverage prior work to new\nintersectional populations, addressing expense 2.",
    "We present an algorithm for effectively generating binary sequences which\nwould be rated by people as highly likely to have been generated by a random\nprocess, such as flipping a fair coin.",
    "Steady-State Visual Evoked Potential (SSVEP) spellers are a promising\ncommunication tool for individuals with disabilities. This Brain-Computer\nInterface utilizes scalp potential data from (electroencephalography) EEG\nelectrodes on a subject's head to decode specific letters or arbitrary targets\nthe subject is looking at on a screen. However, deep neural networks for SSVEP\nspellers often suffer from low accuracy and poor generalizability to unseen\nsubjects, largely due to the high variability in EEG data. In this study, we\npropose a hybrid approach combining data augmentation and language modeling to\nenhance the performance of SSVEP spellers. Using the Benchmark dataset from\nTsinghua University, we explore various data augmentation techniques, including\nfrequency masking, time masking, and noise injection, to improve the robustness\nof deep learning models. Additionally, we integrate a language model (CharRNN)\nwith EEGNet to incorporate linguistic context, significantly enhancing\nword-level decoding accuracy. Our results demonstrate accuracy improvements of\nup to 2.9 percent over the baseline, with time masking and language modeling\nshowing the most promise. This work paves the way for more accurate and\ngeneralizable SSVEP speller systems, offering improved communication solutions\nfor individuals with disabilities.",
    "Deep learning is one of the fastest growing technologies in computer science\nwith a plethora of applications. But this unprecedented growth has so far been\nlimited to the consumption of deep learning experts. The primary challenge\nbeing a steep learning curve for learning the programming libraries and the\nlack of intuitive systems enabling non-experts to consume deep learning.\nTowards this goal, we study the effectiveness of a no-code paradigm for\ndesigning deep learning models. Particularly, a visual drag-and-drop interface\nis found more efficient when compared with the traditional programming and\nalternative visual programming paradigms. We conduct user studies of different\nexpertise levels to measure the entry level barrier and the developer load\nacross different programming paradigms. We obtain a System Usability Scale\n(SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed\nvisual programming compared to 68 and 52, respectively, for the traditional\nprogramming methods.",
    "Distance teaching has become popular these years because of the COVID-19\nepidemic. However, both students and teachers face several challenges in\ndistance teaching, like being easy to distract. We proposed Focus+, a system\ndesigned to detect learners' status with the latest AI technology from their\nweb camera to solve such challenges. By doing so, teachers can know students'\nstatus, and students can regulate their learning experience. In this research,\nwe will discuss the expected model's design for training and evaluating the AI\ndetection model of Focus+.",
    "News archives are an invaluable primary source for placing current events in\nhistorical context. But current search engine tools do a poor job at uncovering\nbroad themes and narratives across documents. We present Rookie: a practical\nsoftware system which uses natural language processing (NLP) to help readers,\nreporters and editors uncover broad stories in news archives. Unlike prior\nwork, Rookie's design emerged from 18 months of iterative development in\nconsultation with editors and computational journalists. This process lead to a\ndramatically different approach from previous academic systems with similar\ngoals. Our efforts offer a generalizable case study for others building\nreal-world journalism software using NLP.",
    "As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process. A set of\nexperiments are presented to show how this methodology can be put in practice.",
    "The paper presents a novel model-based method for intelligent tutoring, with\nparticular emphasis on the problem of selecting teaching interventions in\ninteraction with humans. Whereas previous work has focused on either\npersonalization of teaching or optimization of teaching intervention sequences,\nthe proposed individualized model-based planning approach represents\nconvergence of these two lines of research. Model-based planning picks the best\ninterventions via interactive learning of a user memory model's parameters. The\napproach is novel in its use of a cognitive model that can account for several\nkey individual- and material-specific characteristics related to\nrecall/forgetting, along with a planning technique that considers users'\npractice schedules. Taking a rule-based approach as a baseline, the authors\nevaluated the method's benefits in a controlled study of artificial teaching in\nsecond-language vocabulary learning (N=53).",
    "Large-scale labeled dataset is the indispensable fuel that ignites the AI\nrevolution as we see today. Most such datasets are constructed using\ncrowdsourcing services such as Amazon Mechanical Turk which provides noisy\nlabels from non-experts at a fair price. The sheer size of such datasets\nmandates that it is only feasible to collect a few labels per data point. We\nformulate the problem of test-time label aggregation as a statistical\nestimation problem of inferring the expected voting score. By imitating workers\nwith supervised learners and using them in a doubly robust estimation\nframework, we prove that the variance of estimation can be substantially\nreduced, even if the learner is a poor approximation. Synthetic and real-world\nexperiments show that by combining the doubly robust approach with adaptive\nworker/item selection rules, we often need much lower label cost to achieve\nnearly the same accuracy as in the ideal world where all workers label all data\npoints.",
    "We propose a statistical procedure to characterize and extract features from\na waveform that can be applied as a pre-processing signal stage in a pattern\nrecognition task using Artificial Neural Networks. Such a procedure is based on\nmeasuring a 30-parameters set of moments and cumulants from the waveform, its\nderivative, and its integral. The technique is presented as an extension of the\nStatistical Signal Characterization method existing in the literature.\n  As a testing methodology, we used the procedure to distinguish a pulse-like\nsignal from different versions of itself with frequency spectrum alterations or\ndeformations. The recognition task was performed by single feed-forward\nback-propagation networks trained for the case Sinc-, Gaussian-, and\nChirp-pulse waveform. Because of the success obtained in these examples, we can\nconclude that the proposed extended statistical signal characterization method\nis an effective tool for pattern-recognition applications. In particular, we\ncan use it as a fast pre-processing stage in embedded systems with limited\nmemory or computational capability.",
    "Channel charting (CC) has been proposed recently to enable logical\npositioning of user equipments (UEs) in the neighborhood of a multi-antenna\nbase-station solely from channel-state information (CSI). CC relies on\ndimensionality reduction of high-dimensional CSI features in order to construct\na channel chart that captures spatial and radio geometries so that UEs close in\nspace are close in the channel chart. In this paper, we demonstrate that\nautoencoder (AE)-based CC can be augmented with side information that is\nobtained during the CSI acquisition process. More specifically, we propose to\ninclude pairwise representation constraints into AEs with the goal of improving\nthe quality of the learned channel charts. We show that such\nrepresentation-constrained AEs recover the global geometry of the learned\nchannel charts, which enables CC to perform approximate positioning without\nglobal navigation satellite systems or supervised learning methods that rely on\nextensive and expensive measurement campaigns.",
    "The notion of graph filters can be used to define generative models for graph\ndata. In fact, the data obtained from many examples of network dynamics may be\nviewed as the output of a graph filter. With this interpretation, classical\nsignal processing tools such as frequency analysis have been successfully\napplied with analogous interpretation to graph data, generating new insights\nfor data science. What follows is a user guide on a specific class of graph\ndata, where the generating graph filters are low-pass, i.e., the filter\nattenuates contents in the higher graph frequencies while retaining contents in\nthe lower frequencies. Our choice is motivated by the prevalence of low-pass\nmodels in application domains such as social networks, financial markets, and\npower systems. We illustrate how to leverage properties of low-pass graph\nfilters to learn the graph topology or identify its community structure;\nefficiently represent graph data through sampling, recover missing\nmeasurements, and de-noise graph data; the low-pass property is also used as\nthe baseline to detect anomalies.",
    "Electroencephalograms (EEG) are often contaminated by artifacts which make\ninterpreting them more challenging for clinicians. Hence, automated artifact\nrecognition systems have the potential to aid the clinical workflow. In this\nabstract, we share the first results on applying various machine learning\nalgorithms to the recently released world's largest open-source artifact\nrecognition dataset. We envision that these results will serve as a benchmark\nfor researchers who might work with this dataset in future.",
    "In the present study, six meta-heuristic schemes are hybridized with\nartificial neural network (ANN), adaptive neuro-fuzzy interface system (ANFIS),\nand support vector machine (SVM), to predict monthly groundwater level (GWL),\nevaluate uncertainty analysis of predictions and spatial variation analysis.\nThe six schemes, including grasshopper optimization algorithm (GOA), cat swarm\noptimization (CSO), weed algorithm (WA), genetic algorithm (GA), krill\nalgorithm (KA), and particle swarm optimization (PSO), were used to hybridize\nfor improving the performance of ANN, SVM, and ANFIS models. Groundwater level\n(GWL) data of Ardebil plain (Iran) for a period of 144 months were selected to\nevaluate the hybrid models. The pre-processing technique of principal component\nanalysis (PCA) was applied to reduce input combinations from monthly time\nseries up to 12-month prediction intervals. The results showed that the\nANFIS-GOA was superior to the other hybrid models for predicting GWL in the\nfirst piezometer and third piezometer in the testing stage. The performance of\nhybrid models with optimization algorithms was far better than that of\nclassical ANN, ANFIS, and SVM models without hybridization. The percent of\nimprovements in the ANFIS-GOA versus standalone ANFIS in piezometer 10 were\n14.4%, 3%, 17.8%, and 181% for RMSE, MAE, NSE, and PBIAS in the training stage\nand 40.7%, 55%, 25%, and 132% in testing stage, respectively. The improvements\nfor piezometer 6 in train step were 15%, 4%, 13%, and 208% and in the test step\nwere 33%, 44.6%, 16.3%, and 173%, respectively, that clearly confirm the\nsuperiority of developed hybridization schemes in GWL modeling. Uncertainty\nanalysis showed that ANFIS-GOA and SVM had, respectively, the best and worst\nperformances among other models. In general, GOA enhanced the accuracy of the\nANFIS, ANN, and SVM models.",
    "In this paper we propose a one-dimensional convolutional neural network\n(CNN)-based state of charge estimation algorithm for electric vehicles. The CNN\nis trained using two publicly available battery datasets. The influence of\ndifferent types of noises on the estimation capabilities of the CNN model has\nbeen studied. Moreover, a transfer learning mechanism is proposed in order to\nmake the developed algorithm generalize better and estimate with an acceptable\naccuracy when a battery with different chemical characteristics than the one\nused for training the model, is used. It has been observed that using transfer\nlearning, the model can learn sufficiently well with significantly less amount\nof battery data. The proposed method fares well in terms of estimation\naccuracy, learning speed and generalization capability.",
    "Emotion recognition based on EEG (electroencephalography) has been widely\nused in human-computer interaction, distance education and health care.\nHowever, the conventional methods ignore the adjacent and symmetrical\ncharacteristics of EEG signals, which also contain salient information related\nto emotion. In this paper, a spatial folding ensemble network (SFE-Net) is\npresented for EEG feature extraction and emotion recognition. Firstly, for the\nundetected area between EEG electrodes, an improved Bicubic-EEG interpolation\nalgorithm is developed for EEG channels information completion, which allows us\nto extract a wider range of adjacent space features. Then, motivated by the\nspatial symmetric mechanism of human brain, we fold the input EEG channels data\nwith five different symmetrical strategies, which enable the proposed network\nto extract the information of space features of EEG signals more effectively.\nFinally, a 3DCNN-based spatial, temporal extraction, and a multi-voting\nstrategy of ensemble learning are integrated to model a new neural network.\nWith this network, the spatial features of different symmetric folding signals\ncan be extracted simultaneously, which greatly improves the robustness and\naccuracy of emotion recognition. The experimental results on DEAP and SEED\ndatasets show that the proposed algorithm has comparable performance in terms\nof recognition accuracy.",
    "With millimeter wave wireless communications, the resulting radiation\nreflects on most visible objects, creating rich multipath environments, namely\nin urban scenarios. The radiation captured by a listening device is thus shaped\nby the obstacles encountered, which carry latent information regarding their\nrelative positions. In this paper, a system to convert the received millimeter\nwave radiation into the device's position is proposed, making use of the\naforementioned hidden information. Using deep learning techniques and a\npre-established codebook of beamforming patterns transmitted by a base station,\nthe simulations show that average estimation errors below 10 meters are\nachievable in realistic outdoors scenarios that contain mostly\nnon-line-of-sight positions, paving the way for new positioning systems.",
    "For Device-to-device (D2D) communication of Internet-of-Things (IoT) enabled\n5G system, there is a limit to allocating resources considering a complicated\ninterference between different links in a centralized manner. If D2D link is\ncontrolled by an enhanced node base station (eNB), and thus, remains a burden\non the eNB and it causes delayed latency. This paper proposes a fully\nautonomous power allocation method for IoT-D2D communication underlaying\ncellular networks using deep learning. In the proposed scheme, an IoT-D2D\ntransmitter decides the transmit power independently from an eNB and other\nIoT-D2D devices. In addition, the power set can be nearly optimized by deep\nlearning with distributed manner to achieve higher cell throughput. We present\na distributed deep learning architecture in which the devices are trained as a\ngroup but operate independently. The deep learning can attain near optimal cell\nthroughput while suppressing interference to eNB.",
    "Deep learning methods achieve great success in many areas due to their\npowerful feature extraction capabilities and end-to-end training mechanism, and\nrecently they are also introduced for radio signal modulation classification.\nIn this paper, we propose a novel deep learning framework called SigNet, where\na signal-to-matrix (S2M) operator is adopted to convert the original signal\ninto a square matrix first and is co-trained with a follow-up CNN architecture\nfor classification. This model is further accelerated by integrating 1D\nconvolution operators, leading to the upgraded model SigNet2.0. The simulations\non two signal datasets show that both SigNet and SigNet2.0 outperform a number\nof well-known baselines. More interestingly, our proposed models behave\nextremely well in small-sample learning when only a small training dataset is\nprovided. They can achieve a relatively high accuracy even when 1\\% training\ndata are kept, while other baseline models may lose their effectiveness much\nmore quickly as the datasets get smaller. Such result suggests that\nSigNet/SigNet2.0 could be extremely useful in the situations where labeled\nsignal data are difficult to obtain. The visualization of the output features\nof our models demonstrates that our model can well divide different modulation\ntypes of signals in the feature hyper-space.",
    "Most of the Brain-Computer Interface (BCI) publications, which propose\nartificial neural networks for Motor Imagery (MI) Electroencephalography (EEG)\nsignal classification, are presented using one of the BCI Competition datasets.\nHowever, these databases contain MI EEG data from less than or equal to 10\nsubjects . In addition, these algorithms usually include only bandpass\nfiltering to reduce noise and increase signal quality. In this article, we\ncompared 5 well-known neural networks (Shallow ConvNet, Deep ConvNet, EEGNet,\nEEGNet Fusion, MI-EEGNet) using open-access databases with many subjects next\nto the BCI Competition 4 2a dataset to acquire statistically significant\nresults. We removed artifacts from the EEG using the FASTER algorithm as a\nsignal processing step. Moreover, we investigated whether transfer learning can\nfurther improve the classification results on artifact filtered data. We aimed\nto rank the neural networks; therefore, next to the classification accuracy, we\nintroduced two additional metrics: the accuracy improvement from chance level\nand the effect of transfer learning. The former can be used with different\nclass-numbered databases, while the latter can highlight neural networks with\nsufficient generalization abilities. Our metrics showed that the researchers\nshould not avoid Shallow ConvNet and Deep ConvNet because they can perform\nbetter than the later published ones from the EEGNet family.",
    "The objective of this paper is to provide a temporal dynamic model for\nresting state functional Magnetic Resonance Imaging (fMRI) trajectory to\npredict future brain images based on the given sequence. To this end, we came\nup with the model that takes advantage of representation learning and Neural\nOrdinary Differential Equation (Neural ODE) to compress the fMRI image data\ninto latent representation and learn to predict the trajectory following\ndifferential equation. Latent space was analyzed by Gaussian Mixture Model. The\nlearned fMRI trajectory embedding can be used to explain the variance of the\ntrajectory and predict human traits for each subject. This method achieves\naverage 0.5 spatial correlation for the whole predicted trajectory, and provide\ntrained ODE parameter for further analysis.",
    "This paper explores the potential of conversion-based neuromorphic algorithms\nfor highly accurate and energy-efficient single-snapshot multidimensional\nharmonic retrieval (MHR). By casting the MHR problem as a sparse recovery\nproblem, we devise the currently proposed, deep-unrolling-based Structured\nLearned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it\nefficiently using complex-valued convolutional neural networks with\ncomplex-valued activations, which are trained using a supervised regression\nobjective. Afterward, a novel method for converting the complex-valued\nconvolutional layers and activations into spiking neural networks (SNNs) is\ndeveloped. At the heart of this method lies the recently proposed Few Spikes\n(FS) conversion, which is extended by modifying the neuron model's parameters\nand internal dynamics to account for the inherent coupling between real and\nimaginary parts in complex-valued computations. Finally, the converted SNNs are\nmapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of\nestimation accuracy and power efficiency between the original CNNs deployed on\nan NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement\nresults show that the converted SNNs achieve almost five-fold power efficiency\nat moderate performance loss compared to the original CNNs.",
    "Ubiquitous use of lithium-ion batteries across multiple industries presents\nan opportunity to explore cost saving initiatives as the price to performance\nratio continually decreases in a competitive environment. Manufacturers using\nlithium-ion batteries ranging in applications from mobile phones to electric\nvehicles need to know how long batteries will last for a given service life. To\nunderstand this, expensive testing is required.\n  This paper utilizes the data and methods implemented by Kristen A. Severson,\net al, to explore the methodologies that the research team used and presents\nanother method to compare predicted results vs. actual test data for battery\ncapacity fade. The fundamental effort is to find out if machine learning\ntechniques may be trained to use early life cycle data in order to accurately\npredict battery capacity over the battery life cycle. Results show comparison\nof methods between Gaussian Process Regression (GPR) and Elastic Net Regression\n(ENR) and highlight key data features used from the extensive dataset found in\nthe work of Severson, et al.",
    "Social distancing and temperature screening have been widely employed to\ncounteract the COVID-19 pandemic, sparking great interest from academia,\nindustry and public administrations worldwide. While most solutions have dealt\nwith these aspects separately, their combination would greatly benefit the\ncontinuous monitoring of public spaces and help trigger effective\ncountermeasures. This work presents milliTRACE-IR, a joint mmWave radar and\ninfrared imaging sensing system performing unobtrusive and privacy preserving\nhuman body temperature screening and contact tracing in indoor spaces.\nmilliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and\ninfrared thermal cameras. It achieves fully automated measurement of distancing\nand body temperature, by jointly tracking the subjects's faces in the thermal\ncamera image plane and the human motion in the radar reference system.\nMoreover, milliTRACE-IR performs contact tracing: a person with high body\ntemperature is reliably detected by the thermal camera sensor and subsequently\ntraced across a large indoor area in a non-invasive way by the radars. When\nentering a new room, a subject is re-identified among several other individuals\nby computing gait-related features from the radar reflections through a deep\nneural network and using a weighted extreme learning machine as the final\nre-identification tool. Experimental results, obtained from a real\nimplementation of milliTRACE-IR, demonstrate decimeter-level accuracy in\ndistance/trajectory estimation, inter-personal distance estimation (effective\nfor subjects getting as close as 0.2 m), and accurate temperature monitoring\n(max. errors of 0.5{\\deg}C). Furthermore, milliTRACE-IR provides contact\ntracing through highly accurate (95%) person re-identification, in less than 20\nseconds.",
    "In this paper, a deep learning approach is presented for direction of arrival\nestimation using automotive-grade ultrasonic sensors which are used for driving\nassistance systems such as automatic parking. A study and implementation of the\nstate of the art deterministic direction of arrival estimation algorithms is\nused as a benchmark for the performance of the proposed approach. Analysis of\nthe performance of the proposed algorithms against the existing algorithms is\ncarried out over simulation data as well as data from a measurement campaign\ndone using automotive-grade ultrasonic sensors. Both sets of results clearly\nshow the superiority of the proposed approach under realistic conditions such\nas noise from the environment as well as eventual errors in measurements. It is\ndemonstrated as well how the proposed approach can overcome some of the known\nlimitations of the existing algorithms such as precision dilution of\ntriangulation and aliasing.",
    "Overhead lines are generally used for electrical energy transmission. Also,\nXLPE underground cable lines are generally used in the city center and the\ncrowded areas to provide electrical safety, so high voltage underground cable\nlines are used together with overhead line in the transmission lines, and these\nlines are called as the mixed lines. The distance protection relays are used to\ndetermine the impedance based fault location according to the current and\nvoltage magnitudes in the transmission lines. However, the fault location\ncannot be correctly detected in mixed transmission lines due to different\ncharacteristic impedance per unit length because the characteristic impedance\nof high voltage cable line is significantly different from overhead line. Thus,\ndeterminations of the fault section and location with the distance protection\nrelays are difficult in the mixed transmission lines. In this study, 154 kV\noverhead transmission line and underground cable line are examined as the mixed\ntransmission line for the distance protection relays. Phase to ground faults\nare created in the mixed transmission line, and overhead line section and\nunderground cable section are simulated by using PSCAD. The short circuit fault\nimages are generated in the distance protection relay for the overhead\ntransmission line and underground cable transmission line faults. The images\ninclude the RX impedance diagram of the fault, and the RX impedance diagram\nhave been detected by applying image processing steps. The regression methods\nare used for prediction of the fault location, and the results of image\nprocessing are used as the input parameters for the training process of the\nregression methods. The results of regression methods are compared to select\nthe most suitable method at the end of this study for forecasting of the fault\nlocation in transmission lines.",
    "The high demand for data rate in the next generation of wireless\ncommunication could be ensured by Non-Orthogonal Multiple Access (NOMA)\napproach in the millimetre-wave (mmW) frequency band. Joint power allocation\nand beamforming of mmW-NOMA systems is mandatory which could be met by\noptimization approaches. To this end, we have exploited Deep Reinforcement\nLearning (DRL) approach due to policy generation leading to an optimized\nsum-rate of users. Actor-critic phenomena are utilized to measure the immediate\nreward and provide the new action to maximize the overall Q-value of the\nnetwork. The immediate reward has been defined based on the summation of the\nrate of two users regarding the minimum guaranteed rate for each user and the\nsum of consumed power as the constraints. The simulation results represent the\nsuperiority of the proposed approach rather than the Time-Division Multiple\nAccess (TDMA) and another NOMA optimized strategy in terms of sum-rate of\nusers.",
    "Robustly determining the optimal number of clusters in a data set is an\nessential factor in a wide range of applications. Cluster enumeration becomes\nchallenging when the true underlying structure in the observed data is\ncorrupted by heavy-tailed noise and outliers. Recently, Bayesian cluster\nenumeration criteria have been derived by formulating cluster enumeration as\nmaximization of the posterior probability of candidate models. This article\ngeneralizes robust Bayesian cluster enumeration so that it can be used with any\narbitrary Real Elliptically Symmetric (RES) distributed mixture model. Our\nframework also covers the case of M-estimators that allow for mixture models,\nwhich are decoupled from a specific probability distribution. Examples of\nHuber's and Tukey's M-estimators are discussed. We derive a robust criterion\nfor data sets with finite sample size, and also provide an asymptotic\napproximation to reduce the computational cost at large sample sizes. The\nalgorithms are applied to simulated and real-world data sets, including\nradar-based person identification, and show a significant robustness\nimprovement in comparison to existing methods.",
    "This study suggests a new prediction model for chaotic time series inspired\nby the brain emotional learning of mammals. We describe the structure and\nfunction of this model, which is referred to as BELPM (Brain Emotional\nLearning-Based Prediction Model). Structurally, the model mimics the connection\nbetween the regions of the limbic system, and functionally it uses weighted k\nnearest neighbors to imitate the roles of those regions. The learning algorithm\nof BELPM is defined using steepest descent (SD) and the least square estimator\n(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to\nevaluate the performance of BELPM. The obtained results have been compared with\nthose of other prediction methods. The results show that BELPM has the\ncapability to achieve a reasonable accuracy for long-term prediction of chaotic\ntime series, using a limited amount of training data and a reasonably low\ncomputational time.",
    "Sensor-based human activity recognition is important in daily scenarios such\nas smart healthcare and homes due to its non-intrusive privacy and low cost\nadvantages, but the problem of out-of-domain generalization caused by\ndifferences in focusing individuals and operating environments can lead to\nsignificant accuracy degradation on cross-person behavior recognition due to\nthe inconsistent distributions of training and test data. To address the above\nproblems, this paper proposes a new method, Multi-channel Time Series\nDecomposition Network (MTSDNet). Firstly, MTSDNet decomposes the original\nsignal into a combination of multiple polynomials and trigonometric functions\nby the trainable parameterized temporal decomposition to learn the low-rank\nrepresentation of the original signal for improving the extraterritorial\ngeneralization ability of the model. Then, the different components obtained by\nthe decomposition are classified layer by layer and the layer attention is used\nto aggregate components to obtain the final classification result. Extensive\nevaluation on DSADS, OPPORTUNITY, PAMAP2, UCIHAR and UniMib public datasets\nshows the advantages in predicting accuracy and stability of our method\ncompared with other competing strategies, including the state-of-the-art ones.\nAnd the visualization is conducted to reveal MTSDNet's interpretability and\nlayer-by-layer characteristics.",
    "Many applications require accurate indoor localization. Fingerprint-based\nlocalization methods propose a solution to this problem, but rely on a radio\nmap that is effort-intensive to acquire. We automate the radio map acquisition\nphase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we\nopen-source a radio map acquired with our automated tool for a 3GPP Long-Term\nEvolution (LTE) wireless link. To the best of our knowledge, this is the first\npublicly available radio map containing channel state information (CSI).\nFinally, we describe first localization experiments on this radio map using a\nconvolutional neural network to regress for location coordinates.",
    "Air Quality Multi-sensors Systems (AQMS) are IoT devices based on low cost\nchemical microsensors array that recently have showed capable to provide\nrelatively accurate air pollutant quantitative estimations. Their availability\npermits to deploy pervasive Air Quality Monitoring (AQM) networks that will\nsolve the geographical sparseness issue that affect the current network of AQ\nRegulatory Monitoring Systems (AQRMS). Unfortunately their accuracy have shown\nlimited in long term field deployments due to negative influence of several\ntechnological issues including sensors poisoning or ageing, non target gas\ninterference, lack of fabrication repeatability, etc. Seasonal changes in\nprobability distribution of priors, observables and hidden context variables\n(i.e. non observable interferents) challenge field data driven calibration\nmodels which short to mid term performances recently rose to the attention of\nUrban authorithies and monitoring agencies. In this work, we address this non\nstationary framework with adaptive learning strategies in order to prolong the\nvalidity of multisensors calibration models enabling continuous learning.\nRelevant parameters influence in different network and note-to-node\nrecalibration scenario is analyzed. Results are hence useful for pervasive\ndeployment aimed to permanent high resolution AQ mapping in urban scenarios as\nwell as for the use of AQMS as AQRMS backup systems providing data when AQRMS\ndata are unavailable due to faults or scheduled mainteinance.",
    "CANDECOMP/PARAFAC (CP) decomposition is the mostly used model to formulate\nthe received tensor signal in a massive MIMO system, as the receiver generally\nsums the components from different paths or users. To achieve accurate and\nlow-latency channel estimation, good and fast CP decomposition (CPD) algorithms\nare desired. The CP alternating least squares (CPALS) is the workhorse\nalgorithm for calculating the CPD. However, its performance depends on the\ninitializations, and good starting values can lead to more efficient solutions.\nExisting initialization strategies are decoupled from the CPALS and are not\nnecessarily favorable for solving the CPD. This paper proposes a\ndeep-learning-aided CPALS (DL-CPALS) method that uses a deep neural network\n(DNN) to generate favorable initializations. The proposed DL-CPALS integrates\nthe DNN and CPALS to a model-based deep learning paradigm, where it trains the\nDNN to generate an initialization that facilitates fast and accurate CPD.\nMoreover, benefiting from the CP low-rankness, the proposed method is trained\nusing noisy data and does not require paired clean data. The proposed DL-CPALS\nis applied to millimeter wave MIMO-OFDM channel estimation. Experimental\nresults demonstrate the significant improvements of the proposed method in\nterms of both speed and accuracy for CPD and channel estimation.",
    "Nonnegative matrix factorization (NMF) is a widely used linear dimensionality\nreduction technique for nonnegative data. NMF requires that each data point is\napproximated by a convex combination of basis elements. Archetypal analysis\n(AA), also referred to as convex NMF, is a well-known NMF variant imposing that\nthe basis elements are themselves convex combinations of the data points. AA\nhas the advantage to be more interpretable than NMF because the basis elements\nare directly constructed from the data points. However, it usually suffers from\na high data fitting error because the basis elements are constrained to be\ncontained in the convex cone of the data points. In this letter, we introduce\nnear-convex archetypal analysis (NCAA) which combines the advantages of both AA\nand NMF. As for AA, the basis vectors are required to be linear combinations of\nthe data points and hence are easily interpretable. As for NMF, the additional\nflexibility in choosing the basis elements allows NCAA to have a low data\nfitting error. We show that NCAA compares favorably with a state-of-the-art\nminimum-volume NMF method on synthetic datasets and on a real-world\nhyperspectral image.",
    "Objective: A novel ECG classification algorithm is proposed for continuous\ncardiac monitoring on wearable devices with limited processing capacity.\nMethods: The proposed solution employs a novel architecture consisting of\nwavelet transform and multiple LSTM recurrent neural networks. Results:\nExperimental evaluations show superior ECG classification performance compared\nto previous works. Measurements on different hardware platforms show the\nproposed algorithm meets timing requirements for continuous and real-time\nexecution on wearable devices. Conclusion: In contrast to many\ncompute-intensive deep-learning based approaches, the proposed algorithm is\nlightweight, and therefore, brings continuous monitoring with accurate\nLSTM-based ECG classification to wearable devices. Significance: The proposed\nalgorithm is both accurate and lightweight. The source code is available online\n[1].",
    "Details of Monarch butterfly migration from the U.S. to Mexico remain a\nmystery due to lack of a proper localization technology to accurately localize\nand track butterfly migration. In this paper, we propose a deep learning based\nbutterfly localization algorithm that can estimate a butterfly's daily location\nby analyzing a light and temperature sensor data log continuously obtained from\nan ultra-low power, mm-scale sensor attached to the butterfly. To train and\ntest the proposed neural network based multi-sensor fusion localization\nalgorithm, we collected over 1500 days of real world sensor measurement data\nwith 82 volunteers all over the U.S. The proposed algorithm exhibits a mean\nabsolute error of <1.5 degree in latitude and <0.5 degree in longitude Earth\ncoordinate, satisfying our target goal for the Monarch butterfly migration\nstudy.",
    "Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted\nMRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation,\ndemyelination, edema, and cartilage composition in various pathologies,\nincluding neurodegenerative disorders, osteoarthritis, and tumors. Deep neural\nnetwork (DNN) based methods have been proposed to address the complex inverse\nproblem of estimating $T_2$ distributions from MRI data, but they are not yet\nrobust enough for clinical data with low Signal-to-Noise ratio (SNR) and are\nhighly sensitive to distribution shifts such as variations in echo-times (TE)\nused during acquisition. Consequently, their application is hindered in\nclinical practice and large-scale multi-institutional trials with heterogeneous\nacquisition protocols. We propose a physically-primed DNN approach, called\n$P_2T_2$, that incorporates the signal decay forward model in addition to the\nMRI signal into the DNN architecture to improve the accuracy and robustness of\n$T_2$ distribution estimation. We evaluated our $P_2T_2$ model in comparison to\nboth DNN-based methods and classical methods for $T_2$ distribution estimation\nusing 1D and 2D numerical simulations along with clinical data. Our model\nimproved the baseline model's accuracy for low SNR levels ($SNR<80$) which are\ncommon in the clinical setting. Further, our model achieved a $\\sim$35\\%\nimprovement in robustness against distribution shifts in the acquisition\nprocess compared to previously proposed DNN models. Finally, Our $P_2T_2$ model\nproduces the most detailed Myelin-Water fraction maps compared to baseline\napproaches when applied to real human MRI data. Our $P_2T_2$ model offers a\nreliable and precise means of estimating $T_2$ distributions from MRI data and\nshows promise for use in large-scale multi-institutional trials with\nheterogeneous acquisition protocols.",
    "In current clinical practice, electroencephalograms (EEG) are reviewed and\nanalyzed by well-trained neurologists to provide supports for therapeutic\ndecisions. The way of manual reviewing is labor-intensive and error prone.\nAutomatic and accurate seizure/nonseizure classification methods are needed.\nOne major problem is that the EEG signals for seizure state and nonseizure\nstate exhibit considerable variations. In order to capture essential seizure\nfeatures, this paper integrates an emerging deep learning model, the\nindependently recurrent neural network (IndRNN), with a dense structure and an\nattention mechanism to exploit temporal and spatial discriminating features and\novercome seizure variabilities. The dense structure is to ensure maximum\ninformation flow between layers. The attention mechanism is to capture spatial\nfeatures. Evaluations are performed in cross-validation experiments over the\nnoisy CHB-MIT data set. The obtained average sensitivity, specificity and\nprecision of 88.80%, 88.60% and 88.69% are better than using the current\nstate-of-the-art methods. In addition, we explore how the segment length\naffects the classification performance. Thirteen different segment lengths are\nassessed, showing that the classification performance varies over the segment\nlengths, and the maximal fluctuating margin is more than 4%. Thus, the segment\nlength is an important factor influencing the classification performance.",
    "One of the primary goals in spectrum occupancy mapping is to create a system\nthat is robust to assumptions about the number of sensors, occupancy threshold\n(in dBm), sensor noise, number of emitters and the propagation environment. We\nshow that such a system may be designed with neural networks using a process of\naggregation to allow a variable number of sensors during training and testing.\nThis process transforms the variable number of measurements into approximate\nlog-likelihood ratios (LLRs), which are fed as a fixed-resolution image into a\nneural network. The use of LLR's provides robustness to the effects of noise\nand occupancy threshold. In other words, a system may be trained for a nominal\nnumber of sensors, threshold and noise levels, and still operate well at\nvarious other levels without retraining. Our system operates without knowledge\nof the number of emitters and does not explicitly attempt to estimate their\nnumber or power. Receiver operating curves with realistic propagation\nenvironments using topographic maps with commercial network design tools show\nhow performance of the neural network varies with the environment. The use of\nvery low-resolution sensors in this system can still yield good performance.",
    "Freezing of gait (FoG) is a common gait disability in Parkinson's disease,\nthat usually appears in its advanced stage. Freeze episodes are associated with\nfalls, injuries, and psychological consequences, negatively affecting the\npatients' quality of life. For detecting FoG episodes automatically, a highly\naccurate detection method is necessary. This paper presents an approach for\ndetecting FoG episodes utilizing a deep recurrent neural network (RNN) on\n3D-accelerometer measurements. We investigate suitable features and feature\ncombinations extracted from the sensors' time series data. Specifically, for\ndetecting FoG episodes, we apply a deep RNN with Long Short-Term Memory cells.\nIn our experiments, we perform both user dependent and user independent\nexperiments, to detect freeze episodes. Our experimental results show that the\nfrequency domain features extracted from the trunk sensor are the most\ninformative feature group in the subject independent method, achieving an\naverage AUC score of 93%, Specificity of 90% and Sensitivity of 81%. Moreover,\nfrequency and statistical features of all the sensors are identified as the\nbest single input for the subject dependent method, achieving an average AUC\nscore of 97%, Specificity of 96% and Sensitivity of 87%. Overall, in a\ncomparison to state-of-the-art approaches from literature as baseline methods,\nour proposed approach outperforms these significantly.",
    "Beam alignment (BA) is to ensure the transmitter and receiver beams are\naccurately aligned to establish a reliable communication link in\nmillimeter-wave (mmwave) systems. Existing BA methods search the entire beam\nspace to identify the optimal transmit-receive beam pair, which incurs\nsignificant BA latency on the order of seconds in the worst case. In this\npaper, we develop a learning algorithm to reduce BA latency, namely\nHierarchical Beam Alignment (HBA) algorithm. We first formulate the BA problem\nas a stochastic multi-armed bandit problem with the objective to maximize the\ncumulative received signal strength within a certain period. The proposed\nalgorithm takes advantage of the correlation structure among beams such that\nthe information from nearby beams is extracted to identify the optimal beam,\ninstead of searching the entire beam space. Furthermore, the prior knowledge on\nthe channel fluctuation is incorporated in the proposed algorithm to further\naccelerate the BA process. Theoretical analysis indicates that the proposed\nalgorithm is asymptotically optimal. Extensive simulation results demonstrate\nthat the proposed algorithm can identify the optimal beam with a high\nprobability and reduce the BA latency from hundreds of milliseconds to a few\nmilliseconds in the multipath channel, as compared to the existing BA method in\nIEEE 802.11ad.",
    "Plug-and-play priors (PnP) is a popular framework for regularized signal\nreconstruction by using advanced denoisers within an iterative algorithm. In\nthis paper, we discuss our recent online variant of PnP that uses only a subset\nof measurements at every iteration, which makes it scalable to very large\ndatasets. We additionally present novel convergence results for both batch and\nonline PnP algorithms.",
    "Developments in Brain Computer Interfaces (BCIs) are empowering those with\nsevere physical afflictions through their use in assistive systems. Common\nmethods of achieving this is via Motor Imagery (MI), which maps brain signals\nto code for certain commands. Electroencephalogram (EEG) is preferred for\nrecording brain signal data on account of it being non-invasive. Despite their\npotential utility, MI-BCI systems are yet confined to research labs. A major\ncause for this is lack of robustness of such systems. As hypothesized by two\nteams during Cybathlon 2016, a particular source of the system's vulnerability\nis the sharp change in the subject's state of emotional arousal. This work aims\ntowards making MI-BCI systems resilient to such emotional perturbations. To do\nso, subjects are exposed to high and low arousal-inducing virtual reality (VR)\nenvironments before recording EEG data. The advent of COVID-19 compelled us to\nmodify our methodology. Instead of training machine learning algorithms to\nclassify emotional arousal, we opt for classifying subjects that serve as proxy\nfor each state. Additionally, MI models are trained for each subject instead of\neach arousal state. As training subjects to use MI-BCI can be an arduous and\ntime-consuming process, reducing this variability and increasing robustness can\nconsiderably accelerate the acceptance and adoption of assistive technologies\npowered by BCI.",
    "We consider the problem of joint channel assignment and power allocation in\nunderlaid cellular vehicular-to-everything (C-V2X) systems where multiple\nvehicle-to-network (V2N) uplinks share the time-frequency resources with\nmultiple vehicle-to-vehicle (V2V) platoons that enable groups of connected and\nautonomous vehicles to travel closely together. Due to the nature of high user\nmobility in vehicular environment, traditional centralized optimization\napproach relying on global channel information might not be viable in C-V2X\nsystems with large number of users. Utilizing a multi-agent reinforcement\nlearning (RL) approach, we propose a distributed resource allocation (RA)\nalgorithm to overcome this challenge. Specifically, we model the RA problem as\na multi-agent system. Based solely on the local channel information, each\nplatoon leader, acting as an agent, collectively interacts with each other and\naccordingly selects the optimal combination of sub-band and power level to\ntransmit its signals. Toward this end, we utilize the double deep Q-learning\nalgorithm to jointly train the agents under the objectives of simultaneously\nmaximizing the sum-rate of V2N links and satisfying the packet delivery\nprobability of each V2V link in a desired latency limitation. Simulation\nresults show that our proposed RL-based algorithm provides a close performance\ncompared to that of the well-known exhaustive search algorithm.",
    "Accurate detection of pathological conditions in human subjects can be\nachieved through off-line analysis of recorded biological signals such as\nelectrocardiograms (ECGs). However, human diagnosis is time-consuming and\nexpensive, as it requires the time of medical professionals. This is especially\ninefficient when indicative patterns in the biological signals are infrequent.\nMoreover, patients with suspected pathologies are often monitored for extended\nperiods, requiring the storage and examination of large amounts of\nnon-pathological data, and entailing a difficult visual search task for\ndiagnosing professionals.\n  In this work we propose a compact and sub-mW low power neural processing\nsystem that can be used to perform on-line and real-time preliminary diagnosis\nof pathological conditions, to raise warnings for the existence of possible\npathological conditions, or to trigger an off-line data recording system for\nfurther analysis by a medical professional. We apply the system to real-time\nclassification of ECG data for distinguishing between healthy heartbeats and\npathological rhythms.\n  Multi-channel analog ECG traces are encoded as asynchronous streams of binary\nevents and processed using a spiking recurrent neural network operated in a\nreservoir computing paradigm. An event-driven neuron output layer is then\ntrained to recognize one of several pathologies. Finally, the filtered activity\nof this output layer is used to generate a binary trigger signal indicating the\npresence or absence of a pathological pattern.\n  We validate the approach proposed using a Dynamic Neuromorphic Asynchronous\nProcessor (DYNAP) chip, implemented using a standard 180 nm CMOS VLSI process,\nand present experimental results measured from the chip.",
    "Clinical electroencephalographic (EEG) data varies significantly depending on\na number of operational conditions (e.g., the type and placement of electrodes,\nthe type of electrical grounding used). This investigation explores the\nstatistical differences present in two different referential montages: Linked\nEar (LE) and Averaged Reference (AR). Each of these accounts for approximately\n45% of the data in the TUH EEG Corpus. In this study, we explore the impact\nthis variability has on machine learning performance. We compare the\nstatistical properties of features generated using these two montages, and\nexplore the impact of performance on our standard Hidden Markov Model (HMM)\nbased classification system. We show that a system trained on LE data\nsignificantly outperforms one trained only on AR data (77.2% vs. 61.4%). We\nalso demonstrate that performance of a system trained on both data sets is\nsomewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data\nsuggests that mean, variance and channel normalization should be considered.\nHowever, cepstral mean subtraction failed to produce an improvement in\nperformance, suggesting that the impact of these statistical differences is\nsubtler.",
    "Due to the very narrow beam used in millimeter wave communication (mmWave),\nbeam alignment (BA) is a critical issue. In this work, we investigate the issue\nof mmWave BA and present a novel beam alignment scheme on the basis of a\nmachine learning strategy, Bayesian optimization (BO). In this context, we\nconsider the beam alignment issue to be a black box function and then use BO to\nfind the possible optimal beam pair. During the BA procedure, this strategy\nexploits information from the measured beam pairs to predict the best beam\npair. In addition, we suggest a novel BO algorithm based on the gradient\nboosting regression tree model. The simulation results demonstrate the spectral\nefficiency performance of our proposed schemes for BA using three different\nsurrogate models. They also demonstrate that the proposed schemes can achieve\nspectral efficiency with a small overhead when compared to the orthogonal match\npursuit (OMP) algorithm and the Thompson sampling-based multi-armed bandit\n(TS-MAB) method.",
    "Binarized neural networks (BNNs) have shown exciting potential for utilising\nneural networks in embedded implementations where area, energy and latency\nconstraints are paramount. With BNNs, multiply-accumulate (MAC) operations can\nbe simplified to XnorPopcount operations, leading to massive reductions in both\nmemory and computation resources. Furthermore, multiple efficient\nimplementations of BNNs have been reported on field-programmable gate array\n(FPGA) implementations. This paper proposes a smaller, faster, more\nenergy-efficient approximate replacement for the XnorPopcountoperation, called\nXNorMaj, inspired by state-of-the-art FPGAlook-up table schemes which benefit\nFPGA implementations. Weshow that XNorMaj is up to 2x more resource-efficient\nthan the XnorPopcount operation. While the XNorMaj operation has a minor\ndetrimental impact on accuracy, the resource savings enable us to use larger\nnetworks to recover the loss.",
    "Objective: The aim of this study is to develop an automated classification\nalgorithm for polysomnography (PSG) recordings to detect non-apneic and\nnon-hypopneic arousals. Our particular focus is on detecting the respiratory\neffort-related arousals (RERAs) which are very subtle respiratory events that\ndo not meet the criteria for apnea or hypopnea, and are more challenging to\ndetect. Methods: The proposed algorithm is based on a bidirectional long\nshort-term memory (BiLSTM) classifier and 465 multi-domain features, extracted\nfrom multimodal clinical time series. The features consist of a set of\nphysiology-inspired features (n = 75), obtained by multiple steps of feature\nselection and expert analysis, and a set of physiology-agnostic features (n =\n390), derived from scattering transform. Results: The proposed algorithm is\nvalidated on the 2018 PhysioNet challenge dataset. The overall performance in\nterms of the area under the precision-recall curve (AUPRC) is 0.50 on the\nhidden test dataset. This result is tied for the second-best score during the\nfollow-up and official phases of the 2018 PhysioNet challenge. Conclusions: The\nresults demonstrate that it is possible to automatically detect subtle\nnon-apneic/non-hypopneic arousal events from PSG recordings. Significance:\nAutomatic detection of subtle respiratory events such as RERAs together with\nother non-apneic/non-hypopneic arousals will allow detailed annotations of\nlarge PSG databases. This contributes to a better retrospective analysis of\nsleep data, which may also improve the quality of treatment.",
    "A spiking neural network (SNN) equalizer model suitable for electronic\nneuromorphic hardware is designed for an IM/DD link. The SNN achieves the same\nbit-error-rate as an artificial neural network, outperforming linear\nequalization.",
    "In this study, we adopted visual motion imagery, which is a more intuitive\nbrain-computer interface (BCI) paradigm, for decoding the intuitive user\nintention. We developed a 3-dimensional BCI training platform and applied it to\nassist the user in performing more intuitive imagination in the visual motion\nimagery experiment. The experimental tasks were selected based on the movements\nthat we commonly used in daily life, such as picking up a phone, opening a\ndoor, eating food, and pouring water. Nine subjects participated in our\nexperiment. We presented statistical evidence that visual motion imagery has a\nhigh correlation from the prefrontal and occipital lobes. In addition, we\nselected the most appropriate electroencephalography channels using a\nfunctional connectivity approach for visual motion imagery decoding and\nproposed a convolutional neural network architecture for classification. As a\nresult, the averaged classification performance of the proposed architecture\nfor 4 classes from 16 channels was 67.50 % across all subjects. This result is\nencouraging, and it shows the possibility of developing a BCI-based device\ncontrol system for practical applications such as neuroprosthesis and a robotic\narm.",
    "It is challenging to visually detect heart disease from the\nelectrocardiographic (ECG) signals. Implementing an automated ECG signal\ndetection system can help diagnosis arrhythmia in order to improve the accuracy\nof diagnosis. In this paper, we proposed, implemented, and compared an\nautomated system using two different frameworks of the combination of\nconvolutional neural network (CNN) and long-short term memory (LSTM) for\nclassifying normal sinus signals, atrial fibrillation, and other noisy signals.\nThe dataset we used is from the MIT-BIT Arrhythmia Physionet. Our approach\ndemonstrated that the cascade of two deep learning network has higher\nperformance than the concatenation of them, achieving a weighted f1 score of\n0.82. The experimental results have successfully validated that the cascade of\nCNN and LSTM can achieve satisfactory performance on discriminating ECG\nsignals.",
    "In this work we propose an autoencoder based framework for simultaneous\nreconstruction and classification of biomedical signals. Previously these two\ntasks, reconstruction and classification were treated as separate problems.\nThis is the first work to propose a combined framework to address the issue in\na holistic fashion. Reconstruction techniques for biomedical signals for\ntele-monitoring are largely based on compressed sensing (CS) based method,\nthese are designed techniques where the reconstruction formulation is based on\nsome assumption regarding the signal. In this work, we propose a new paradigm\nfor reconstruction we learn to reconstruct. An autoencoder can be trained for\nthe same. But since the final goal is to analyze classify the signal we learn a\nlinear classification map inside the autoencoder. The ensuing optimization\nproblem is solved using the Split Bregman technique. Experiments have been\ncarried out on reconstruction and classification of ECG arrhythmia\nclassification and EEG seizure classification signals. Our proposed tool is\ncapable of operating in a semi-supervised fashion. We show that our proposed\nmethod is better and more than an order magnitude faster in reconstruction than\nCS based methods; it is capable of real-time operation. Our method is also\nbetter than recently proposed classification methods. Significance: This is the\nfirst work offering an alternative to CS based reconstruction. It also shows\nthat representation learning can yield better results than hand-crafted\nfeatures for signal analysis.",
    "In this paper we present a memristor-inspired computational method for\nobtaining a type of running spectrogram or fingerprint of epileptiform activity\ngenerated by rodent hippocampal spheroids. It can be used to compute on the fly\nand with low computational cost an alert-level signal for epileptiform events\nonset. Here, we describe the computational method behind this fingerprint\ntechnique and illustrate it using epileptiform events recorded from hippocampal\nspheroids using a microelectrode array system.",
    "The use of electroencephalogram (EEG) as the main input signal in\nbrain-machine interfaces has been widely proposed due to the non-invasive\nnature of the EEG. Here we are specifically interested in interfaces that\nextract information from the auditory system and more specifically in the task\nof classifying heard speech from EEGs. To do so, we propose to limit the\npreprocessing of the EEGs and use machine learning approaches to automatically\nextract their meaningful characteristics. More specifically, we use a regulated\nrecurrent neural network (RNN) reservoir, which has been shown to outperform\nclassic machine learning approaches when applied to several different\nbio-signals, and we compare it with a deep neural network approach. Moreover,\nwe also investigate the classification performance as a function of the number\nof EEG electrodes. A set of 8 subjects were presented randomly with 3 different\nauditory stimuli (English vowels a, i and u). We obtained an excellent\nclassification rate of 83.2% with the RNN when considering all 64 electrodes. A\nrate of 81.7% was achieved with only 10 electrodes.",
    "Autism Spectrum Disorder (ASD) is a developmental disorder that often impairs\na child's normal development of the brain. According to CDC, it is estimated\nthat 1 in 6 children in the US suffer from development disorders, and 1 in 68\nchildren in the US suffer from ASD. This condition has a negative impact on a\nperson's ability to hear, socialize and communicate. Overall, ASD has a broad\nrange of symptoms and severity; hence the term spectrum is used. One of the\nmain contributors to ASD is known to be genetics. Up to date, no suitable cure\nfor ASD has been found. Early diagnosis is crucial for the long-term treatment\nof ASD, but this is challenging due to the lack of a proper objective measures.\nSubjective measures often take more time, resources, and have false positives\nor false negatives. There is a need for efficient objective measures that can\nhelp in diagnosing this disease early as possible with less effort.\n  EEG measures the electric signals of the brain via electrodes placed on\nvarious places on the scalp. These signals can be used to study complex\nneuropsychiatric issues. Studies have shown that EEG has the potential to be\nused as a biomarker for various neurological conditions including ASD. This\nchapter will outline the usage of EEG measurement for the classification of ASD\nusing machine learning algorithms.",
    "Brain Computer Interfaces (BCI) have become very popular with\nElectroencephalography (EEG) being one of the most commonly used signal\nacquisition techniques. A major challenge in BCI studies is the individualistic\nanalysis required for each task. Thus, task-specific feature extraction and\nclassification are performed, which fails to generalize to other tasks with\nsimilar time-series EEG input data. To this end, we design a GRU-based\nuniversal deep encoding architecture to extract meaningful features from\npublicly available datasets for five diverse EEG-based classification tasks.\nOur network can generate task and format-independent data representation and\noutperform the state of the art EEGNet architecture on most experiments. We\nalso compare our results with CNN-based, and Autoencoder networks, in turn\nperforming local, spatial, temporal and unsupervised analysis on the data.",
    "This work proposes a low-power high-accuracy embedded hand-gesture\nrecognition algorithm targeting battery-operated wearable devices using low\npower short-range RADAR sensors. A 2D Convolutional Neural Network (CNN) using\nrange frequency Doppler features is combined with a Temporal Convolutional\nNeural Network (TCN) for time sequence prediction. The final algorithm has a\nmodel size of only 46 thousand parameters, yielding a memory footprint of only\n92 KB. Two datasets containing 11 challenging hand gestures performed by 26\ndifferent people have been recorded containing a total of 20,210 gesture\ninstances. On the 11 hand gesture dataset, accuracies of 86.6% (26 users) and\n92.4% (single user) have been achieved, which are comparable to the\nstate-of-the-art, which achieves 87% (10 users) and 94% (single user), while\nusing a TCN-based network that is 7500x smaller than the state-of-the-art.\nFurthermore, the gesture recognition classifier has been implemented on a\nParallel Ultra-Low Power Processor, demonstrating that real-time prediction is\nfeasible with only 21 mW of power consumption for the full TCN sequence\nprediction network, while a system-level power consumption of less than 100 mW\nis achieved. We provide open-source access to all the code and data collected\nand used in this work on tinyradar.ethz.ch.",
    "Recent advances in unmanned aerial vehicle (UAV) technology have\nrevolutionized a broad class of civil and military applications. However, the\ndesigns of wireless technologies that enable real-time streaming of\nhigh-definition video between UAVs and ground clients present a conundrum. Most\nexisting adaptive bitrate (ABR) algorithms are not optimized for the\nair-to-ground links, which usually fluctuate dramatically due to the dynamic\nflight states of the UAV. In this paper, we present SA-ABR, a new\nsensor-augmented system that generates ABR video streaming algorithms with the\nassistance of various kinds of inherent sensor data that are used to pilot\nUAVs. By incorporating the inherent sensor data with network observations,\nSA-ABR trains a deep reinforcement learning (DRL) model to extract salient\nfeatures from the flight state information and automatically learn an ABR\nalgorithm to adapt to the varying UAV channel capacity through the training\nprocess. SA-ABR does not rely on any assumptions or models about UAV's flight\nstates or the environment, but instead, it makes decisions by exploiting\ntemporal properties of past throughput through the long short-term memory\n(LSTM) to adapt itself to a wide range of highly dynamic environments. We have\nimplemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare\nSA-ABR with a variety of existing state-of-the-art ABR algorithms, and the\nresults show that our system outperforms the best known existing ABR algorithm\nby 21.4% in terms of the average quality of experience (QoE) reward.",
    "Identification of the type of communication technology and/or modulation\nscheme based on detected radio signal are challenging problems encountered in a\nvariety of applications including spectrum allocation and radio interference\nmitigation. They are rendered difficult due to a growing number of emitter\ntypes and varied effects of real-world channels upon the radio signal. Existing\nspectrum monitoring techniques are capable of acquiring massive amounts of\nradio and real-time spectrum data using compact sensors deployed in a variety\nof settings. However, state-of-the-art methods that use such data to classify\nemitter types and detect communication schemes struggle to achieve required\nlevels of accuracy at a computational efficiency that would allow their\nimplementation on low-cost computational platforms. In this paper, we present a\nlearning framework based on an LSTM denoising auto-encoder designed to\nautomatically extract stable and robust features from noisy radio signals, and\ninfer modulation or technology type using the learned features. The algorithm\nutilizes a compact neural network architecture readily implemented on a\nlow-cost computational platform while exceeding state-of-the-art accuracy.\nResults on realistic synthetic as well as over-the-air radio data demonstrate\nthat the proposed framework reliably and efficiently classifies received radio\nsignals, often demonstrating superior performance compared to state-of-the-art\nmethods.",
    "Accelerated multi-coil magnetic resonance imaging reconstruction has seen a\nsubstantial recent improvement combining compressed sensing with deep learning.\nHowever, most of these methods rely on estimates of the coil sensitivity\nprofiles, or on calibration data for estimating model parameters. Prior work\nhas shown that these methods degrade in performance when the quality of these\nestimators are poor or when the scan parameters differ from the training\nconditions. Here we introduce Deep J-Sense as a deep learning approach that\nbuilds on unrolled alternating minimization and increases robustness: our\nalgorithm refines both the magnetization (image) kernel and the coil\nsensitivity maps. Experimental results on a subset of the knee fastMRI dataset\nshow that this increases reconstruction performance and provides a significant\ndegree of robustness to varying acceleration factors and calibration region\nsizes.",
    "The accuracy of channel state information (CSI) acquisition directly affects\nthe performance of millimeter wave (mmWave) communications. In this article, we\nprovide an overview on CSI acquisition, including beam training and channel\nestimation for mmWave massive multiple-input multiple-output systems. The beam\ntraining can avoid the estimation of a high-dimension channel matrix while the\nchannel estimation can flexibly exploit advanced signal processing techniques.\nIn addition to introducing the traditional and machine learning-based\napproaches in this article, we also compare different approaches in terms of\nspectral efficiency, computational complexity, and overhead.",
    "Effective analysis of EEG signals for potential clinical applications remains\na challenging task. So far, the analysis and conditioning of EEG have largely\nremained sex-neutral. This paper employs a machine learning approach to explore\nthe evidence of sex effects on EEG signals, and confirms the generality of\nthese effects by achieving successful sex prediction of resting-state EEG\nsignals. We have found that the brain connectivity represented by the coherence\nbetween certain sensor channels are good predictors of sex.",
    "Online machine learning (OML) algorithms do not need any training phase and\ncan be deployed directly in an unknown environment. OML includes multi-armed\nbandit (MAB) algorithms that can identify the best arm among several arms by\nachieving a balance between exploration of all arms and exploitation of optimal\narm. The Kullback-Leibler divergence based upper confidence bound (KLUCB) is\nthe state-of-the-art MAB algorithm that optimizes exploration-exploitation\ntrade-off but it is complex due to underlining optimization routine. This\nlimits its usefulness for robotics and radio applications which demand\nintegration of KLUCB with the PHY on the system on chip (SoC). In this paper,\nwe efficiently map the KLUCB algorithm on SoC by realizing optimization routine\nvia alternative synthesizable computation without compromising on the\nperformance. The proposed architecture is dynamically reconfigurable such that\nthe number of arms, as well as type of algorithm, can be changed on-the-fly.\nSpecifically, after initial learning, on-the-fly switch to light-weight UCB\noffers around 10-factor improvement in latency and throughput. Since learning\nduration depends on the unknown arm statistics, we offer intelligence embedded\nin architecture to decide the switching instant. We validate the functional\ncorrectness and usefulness of the proposed architecture via a realistic\nwireless application and detailed complexity analysis demonstrates its\nfeasibility in realizing intelligent radios.",
    "Radar pulse streams exhibit increasingly complex temporal patterns and can no\nlonger rely on a purely value-based analysis of the pulse attributes for the\npurpose of emitter classification. In this paper, we employ Recurrent Neural\nNetworks (RNNs) to efficiently model and exploit the temporal dependencies\npresent inside pulse streams. With the purpose of enhancing the network\nprediction capability, we introduce two novel techniques: a per-sequence\nnormalization, able to mine the useful temporal patterns; and\nattribute-specific RNN processing, capable of processing the extracted\ninformation effectively. The new techniques are evaluated with an ablation\nstudy and the proposed solution is compared to previous Deep Learning (DL)\napproaches. Finally, a comparative study on the robustness of the same\napproaches is conducted and its results are presented.",
    "Geospatial observations combined with computational models have become key to\nunderstanding the physical systems of our environment and enable the design of\nbest practices to reduce societal harm. Cloud-based deployments help to scale\nup these modeling and AI workflows. Yet, for practitioners to make robust\nconclusions, model tuning and testing is crucial, a resource intensive process\nwhich involves the variation of model input variables. We have developed the\nVariational Exploration Module which facilitates the optimization and\nvalidation of modeling workflows deployed in the cloud by orchestrating\nworkflow executions and using Bayesian and machine learning-based methods to\nanalyze model behavior. User configurations allow the combination of diverse\nsampling strategies in multi-agent environments. The flexibility and robustness\nof the model-agnostic module is demonstrated using real-world applications.",
    "User Stories record what must be built in projects that use agile practices.\nUser Stories serve both to estimate effort, generally measured in Story Points,\nand to plan what should be done in a Sprint. Therefore, it is essential to\ntrain software engineers on how to create simple, easily readable, and\ncomprehensive User Stories. For that reason, we designed, implemented, applied,\nand evaluated a web application called User Story Tutor (UST). UST checks the\ndescription of a given User Story for readability, and if needed, recommends\nappropriate practices for improvement. UST also estimates a User Story effort\nin Story Points using Machine Learning techniques. As such UST may support the\ncontinuing education of agile development teams when writing and reviewing User\nStories. UST's ease of use was evaluated by 40 agile practitioners according to\nthe Technology Acceptance Model (TAM) and AttrakDiff. The TAM evaluation\naverages were good in almost all considered variables. Application of the\nAttrakDiff evaluation framework produced similar good results. Apparently, UST\ncan be used with good reliability. Applying UST to assist in the construction\nof User Stories is a viable technique that, at the very least, can be used by\nagile developments to complement and enhance current User Story creation.",
    "Formal methods apply algorithms based on mathematical principles to enhance\nthe reliability of systems. It would only be natural to try to progress from\nverification, model checking or testing a system against its formal\nspecification into constructing it automatically. Classical algorithmic\nsynthesis theory provides interesting algorithms but also alarming high\ncomplexity and undecidability results. The use of genetic programming, in\ncombination with model checking and testing, provides a powerful heuristic to\nsynthesize programs. The method is not completely automatic, as it is fine\ntuned by a user that sets up the specification and parameters. It also does not\nguarantee to always succeed and converge towards a solution that satisfies all\nthe required properties. However, we applied it successfully on quite\nnontrivial examples and managed to find solutions to hard programming\nchallenges, as well as to improve and to correct code. We describe here several\nversions of our method for synthesizing sequential and concurrent systems.",
    "Despite the immense popularity of the Automated Program Repair (APR) field,\nthe question of patch validation is still open. Most of the present-day\napproaches follow the so-called Generate-and-Validate approach, where first a\ncandidate solution is being generated and after validated against an oracle.\nThe latter, however, might not give a reliable result, because of the\nimperfections in such oracles; one of which is usually the test suite. Although\n(re-) running the test suite is right under one's nose, in real life\napplications the problem of over- and underfitting often occurs, resulting in\ninadequate patches. Efforts that have been made to tackle with this problem\ninclude patch filtering, test suite expansion, careful patch producing and many\nmore. Most approaches to date use post-filtering relying either on test\nexecution traces or make use of some similarity concept measured on the\ngenerated patches. Our goal is to investigate the nature of these\nsimilarity-based approaches. To do so, we trained a Doc2Vec model on an\nopen-source JavaScript project and generated 465 patches for 10 bugs in it.\nThese plausible patches alongside with the developer fix are then ranked based\non their similarity to the original program. We analyzed these similarity lists\nand found that plain document embeddings may lead to misclassification - it\nfails to capture nuanced code semantics. Nevertheless, in some cases it also\nprovided useful information, thus helping to better understand the area of\nAutomated Program Repair.",
    "In recent years, Explainable AI (xAI) attracted a lot of attention as various\ncountries turned explanations into a legal right. xAI allows for improving\nmodels beyond the accuracy metric by, e.g., debugging the learned pattern and\ndemystifying the AI's behavior. The widespread use of xAI brought new\nchallenges. On the one hand, the number of published xAI algorithms underwent a\nboom, and it became difficult for practitioners to select the right tool. On\nthe other hand, some experiments did highlight how easy data scientists could\nmisuse xAI algorithms and misinterpret their results. To tackle the issue of\ncomparing and correctly using feature importance xAI algorithms, we propose\nCompare-xAI, a benchmark that unifies all exclusive functional testing methods\napplied to xAI algorithms. We propose a selection protocol to shortlist\nnon-redundant functional tests from the literature, i.e., each targeting a\nspecific end-user requirement in explaining a model. The benchmark encapsulates\nthe complexity of evaluating xAI methods into a hierarchical scoring of three\nlevels, namely, targeting three end-user groups: researchers, practitioners,\nand laymen in xAI. The most detailed level provides one score per test. The\nsecond level regroups tests into five categories (fidelity, fragility,\nstability, simplicity, and stress tests). The last level is the aggregated\ncomprehensibility score, which encapsulates the ease of correctly interpreting\nthe algorithm's output in one easy to compare value. Compare-xAI's interactive\nuser interface helps mitigate errors in interpreting xAI results by quickly\nlisting the recommended xAI solutions for each ML task and their current\nlimitations. The benchmark is made available at\nhttps://karim-53.github.io/cxai/",
    "Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.",
    "In software development, it is common for programmers to copy-paste or port\ncode snippets and then adapt them to their use case. This scenario motivates\nthe code adaptation task -- a variant of program repair which aims to adapt\nvariable identifiers in a pasted snippet of code to the surrounding,\npreexisting source code. However, no existing approach has been shown to\neffectively address this task. In this paper, we introduce AdaptivePaste, a\nlearning-based approach to source code adaptation, based on transformers and a\ndedicated dataflow-aware deobfuscation pre-training task to learn meaningful\nrepresentations of variable usage patterns. We evaluate AdaptivePaste on a\ndataset of code snippets in Python. Results suggest that our model can learn to\nadapt source code with 79.8% accuracy. To evaluate how valuable is\nAdaptivePaste in practice, we perform a user study with 10 Python developers on\na hundred real-world copy-paste instances. The results show that AdaptivePaste\nreduces the dwell time to nearly half the time it takes for manual code\nadaptation, and helps to avoid bugs. In addition, we utilize the participant\nfeedback to identify potential avenues for improvement of AdaptivePaste.",
    "Performance becomes an issue particularly when execution cost hinders the\nfunctionality of a program. Typically a profiler can be used to find program\ncode execution which represents a large portion of the overall execution cost\nof a program. Pinpointing where a performance issue exists provides a starting\npoint for tracing cause back through a program.\n  While profiling shows where a performance issue manifests, we use mutation\nanalysis to show where a performance improvement is likely to exist. We find\nthat mutation analysis can indicate locations within a program which are highly\nimpactful to the overall execution cost of a program yet are executed\nrelatively infrequently. By better locating potential performance improvements\nin programs we hope to make performance improvement more amenable to\nautomation.",
    "Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.",
    "Vulnerability identification is crucial for cyber security in the\nsoftware-related industry. Early identification methods require significant\nmanual efforts in crafting features or annotating vulnerable code. Although the\nrecent pre-trained models alleviate this issue, they overlook the multiple rich\nstructural information contained in the code itself. In this paper, we propose\na novel Multi-View Pre-Trained Model (MV-PTM) that encodes both sequential and\nmulti-type structural information of the source code and uses contrastive\nlearning to enhance code representations. The experiments conducted on two\npublic datasets demonstrate the superiority of MV-PTM. In particular, MV-PTM\nimproves GraphCodeBERT by 3.36\\% on average in terms of F1 score.",
    "Over the last decades, the amount of data of all kinds available\nelectronically has increased dramatically. Data are accessible through a range\nof interfaces including Web browsers, database query languages,\napplication-specific interfaces, built on top of a number of different data\nexchange formats. All these data span from un-structured to highly structured\ndata. Very often, some of them have structure even if the structure is\nimplicit, and not as rigid or regular as that found in standard database\nsystems. Spreadsheet documents are prototypical in this respect. Spreadsheets\nare the lightweight technology able to supply companies with easy to build\nbusiness management and business intelligence applications, and business people\nlargely adopt spreadsheets as smart vehicles for data files generation and\nsharing. Actually, the more spreadsheets grow in complexity (e.g., their use in\nproduct development plans and quoting), the more their arrangement,\nmaintenance, and analysis appear as a knowledge-driven activity. The\nalgorithmic approach to the problem of automatic data structure extraction from\nspreadsheet documents (i.e., grid-structured and free topological-related data)\nemerges from the WIA project: Worksheets Intelligent Analyser. The\nWIA-algorithm shows how to provide a description of spreadsheet contents in\nterms of higher level of abstractions or conceptualisations. In particular, the\nWIA-algorithm target is about the extraction of i) the calculus work-flow\nimplemented in the spreadsheets formulas and ii) the logical role played by the\ndata which take part into the calculus. The aim of the resulting\nconceptualisations is to provide spreadsheets with abstract representations\nuseful for further model refinements and optimizations through evolutionary\nalgorithms computations.",
    "Automation engineering is the task of integrating, via software, various\nsensors, actuators, and controls for automating a real-world process. Today,\nautomation engineering is supported by a suite of software tools including\nintegrated development environments (IDE), hardware configurators, compilers,\nand runtimes. These tools focus on the automation code itself, but leave the\nautomation engineer unassisted in their decision making. This can lead to\nincreased time for software development because of imperfections in decision\nmaking leading to multiple iterations between software and hardware. To address\nthis, this paper defines multiple challenges often faced in automation\nengineering and propose solutions using machine learning to assist engineers\ntackle such challenges. We show that machine learning can be leveraged to\nassist the automation engineer in classifying automation, finding similar code\nsnippets, and reasoning about the hardware selection of sensors and actuators.\nWe validate our architecture on two real datasets consisting of 2,927 Arduino\nprojects, and 683 Programmable Logic Controller (PLC) projects. Our results\nshow that paragraph embedding techniques can be utilized to classify automation\nusing code snippets with precision close to human annotation, giving an\nF1-score of 72%. Further, we show that such embedding techniques can help us\nfind similar code snippets with high accuracy. Finally, we use autoencoder\nmodels for hardware recommendation and achieve a p@3 of 0.79 and p@5 of 0.95.",
    "This paper presents an ensemble part-of-speech tagging approach for source\ncode identifiers. Ensemble tagging is a technique that uses machine-learning\nand the output from multiple part-of-speech taggers to annotate natural\nlanguage text at a higher quality than the part-of-speech taggers are able to\nobtain independently. Our ensemble uses three state-of-the-art part-of-speech\ntaggers: SWUM, POSSE, and Stanford. We study the quality of the ensemble's\nannotations on five different types of identifier names: function, class,\nattribute, parameter, and declaration statement at the level of both individual\nwords and full identifier names. We also study and discuss the weaknesses of\nour tagger to promote the future amelioration of these problems through further\nresearch. Our results show that the ensemble achieves 75\\% accuracy at the\nidentifier level and 84-86\\% accuracy at the word level. This is an increase of\n+17\\% points at the identifier level from the closest independent\npart-of-speech tagger.",
    "Program source code contains complex structure information, which can be\nrepresented in structured data forms like trees or graphs. To acquire the\nstructural information in source code, most existing researches use abstract\nsyntax trees (AST). A group of works add additional edges to ASTs to convert\nsource code into graphs and use graph neural networks to learn representations\nfor program graphs. Although these works provide additional control or data\nflow information to ASTs for downstream tasks, they neglect an important aspect\nof structure information in AST itself: the different types of nodes and edges.\nIn ASTs, different nodes contain different kinds of information like variables\nor control flow, and the relation between a node and all its children can also\nbe different.\n  To address the information of node and edge types, we bring the idea of\nheterogeneous graphs to learning on source code and present a new formula of\nbuilding heterogeneous program graphs from ASTs with additional type\ninformation for nodes and edges. We use the ASDL grammar of programming\nlanguage to define the node and edge types of program graphs. Then we use\nheterogeneous graph neural networks to learn on these graphs. We evaluate our\napproach on two tasks: code comment generation and method naming. Both tasks\nrequire reasoning on the semantics of complete code snippets. Experiment\nresults show that our approach outperforms baseline models, including\nhomogeneous graph-based models, showing that leveraging the type information of\nnodes and edges in program graphs can help in learning program semantics.",
    "Cyber-Physical System (CPS) represents systems that join both hardware and\nsoftware components to perform real-time services. Maintaining the system's\nreliability is critical to the continuous delivery of these services. However,\nthe CPS running environment is full of uncertainties and can easily lead to\nperformance degradation. As a result, the need for a recovery technique is\nhighly needed to achieve resilience in the system, with keeping in mind that\nthis technique should be as green as possible. This early doctorate proposal,\nsuggests a game theory solution to achieve resilience and green in CPS. Game\ntheory has been known for its fast performance in decision-making, helping the\nsystem to choose what maximizes its payoffs. The proposed game model is\ndescribed over a real-life collaborative artificial intelligence system (CAIS),\nthat involves robots with humans to achieve a common goal. It shows how the\nexpected results of the system will achieve the resilience of CAIS with\nminimized CO2 footprint.",
    "This paper aims to evaluate GitHub Copilot's generated code quality based on\nthe LeetCode problem set using a custom automated framework. We evaluate the\nresults of Copilot for 4 programming languages: Java, C++, Python3 and Rust. We\naim to evaluate Copilot's reliability in the code generation stage, the\ncorrectness of the generated code and its dependency on the programming\nlanguage, problem's difficulty level and problem's topic. In addition to that,\nwe evaluate code's time and memory efficiency and compare it to the average\nhuman results. In total, we generate solutions for 1760 problems for each\nprogramming language and evaluate all the Copilot's suggestions for each\nproblem, resulting in over 50000 submissions to LeetCode spread over a 2-month\nperiod. We found that Copilot successfully solved most of the problems.\nHowever, Copilot was rather more successful in generating code in Java and C++\nthan in Python3 and Rust. Moreover, in case of Python3 Copilot proved to be\nrather unreliable in the code generation phase. We also discovered that\nCopilot's top-ranked suggestions are not always the best. In addition, we\nanalysed how the topic of the problem impacts the correctness rate. Finally,\nbased on statistics information from LeetCode, we can conclude that Copilot\ngenerates more efficient code than an average human.",
    "Large language models (LLMs) have shown remarkable abilities to generate\ncode, however their ability to develop software for embedded systems, which\nrequires cross-domain knowledge of hardware and software has not been studied.\nIn this paper we develop an extensible, open source hardware-in-the-loop\nframework to systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to\nassess their capabilities and limitations for embedded system development. We\nobserve through our study that even when these tools fail to produce working\ncode, they consistently generate helpful reasoning about embedded design tasks.\nWe leverage this finding to study how human programmers interact with these\ntools, and develop an human-AI based software engineering workflow for building\nembedded systems.\n  Our evaluation platform for verifying LLM generated programs uses sensor\nactuator pairs for physical evaluation. We compare all three models with N=450\nexperiments and find surprisingly that GPT-4 especially shows an exceptional\nlevel of cross-domain understanding and reasoning, in some cases generating\nfully correct programs from a single prompt. In N=50 trials, GPT-4 produces\nfunctional I2C interfaces 66% of the time. GPT-4 also produces register-level\ndrivers, code for LoRa communication, and context-specific power optimizations\nfor an nRF52 program resulting in over 740x current reduction to 12.2uA. We\nalso characterize the models' limitations to develop a generalizable human-AI\nworkflow for using LLMs in embedded system development. We evaluate our\nworkflow with 15 users including novice and expert programmers. We find that\nour workflow improves productivity for all users and increases the success rate\nfor building a LoRa environmental sensor from 25% to 100%, including for users\nwith zero hardware or C/C++ experience.",
    "A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical\nsystem that learns actions in collaboration with humans in a shared environment\nto achieve a common goal. In particular, a CAIS is equipped with an AI model to\nsupport the decision-making process of this collaboration. When an event\ndegrades the performance of CAIS (i.e., a disruptive event), this\ndecision-making process may be hampered or even stopped. Thus, it is of\nparamount importance to monitor the learning of the AI model, and eventually\nsupport its decision-making process in such circumstances. This paper\nintroduces a new methodology to automatically support the decision-making\nprocess in CAIS when the system experiences performance degradation after a\ndisruptive event. To this aim, we develop a framework that consists of three\ncomponents: one manages or simulates CAIS's environment and disruptive events,\nthe second automates the decision-making process, and the third provides a\nvisual analysis of CAIS behavior. Overall, our framework automatically monitors\nthe decision-making process, intervenes whenever a performance degradation\noccurs, and recommends the next action. We demonstrate our framework by\nimplementing an example with a real-world collaborative robot, where the\nframework recommends the next action that balances between minimizing the\nrecovery time (i.e., resilience), and minimizing the energy adverse effects\n(i.e., greenness).",
    "Unit testing frameworks are nowadays considered a best practice, included in\nalmost all modern software development processes, to achieve rapid development\nof correct specifications. Knowledge representation and reasoning paradigms\nsuch as Answer Set Programming (ASP), that have been used in industry-level\napplications, are not an exception. Indeed, the first unit testing\nspecification language for ASP was proposed in 2011 as a feature of the ASPIDE\ndevelopment environment. Later, a more portable unit testing language was\nincluded in the LANA annotation language. In this paper we revisit both\nlanguages and tools for unit testing in ASP. We propose a new unit test\nspecification language that allows one to inline tests within ASP programs, and\nwe identify the computational complexity of the tasks associated with checking\nthe various program-correctness assertions. Test-case specifications are\ntransparent to the traditional evaluation, but can be interpreted by a specific\ntesting tool. Thus, we present a novel environment supporting test driven\ndevelopment of ASP programs.",
    "Self-adaptive software can assess and modify its behavior when the assessment\nindicates that the program is not performing as intended or when improved\nfunctionality or performance is available. Since the mid-1960s, the subject of\nsystem adaptivity has been extensively researched, and during the last decade,\nmany application areas and technologies involving self-adaptation have gained\nprominence. All of these efforts have in common the introduction of\nself-adaptability through software. Thus, it is essential to investigate\nsystematic software engineering methods to create self-adaptive systems that\nmay be used across different domains. The primary objective of this research is\nto summarize current advances in awareness requirements for adaptive strategies\nbased on an examination of state-of-the-art methods described in the\nliterature. This paper presents a review of self-adaptive systems in the\ncontext of requirement awareness and summarizes the most common methodologies\napplied. At first glance, it gives a review of the previous surveys and works\nabout self-adaptive systems. Afterward, it classifies the current self-adaptive\nsystems based on six criteria. Then, it presents and evaluates the most common\nself-adaptive approaches. Lastly, an evaluation among the self-adaptive models\nis conducted based on four concepts (requirements description, monitoring,\nrelationship, dependency/impact, and tools).",
    "(Source) Code summarization aims to automatically generate summaries/comments\nfor a given code snippet in the form of natural language. Such summaries play a\nkey role in helping developers understand and maintain source code. Existing\ncode summarization techniques can be categorized into extractive methods and\nabstractive methods. The extractive methods extract a subset of important\nstatements and keywords from the code snippet using retrieval techniques, and\ngenerate a summary that preserves factual details in important statements and\nkeywords. However, such a subset may miss identifier or entity naming, and\nconsequently, the naturalness of generated summary is usually poor. The\nabstractive methods can generate human-written-like summaries leveraging\nencoder-decoder models from the neural machine translation domain. The\ngenerated summaries however often miss important factual details.\n  To generate human-written-like summaries with preserved factual details, we\npropose a novel extractive-and-abstractive framework. The extractive module in\nthe framework performs a task of extractive code summarization, which takes in\nthe code snippet and predicts important statements containing key factual\ndetails. The abstractive module in the framework performs a task of abstractive\ncode summarization, which takes in the entire code snippet and important\nstatements in parallel and generates a succinct and human-written-like natural\nlanguage summary. We evaluate the effectiveness of our technique, called EACS,\nby conducting extensive experiments on three datasets involving six programming\nlanguages. Experimental results show that EACS significantly outperforms\nstate-of-the-art techniques in terms of all three widely used metrics,\nincluding BLEU, METEOR, and ROUGH-L.",
    "With the ever-increasing use of web APIs in modern-day applications, it is\nbecoming more important to test the system as a whole. In the last decade,\ntools and approaches have been proposed to automate the creation of\nsystem-level test cases for these APIs using evolutionary algorithms (EAs). One\nof the limiting factors of EAs is that the genetic operators (crossover and\nmutation) are fully randomized, potentially breaking promising patterns in the\nsequences of API requests discovered during the search. Breaking these patterns\nhas a negative impact on the effectiveness of the test case generation process.\nTo address this limitation, this paper proposes a new approach that uses\nagglomerative hierarchical clustering (AHC) to infer a linkage tree model,\nwhich captures, replicates, and preserves these patterns in new test cases. We\nevaluate our approach, called LT-MOSA, by performing an empirical study on 7\nreal-world benchmark applications w.r.t. branch coverage and real-fault\ndetection capability. We also compare LT-MOSA with the two existing\nstate-of-the-art white-box techniques (MIO, MOSA) for REST API testing. Our\nresults show that LT-MOSA achieves a statistically significant increase in test\ntarget coverage (i.e., lines and branches) compared to MIO and MOSA in 4 and 5\nout of 7 applications, respectively. Furthermore, LT-MOSA discovers 27 and 18\nunique real-faults that are left undetected by MIO and MOSA, respectively.",
    "Developers often wonder how to implement a certain functionality (e.g., how\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\nAPI-related natural language query is very helpful in this regard. Given a\nquery, existing approaches utilize information retrieval models to search for\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\n(i.e., keyword matching or word-to-word alignment) and lack a deep\nunderstanding of the semantics of the query.\n  We propose DeepAPI, a deep learning based approach to generate API usage\nsequences for a given natural language query. Instead of a bags-of-words\nassumption, it learns the sequence of words in a query and the sequence of\nassociated APIs. DeepAPI adapts a neural language model named RNN\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\ncontext vector, and generates an API sequence based on the context vector. We\nalso augment the RNN Encoder-Decoder by considering the importance of\nindividual APIs. We empirically evaluate our approach with more than 7 million\nannotated code snippets collected from GitHub. The results show that our\napproach generates largely accurate API sequences and outperforms the related\napproaches.",
    "In the process of code generation, it is essential to guarantee the generated\ncode satisfies grammar constraints of programming language (PL). However,\nneglecting grammar constraints is a fatal drawback of commonly used\nsequence-based code generation. In this paper, we devise a pushdown automaton\n(PDA)-based methodology to address this problem, exploiting the principle that\nPL is a subset of PDA recognizable language and code accepted by PDA is\ngrammatical. Specifically, we construct a PDA module and design an algorithm to\nconstrain the generation of sequence-based models to ensure grammatical\ncorrectness. Guided by this methodology, we further propose CodePAD, a\nsequence-based code generation framework equipped with a PDA module, to\nintegrate the deduction of PDA into deep learning. Additionally, this framework\ncan leverage states of PDA deduction (including state representation, state\nprediction task, and joint prediction with state) to assist models in learning\nPDA deduction. To comprehensively evaluate CodePAD, we construct a PDA for\nPython and conduct extensive experiments on four public benchmark datasets.\nCodePAD can leverage existing sequence-based models, and we show that it can\nachieve 100\\% grammatical correctness percentage on these benchmark datasets.\nThus, it relatively improve 17\\% CodeBLEU on CONALA, 8\\% EM on DJANGO, and 15\\%\nCodeBLEU on JUICE-10K compared to base models. In addition, our method\nsignificantly enhances pre-trained models, e.g., CodeBLEU of CodeGen-350M\nimprovement from 3.21 to 21.54 on MBPP in zero-shot setting.",
    "The successful completion of a software development process depends on the\nanalytical capability and foresightedness of the project manager. For the\nproject manager, the main intriguing task is to manage the risk factors as they\nadversely influence the completion deadline. One such key risk factor is staff\ntraining. The risk of this factor can be avoided by pre-judging the amount of\ntraining required by the staff. So, a procedure is required to help the project\nmanager make this decision. This paper presents a system that uses influence\ndiagrams to implement the risk model to aid decision making. The system also\nconsiders the cost of conducting the training, based on various risk factors\nsuch as, (i) Lack of experience with project software; (ii) Newly appointed\nstaff; (iii) Staff not well versed with the required quality standards; and\n(iv) Lack of experience with project environment. The system provides estimated\nrequirement details for staff training at the beginning of a software\ndevelopment project.",
    "Model compression can significantly reduce the sizes of deep neural network\n(DNN) models, and thus facilitates the dissemination of sophisticated, sizable\nDNN models, especially for their deployment on mobile or embedded devices.\nHowever, the prediction results of compressed models may deviate from those of\ntheir original models. To help developers thoroughly understand the impact of\nmodel compression, it is essential to test these models to find those deviated\nbehaviors before dissemination. However, this is a non-trivial task because the\narchitectures and gradients of compressed models are usually not available.\n  To this end, we propose DFLARE, a novel, search-based, black-box testing\ntechnique to automatically find triggering inputs that result in deviated\nbehaviors in image classification tasks. DFLARE iteratively applies a series of\nmutation operations to a given seed image, until a triggering input is found.\nFor better efficacy and efficiency, DFLARE models the search problem as Markov\nChains and leverages the Metropolis-Hasting algorithm to guide the selection of\nmutation operators in each iteration. Further, DFLARE utilizes a novel fitness\nfunction to prioritize the mutated inputs that either cause large differences\nbetween two models' outputs, or trigger previously unobserved models'\nprobability vectors. We evaluated DFLARE on 21 compressed models for image\nclassification tasks with three datasets. The results show that DFLARE\noutperforms the baseline in terms of efficacy and efficiency. We also\ndemonstrated that the triggering inputs found by DFLARE can be used to repair\nup to 48.48% deviated behaviors in image classification tasks and further\ndecrease the effectiveness of DFLARE on the repaired models.",
    "Testing is an integral part of the software development process. Yet, writing\ntests is time-consuming and therefore often neglected. Classical test\ngeneration tools such as EvoSuite generate behavioral test suites by optimizing\nfor coverage, but tend to produce tests that are hard to understand. Language\nmodels trained on code can generate code that is highly similar to that written\nby humans, but current models are trained to generate each file separately, as\nis standard practice in natural language processing, and thus fail to consider\nthe code-under-test context when producing a test file. In this work, we\npropose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style\nlanguage model with 2.7 Billion parameters, trained on a corpus of Python and\nJava projects. We utilize a novel pretraining signal that explicitly considers\nthe mapping between code and test files when available. We also drastically\nincrease the maximum sequence length of inputs to 8,192 tokens, 4x more than\ntypical code generation models, to ensure that the code context is available to\nthe model when generating test code. We analyze its usefulness for realistic\napplications, showing that sampling with filtering (e.g., by compilability,\ncoverage) allows it to efficiently produce tests that achieve coverage similar\nto ones written by developers while resembling their writing style. By\nutilizing the code context, CAT-LM generates more valid tests than even much\nlarger language models trained with more data (CodeGen 16B and StarCoder) and\nsubstantially outperforms a recent test-specific model (TeCo) at test\ncompletion. Overall, our work highlights the importance of incorporating\nsoftware-specific insights when training language models for code and paves the\nway to more powerful automated test generation.",
    "There is growing interest in software migration as the development of\nsoftware and society. Manually migrating projects between languages is\nerror-prone and expensive. In recent years, researchers have begun to explore\nautomatic program translation using supervised deep learning techniques by\nlearning from large-scale parallel code corpus. However, parallel resources are\nscarce in the programming language domain, and it is costly to collect\nbilingual data manually. To address this issue, several unsupervised\nprogramming translation systems are proposed. However, these systems still rely\non huge monolingual source code to train, which is very expensive. Besides,\nthese models cannot perform well for translating the languages that are not\nseen during the pre-training procedure. In this paper, we propose SDA-Trans, a\nsyntax and domain-aware model for program translation, which leverages the\nsyntax structure and domain knowledge to enhance the cross-lingual transfer\nability. SDA-Trans adopts unsupervised training on a smaller-scale corpus,\nincluding Python and Java monolingual programs. The experimental results on\nfunction translation tasks between Python, Java, and C++ show that SDA-Trans\noutperforms many large-scale pre-trained models, especially for unseen language\ntranslation.",
    "Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide\nefficient automatic analysis of real-world feature models (FM) of systems\nranging from cars to operating systems. It is well-known that solver-based\nanalysis of real-world FMs scale very well even though SAT instances obtained\nfrom such FMs are large, and the corresponding analysis problems are known to\nbe NP-complete. To better understand why SAT solvers are so effective, we\nsystematically studied many syntactic and semantic characteristics of a\nrepresentative set of large real-world FMs. We discovered that a key reason why\nlarge real-world FMs are easy-to-analyze is that the vast majority of the\nvariables in these models are unrestricted, i.e., the models are satisfiable\nfor both true and false assignments to such variables under the current partial\nassignment. Given this discovery and our understanding of CDCL SAT solvers, we\nshow that solvers can easily find satisfying assignments for such models\nwithout too many backtracks relative to the model size, explaining why solvers\nscale so well. Further analysis showed that the presence of unrestricted\nvariables in these real-world models can be attributed to their high-degree of\nvariability. Additionally, we experimented with a series of well-known\nnon-backtracking simplifications that are particularly effective in solving\nFMs. The remaining variables/clauses after simplifications, called the core,\nare so few that they are easily solved even with backtracking, further\nstrengthening our conclusions.",
    "Self-admitted technical debt (SATD) refers to a form of technical debt in\nwhich developers explicitly acknowledge and document the existence of technical\nshortcuts, workarounds, or temporary solutions within the codebase. Over recent\nyears, researchers have manually labeled datasets derived from various software\ndevelopment artifacts: source code comments, messages from the issue tracker\nand pull request sections, and commit messages. These datasets are designed for\ntraining, evaluation, performance validation, and improvement of machine\nlearning and deep learning models to accurately identify SATD instances.\nHowever, class imbalance poses a serious challenge across all the existing\ndatasets, particularly when researchers are interested in categorizing the\nspecific types of SATD. In order to address the scarcity of labeled data for\nSATD \\textit{identification} (i.e., whether an instance is SATD or not) and\n\\textit{categorization} (i.e., which type of SATD is being classified) in\nexisting datasets, we share the \\textit{SATDAUG} dataset, an augmented version\nof existing SATD datasets, including source code comments, issue tracker, pull\nrequests, and commit messages. These augmented datasets have been balanced in\nrelation to the available artifacts and provide a much richer source of labeled\ndata for training machine learning or deep learning models.",
    "Log analysis and monitoring are essential aspects in software maintenance and\nidentifying defects. In particular, the temporal nature and vast size of log\ndata leads to an interesting and important research question: How can logs be\nsummarised and monitored over time? While this has been a fundamental topic of\nresearch in the software engineering community, work has typically focused on\nheuristic-, syntax-, or static-based methods. In this work, we suggest an\nonline semantic-based clustering approach to error logs that dynamically\nupdates the log clusters to enable monitoring code error life-cycles. We also\nintroduce a novel metric to evaluate the performance of temporal log clusters.\nWe test our system and evaluation metric with an industrial dataset and find\nthat our solution outperforms similar systems. We hope that our work encourages\nfurther temporal exploration in defect datasets.",
    "In software engineering, software maintenance is the process of correction,\nupdating, and improvement of software products after handed over to the\ncustomer. Through offshore software maintenance outsourcing clients can get\nadvantages like reduce cost, save time, and improve quality. In most cases, the\nOSMO vendor generates considerable revenue. However, the selection of an\nappropriate proposal among multiple clients is one of the critical problems for\nOSMO vendors. The purpose of this paper is to suggest an effective machine\nlearning technique that can be used by OSMO vendors to assess or predict the\nOSMO client proposal. The dataset is generated through a survey of OSMO vendors\nworking in a developing country. The results showed that supervised\nlearning-based classifiers like Na\\\"ive Bayesian, SMO, Logistics apprehended\n69.75, 81.81, and 87.27 percent testing accuracy respectively. This study\nconcludes that supervised learning is the most suitable technique to predict\nthe OSMO client's proposal.",
    "DevOps has emerged as one of the most rapidly evolving software development\nparadigms. With the growing concerns surrounding security in software systems,\nthe DevSecOps paradigm has gained prominence, urging practitioners to\nincorporate security practices seamlessly into the DevOps workflow. However,\nintegrating security into the DevOps workflow can impact agility and impede\ndelivery speed. Recently, the advancement of artificial intelligence (AI) has\nrevolutionized automation in various software domains, including software\nsecurity. AI-driven security approaches, particularly those leveraging machine\nlearning or deep learning, hold promise in automating security workflows. They\nreduce manual efforts, which can be integrated into DevOps to ensure\nuninterrupted delivery speed and align with the DevSecOps paradigm\nsimultaneously. This paper seeks to contribute to the critical intersection of\nAI and DevSecOps by presenting a comprehensive landscape of AI-driven security\ntechniques applicable to DevOps and identifying avenues for enhancing security,\ntrust, and efficiency in software development processes. We analyzed 99\nresearch papers spanning from 2017 to 2023. Specifically, we address two key\nresearch questions (RQs). In RQ1, we identified 12 security tasks associated\nwith the DevSecOps process and reviewed existing AI-driven security approaches,\nthe problems they addressed, and the 65 benchmarks used to evaluate those\napproaches. Drawing insights from our findings, in RQ2, we discussed\nstate-of-the-art AI-driven security approaches, highlighted 15 challenges in\nexisting research, and proposed 15 corresponding avenues for future\nopportunities.",
    "This paper introduces a novel concept of self-forensics to complement the\nstandard autonomic self-CHOP properties of the self-managed systems, to be\nspecified in the Forensic Lucid language. We argue that self-forensics, with\nthe forensics taken out of the cybercrime domain, is applicable to\n\"self-dissection\" for the purpose of verification of autonomous software and\nhardware systems of flight-critical systems for automated incident and anomaly\nanalysis and event reconstruction by the engineering teams in a variety of\nincident scenarios during design and testing as well as actual flight data.",
    "Automated Program Repair (APR) helps improve the efficiency of software\ndevelopment and maintenance. Recent APR techniques use deep learning,\nparticularly the encoder-decoder architecture, to generate patches. Though\nexisting DL-based APR approaches have proposed different encoder architectures,\nthe decoder remains to be the standard one, which generates a sequence of\ntokens one by one to replace the faulty statement. This decoder has multiple\nlimitations: 1) allowing to generate syntactically incorrect programs, 2)\ninefficiently representing small edits, and 3) not being able to generate\nproject-specific identifiers.\n  In this paper, we propose Recoder, a syntax-guided edit decoder with\nplaceholder generation. Recoder is novel in multiple aspects: 1) Recoder\ngenerates edits rather than modified code, allowing efficient representation of\nsmall edits; 2) Recoder is syntax-guided, with the novel provider/decider\narchitecture to ensure the syntactic correctness of the patched program and\naccurate generation; 3) Recoder generates placeholders that could be\ninstantiated as project-specific identifiers later.\n  We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2\nand 420 additional bugs from Defects4J v2.0. Our results show that Recoder\nrepairs 53 bugs on Defects4J v1.2, which achieves 21.4% improvement over the\nprevious state-of-the-art approach for single-hunk bugs (TBar). Importantly, to\nour knowledge, Recoder is the first DL-based APR approach that has outperformed\nthe traditional APR approaches on this dataset. Furthermore, Recoder also\nrepairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5%\nmore than TBar (8 bugs) and 850% more than SimFix (2 bugs). This result\nsuggests that Recoder has better generalizability than existing APR approaches.",
    "While specifications for digital systems are provided in natural language,\nengineers undertake significant efforts to translate them into the programming\nlanguages understood by compilers for digital systems. Automating this process\nallows designers to work with the language in which they are most comfortable\n--the original natural language -- and focus instead on other downstream design\nchallenges. We explore the use of state-of-the-art machine learning (ML) to\nautomatically derive Verilog snippets from English via fine-tuning GPT-2, a\nnatural language ML system. We describe our approach for producing a suitable\ndataset of novice-level digital design tasks and provide a detailed exploration\nof GPT-2, finding encouraging translation performance across our task sets\n(94.8% correct), with the ability to handle both simple and abstract design\ntasks.",
    "While AI is extensively transforming Software Engineering (SE) fields, SE is\nstill in need of a framework to overall consider all phases to facilitate\nAutomated Software Evolution (ASEv), particularly for intelligent applications\nthat are context-rich, instead of conquering each division independently. Its\ncomplexity comes from the intricacy of the intelligent applications, the\nheterogeneity of the data sources, and the constant changes in the context.\nThis study proposes a conceptual framework for achieving automated software\nevolution, emphasizing the importance of multimodality learning. A Selective\nSequential Scope Model (3S) model is developed based on the conceptual\nframework, and it can be used to categorize existing and future research when\nit covers different SE phases and multimodal learning tasks. This research is a\npreliminary step toward the blueprint of a higher-level ASEv. The proposed\nconceptual framework can act as a practical guideline for practitioners to\nprepare themselves for diving into this area. Although the study is about\nintelligent applications, the framework and analysis methods may be adapted for\nother types of software as AI brings more intelligence into their life cycles.",
    "CodeCompose is an AI-assisted code authoring tool powered by large language\nmodels (LLMs) that provides inline suggestions to 10's of thousands of\ndevelopers at Meta. In this paper, we present how we scaled the product from\ndisplaying single-line suggestions to multi-line suggestions. This evolution\nrequired us to overcome several unique challenges in improving the usability of\nthese suggestions for developers.\n  First, we discuss how multi-line suggestions can have a 'jarring' effect, as\nthe LLM's suggestions constantly move around the developer's existing code,\nwhich would otherwise result in decreased productivity and satisfaction.\n  Second, multi-line suggestions take significantly longer to generate; hence\nwe present several innovative investments we made to reduce the perceived\nlatency for users. These model-hosting optimizations sped up multi-line\nsuggestion latency by 2.5x.\n  Finally, we conduct experiments on 10's of thousands of engineers to\nunderstand how multi-line suggestions impact the user experience and contrast\nthis with single-line suggestions. Our experiments reveal that (i) multi-line\nsuggestions account for 42% of total characters accepted (despite only\naccounting for 16% for displayed suggestions) (ii) multi-line suggestions\nalmost doubled the percentage of keystrokes saved for users from 9% to 17%.\nMulti-line CodeCompose has been rolled out to all engineers at Meta, and less\nthan 1% of engineers have opted out of multi-line suggestions.",
    "With the growing adoption of self-adaptive systems in various domains, there\nis an increasing need for strategies to assess their correct behavior. In\nparticular self-healing systems, which aim to provide resilience and\nfault-tolerance, often deal with unanticipated failures in critical and highly\ndynamic environments. Their reactive and complex behavior makes it challenging\nto assess if these systems execute according to the desired goals. Recently,\nseveral studies have expressed concern about the lack of systematic evaluation\nmethods for self-healing behavior.\n  In this paper, we propose CHESS, an approach for the systematic evaluation of\nself-adaptive and self-healing systems that builds on chaos engineering. Chaos\nengineering is a methodology for subjecting a system to unexpected conditions\nand scenarios. It has shown great promise in helping developers build resilient\nmicroservice architectures and cyber-physical systems. CHESS turns this idea\naround by using chaos engineering to evaluate how well a self-healing system\ncan withstand such perturbations. We investigate the viability of this approach\nthrough an exploratory study on a self-healing smart office environment. The\nstudy helps us explore the promises and limitations of the approach, as well as\nidentify directions where additional work is needed. We conclude with a summary\nof lessons learned.",
    "Test Case Selection (TCS) aims to select a subset of the test suite to run\nfor regression testing. The selection is typically based on past coverage and\nexecution cost data. Researchers have successfully used multi-objective\nevolutionary algorithms (MOEAs), such as NSGA-II and its variants, to solve\nthis problem. These MOEAs use traditional crossover operators to create new\ncandidate solutions through genetic recombination. Recent studies in numerical\noptimization have shown that better recombinations can be made using machine\nlearning, in particular link-age learning. Inspired by these recent advances in\nthis field, we propose a new variant of NSGA-II, called L2-NSGA, that uses\nlinkage learning to optimize test case selection. In particular, we use an\nunsupervised clustering algorithm to infer promising patterns among the\nsolutions (subset of test suites). Then, these patterns are used in the next\niterations of L2-NSGA to create solutions that preserve these inferred\npatterns. Our results show that our customizations make NSGA-II more effective\nfor test case selection. The test suite sub-sets generated by L2-NSGA are less\nexpensive and detect more faults than those generated by MOEAs used in the\nliterature for regression testing.",
    "Public vulnerability databases such as CVE and NVD account for only 60% of\nsecurity vulnerabilities present in open-source projects, and are known to\nsuffer from inconsistent quality. Over the last two years, there has been\nconsiderable growth in the number of known vulnerabilities across projects\navailable in various repositories such as NPM and Maven Central. Such an\nincreasing risk calls for a mechanism to infer the presence of security threats\nin a timely manner. We propose novel hierarchical deep learning models for the\nidentification of security-relevant commits from either the commit diff or the\nsource code for the Java classes. By comparing the performance of our model\nagainst code2vec, a state-of-the-art model that learns from path-based\nrepresentations of code, and a logistic regression baseline, we show that deep\nlearning models show promising results in identifying security-related commits.\nWe also conduct a comparative analysis of how various deep learning models\nlearn across different input representations and the effect of regularization\non the generalization of our models.",
    "This paper presents an ontology-based approach for the design of a\ncollaborative business process model (CBP). This CBP is considered as a\nspecification of needs in order to build a collaboration information system\n(CIS) for a network of organisations. The study is a part of a model driven\nengineering approach of the CIS in a specific enterprise interoperability\nframework that will be summarised. An adaptation of the Business Process\nModeling Notation (BPMN) is used to represent the CBP model. We develop a\nknowledge-based system (KbS) which is composed of three main parts: knowledge\ngathering, knowledge representation and reasoning, and collaborative business\nprocess modelling. The first part starts from a high abstraction level where\nknowledge from business partners is captured. A collaboration ontology is\ndefined in order to provide a structure to store and use the knowledge\ncaptured. In parallel, we try to reuse generic existing knowledge about\nbusiness processes from the MIT Process Handbook repository. This results in a\ncollaboration process ontology that is also described. A set of rules is\ndefined in order to extract knowledge about fragments of the CBP model from the\ntwo previous ontologies. These fragments are finally assembled in the third\npart of the KbS. A prototype of the KbS has been developed in order to\nimplement and support this approach. The prototype is a computer-aided design\ntool of the CBP. In this paper, we will present the theoretical aspects of each\npart of this KbS as well as the tools that we developed and used in order to\nsupport its functionalities.",
    "There have been major developments in Automated Driving (AD) and Driving\nAssist Systems (ADAS) in recent years. However, their safety assurance, thus\nmethodologies for testing, verification and validation AD/ADAS safety-critical\napplications remain as one the main challenges. Inevitably AI also penetrates\ninto AD/ADAS applications, such as object detection. Despite important\nbenefits, adoption of such learned-enabled components and systems in\nsafety-critical scenarios causes that conventional testing approaches (e.g.,\ndistance-based testing in automotive) quickly become infeasible. Similarly,\nsafety engineering approaches usually assume model-based components and do not\nhandle learning-enabled ones well. The authors have participated in the\npublic-funded project FOCETA , and developed an Automated Valet Parking (AVP)\nuse case. As the nature of the baseline implementation is imperfect, it offers\na space for continuous improvement based on modelling, verification,\nvalidation, and monitoring techniques. In this publication, we explain the\nsimulation-based development platform that is designed to verify and validate\nsafety-critical learning-enabled systems in continuous engineering loops.",
    "Monolithic software encapsulates all functional capabilities into a single\ndeployable unit. But managing it becomes harder as the demand for new\nfunctionalities grow. Microservice architecture is seen as an alternate as it\nadvocates building an application through a set of loosely coupled small\nservices wherein each service owns a single functional responsibility. But the\nchallenges associated with the separation of functional modules, slows down the\nmigration of a monolithic code into microservices. In this work, we propose a\nrepresentation learning based solution to tackle this problem. We use a\nheterogeneous graph to jointly represent software artifacts (like programs and\nresources) and the different relationships they share (function calls,\ninheritance, etc.), and perform a constraint-based clustering through a novel\nheterogeneous graph neural network. Experimental studies show that our approach\nis effective on monoliths of different types.",
    "Despite decades of research, SE lacks widely accepted models (that offer\nprecise quantitative stable predictions) about what factors most influence\nsoftware quality. This paper provides a promising result showing such stable\nmodels can be generated using a new transfer learning framework called\n\"STABILIZER\". Given a tree of recursively clustered projects (using project\nmeta-data), STABILIZER promotes a model upwards if it performs best in the\nlower clusters (stopping when the promoted model performs worse than the models\nseen at a lower level).\n  The number of models found by STABILIZER is minimal: one for defect\nprediction (756 projects) and less than a dozen for project health (1628\nprojects). Hence, via STABILIZER, it is possible to find a few projects which\ncan be used for transfer learning and make conclusions that hold across\nhundreds of projects at a time. Further, the models produced in this manner\noffer predictions that perform as well or better than the prior\nstate-of-the-art.\n  To the best of our knowledge, STABILIZER is order of magnitude faster than\nthe prior state-of-the-art transfer learners which seek to find conclusion\nstability, and these case studies are the largest demonstration of the\ngeneralizability of quantitative predictions of project quality yet reported in\nthe SE literature.\n  In order to support open science, all our scripts and data are online at\nhttps://github.com/Anonymous633671/STABILIZER.",
    "In industry as well as education as well as academics we see a growing need\nfor knowledge on how to apply machine learning in software applications. With\nthe educational programme ICT & AI at Fontys UAS we had to find an answer to\nthe question: \"How should we educate software engineers to become AI\nengineers?\" This paper describes our educational programme, the open source\ntools we use, and the literature it is based on. After three years of\nexperience, we present our lessons learned for both educational institutions\nand software engineers in practice.",
    "In software engineering practice, fixing a bug promptly reduces the\nassociated costs. On the other hand, the manual bug fixing process can be\ntime-consuming, cumbersome, and error-prone. In this work, we introduce a bug\ntriaging method, called Dependency-aware Bug Triaging (DABT), which leverages\nnatural language processing and integer programming to assign bugs to\nappropriate developers. Unlike previous works that mainly focus on one aspect\nof the bug reports, DABT considers the textual information, cost associated\nwith each bug, and dependency among them. Therefore, this comprehensive\nformulation covers the most important aspect of the previous works while\nconsidering the blocking effect of the bugs. We report the performance of the\nalgorithm on three open-source software systems, i.e., EclipseJDT, LibreOffice,\nand Mozilla. Our result shows that DABT is able to reduce the number of overdue\nbugs up to 12\\%. It also decreases the average fixing time of the bugs by half.\nMoreover, it reduces the complexity of the bug dependency graph by prioritizing\nblocking bugs.",
    "Artificial intelligence (AI) in its various forms finds more and more its way\ninto complex distributed systems. For instance, it is used locally, as part of\na sensor system, on the edge for low-latency high-performance inference, or in\nthe cloud, e.g. for data mining. Modern complex systems, such as connected\nvehicles, are often part of an Internet of Things (IoT). To manage complexity,\narchitectures are described with architecture frameworks, which are composed of\na number of architectural views connected through correspondence rules. Despite\nsome attempts, the definition of a mathematical foundation for architecture\nframeworks that are suitable for the development of distributed AI systems\nstill requires investigation and study. In this paper, we propose to extend the\nstate of the art on architecture framework by providing a mathematical model\nfor system architectures, which is scalable and supports co-evolution of\ndifferent aspects for example of an AI system. Based on Design Science\nResearch, this study starts by identifying the challenges with architectural\nframeworks. Then, we derive from the identified challenges four rules and we\nformulate them by exploiting concepts from category theory. We show how\ncompositional thinking can provide rules for the creation and management of\narchitectural frameworks for complex systems, for example distributed systems\nwith AI. The aim of the paper is not to provide viewpoints or architecture\nmodels specific to AI systems, but instead to provide guidelines based on a\nmathematical formulation on how a consistent framework can be built up with\nexisting, or newly created, viewpoints. To put in practice and test the\napproach, the identified and formulated rules are applied to derive an\narchitectural framework for the EU Horizon 2020 project ``Very efficient deep\nlearning in the IoT\" (VEDLIoT) in the form of a case study.",
    "Software quality is one of the essential aspects of a software. With\nincreasing demand, software designs are becoming more complex, increasing the\nprobability of software defects. Testers improve the quality of software by\nfixing defects. Hence the analysis of defects significantly improves software\nquality. The complexity of software also results in a higher number of defects,\nand thus manual detection can become a very time-consuming process. This gave\nresearchers incentives to develop techniques for automatic software defects\ndetection. In this paper, we try to analyze the state of the art machine\nlearning algorithms' performance for software defect classification. We used\nseven datasets from the NASA promise dataset repository for this research work.\nThe performance of Neural Networks and Gradient Boosting classifier dominated\nother algorithms.",
    "Online social networks have become an integral aspect of our daily lives and\nplay a crucial role in shaping our relationships with others. However, bugs and\nglitches, even minor ones, can cause anything from frustrating problems to\nserious data leaks that can have farreaching impacts on millions of users. To\nmitigate these risks, fuzz testing, a method of testing with randomised inputs,\ncan provide increased confidence in the correct functioning of a social\nnetwork. However, implementing traditional fuzz testing methods can be\nprohibitively difficult or impractical for programmers outside of the social\nnetwork's development team. To tackle this challenge, we present Socialz, a\nnovel approach to social fuzz testing that (1) characterises real users of a\nsocial network, (2) diversifies their interaction using evolutionary\ncomputation across multiple, non-trivial features, and (3) collects performance\ndata as these interactions are executed. With Socialz, we aim to put social\ntesting tools in everybody's hands, thereby improving the reliability and\nsecurity of social networks used worldwide. In our study, we came across (1)\none known limitation of the current GitLab CE and (2) 6,907 errors, of which\n40.16% are beyond our debugging skills.",
    "In this paper, we present NSGA-II-SVM (Non-dominated Sorting Genetic\nAlgorithm with Support Vector Machine Guidance), a novel learnable evolutionary\nand search-based testing algorithm that leverages Support Vector Machine (SVM)\nclassification models to direct the search towards failure-revealing test\ninputs. Supported by genetic search, NSGA-II-SVM creates iteratively SVM-based\nmodels of the test input space, learning which regions in the search space are\npromising to be explored. A subsequent sampling and repetition of evolutionary\nsearch iterations allow to refine and make the model more accurate in the\nprediction. Our preliminary evaluation of NSGA-II-SVM by testing an Automated\nValet Parking system shows that NSGA-II-SVM is more effective in identifying\nmore critical test cases than a state of the art learnable evolutionary testing\ntechnique as well as naive random search.",
    "Deep learning (DL) defines a new data-driven programming paradigm that\nconstructs the internal system logic of a crafted neuron network through a set\nof training data. We have seen wide adoption of DL in many safety-critical\nscenarios. However, a plethora of studies have shown that the state-of-the-art\nDL systems suffer from various vulnerabilities which can lead to severe\nconsequences when applied to real-world applications. Currently, the testing\nadequacy of a DL system is usually measured by the accuracy of test data.\nConsidering the limitation of accessible high quality test data, good accuracy\nperformance on test data can hardly provide confidence to the testing adequacy\nand generality of DL systems. Unlike traditional software systems that have\nclear and controllable logic and functionality, the lack of interpretability in\na DL system makes system analysis and defect detection difficult, which could\npotentially hinder its real-world deployment. In this paper, we propose\nDeepGauge, a set of multi-granularity testing criteria for DL systems, which\naims at rendering a multi-faceted portrayal of the testbed. The in-depth\nevaluation of our proposed testing criteria is demonstrated on two well-known\ndatasets, five DL systems, and with four state-of-the-art adversarial attack\ntechniques against DL. The potential usefulness of DeepGauge sheds light on the\nconstruction of more generic and robust DL systems.",
    "With the maturity of web services, containers, and cloud computing\ntechnologies, large services in traditional systems (e.g. the computation\nservices of machine learning and artificial intelligence) are gradually being\nbroken down into many microservices to increase service reusability and\nflexibility. Therefore, this study proposes an efficiency analysis framework\nbased on queuing models to analyze the efficiency difference of breaking down\ntraditional large services into n microservices. For generalization, this study\nconsiders different service time distributions (e.g. exponential distribution\nof service time and fixed service time) and explores the system efficiency in\nthe worst-case and best-case scenarios through queuing models (i.e. M/M/1\nqueuing model and M/D/1 queuing model). In each experiment, it was shown that\nthe total time required for the original large service was higher than that\nrequired for breaking it down into multiple microservices, so breaking it down\ninto multiple microservices can improve system efficiency. It can also be\nobserved that in the best-case scenario, the improvement effect becomes more\nsignificant with an increase in arrival rate. However, in the worst-case\nscenario, only slight improvement was achieved. This study found that breaking\ndown into multiple microservices can effectively improve system efficiency and\nproved that when the computation time of the large service is evenly\ndistributed among multiple microservices, the best improvement effect can be\nachieved. Therefore, this study's findings can serve as a reference guide for\nfuture development of microservice architecture.",
    "Data-driven engineering refers to systematic data collection and processing\nusing machine learning to improve engineering systems. Currently, the\nimplementation of data-driven engineering relies on fundamental data science\nand software engineering skills. At the same time, model-based engineering is\ngaining relevance for the engineering of complex systems. In previous work, a\nmodel-based engineering approach integrating the formalization of machine\nlearning tasks using the general-purpose modeling language SysML is presented.\nHowever, formalized machine learning tasks still require the implementation in\na specialized programming languages like Python. Therefore, this work aims to\nfacilitate the implementation of data-driven engineering in practice by\nextending the previous work of formalizing machine learning tasks by\nintegrating model transformation to generate executable code. The method\nfocuses on the modifiability and maintainability of the model transformation so\nthat extensions and changes to the code generation can be integrated without\nrequiring modifications to the code generator. The presented method is\nevaluated for feasibility in a case study to predict weather forecasts. Based\nthereon, quality attributes of model transformations are assessed and\ndiscussed. Results demonstrate the flexibility and the simplicity of the method\nreducing efforts for implementation. Further, the work builds a theoretical\nbasis for standardizing data-driven engineering implementation in practice.",
    "Existing language models such as n-grams for software code often fail to\ncapture a long context where dependent code elements scatter far apart. In this\npaper, we propose a novel approach to build a language model for software code\nto address this particular issue. Our language model, partly inspired by human\nmemory, is built upon the powerful deep learning-based Long Short Term Memory\narchitecture that is capable of learning long-term dependencies which occur\nfrequently in software code. Results from our intrinsic evaluation on a corpus\nof Java projects have demonstrated the effectiveness of our language model.\nThis work contributes to realizing our vision for DeepSoft, an end-to-end,\ngeneric deep learning-based framework for modeling software and its development\nprocess.",
    "The paper presents two equivalent definitions of answer sets for logic\nprograms with aggregates. These definitions build on the notion of unfolding of\naggregates, and they are aimed at creating methodologies to translate logic\nprograms with aggregates to normal logic programs or positive programs, whose\nanswer set semantics can be used to defined the semantics of the original\nprograms. The first definition provides an alternative view of the semantics\nfor logic programming with aggregates described by Pelov et al.\n  The second definition is similar to the traditional answer set definition for\nnormal logic programs, in that, given a logic program with aggregates and an\ninterpretation, the unfolding process produces a positive program. The paper\nshows how this definition can be extended to consider aggregates in the head of\nthe rules.\n  The proposed views of logic programming with aggregates are simple and\ncoincide with the ultimate stable model semantics, and with other semantic\ncharacterizations for large classes of program (e.g., programs with monotone\naggregates and programs that are aggregate-stratified).\n  Moreover, it can be directly employed to support an implementation using\navailable answer set solvers. The paper describes a system, called ASP^A, that\nis capable of computing answer sets of programs with arbitrary (e.g.,\nrecursively defined) aggregates."
  ]
}